{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-18 16:10:38.614027: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-18 16:10:38.631251: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-18 16:10:38.646836: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-18 16:10:38.651692: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-18 16:10:38.667380: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-18 16:10:39.346821: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device /gpu:7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set CUDA device order and visible devices\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7,8,9\"\n",
    "\n",
    "# Set the device\n",
    "device = '/cpu:0'\n",
    "if tf.config.experimental.list_physical_devices('GPU'):\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the second GPU\n",
    "        gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            tf.config.experimental.set_visible_devices(gpus[7], 'GPU')\n",
    "            device = '/gpu:7'\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (60000, 64, 64)\n",
      "Bounding boxes shape: (60000, 12, 5)\n",
      "Centers shape: (60000, 12, 3)\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def load_data(file_path, num_images):\n",
    "    images = []\n",
    "    bounding_boxes = []\n",
    "    centers = []\n",
    "\n",
    "    with h5py.File(file_path, 'r') as h5file:\n",
    "        for i in range(num_images):\n",
    "            # Load image data\n",
    "            image_dataset = f'image_{i}_image'\n",
    "            image_data = np.array(h5file[image_dataset])\n",
    "            images.append(image_data)\n",
    "\n",
    "            # Load bounding boxes\n",
    "            bbox_dataset = f'image_{i}_bounding_boxes_training'\n",
    "            bbox_data = np.array(h5file[bbox_dataset])\n",
    "            bounding_boxes.append(bbox_data)\n",
    "\n",
    "            # Load center positions\n",
    "            center_dataset = f'image_{i}_center_positions_training'\n",
    "            center_data = np.array(h5file[center_dataset])\n",
    "            centers.append(center_data)\n",
    "\n",
    "    # Convert lists to NumPy arrays for easier handling in deep learning pipelines\n",
    "    images = np.array(images)\n",
    "    bounding_boxes = np.array(bounding_boxes)\n",
    "    centers = np.array(centers)\n",
    "\n",
    "    return images, bounding_boxes, centers\n",
    "\n",
    "# Example usage\n",
    "file_path = '/home/da886/ElectronCountingProject/60KImages_64x64Training10electronhits.h5'\n",
    "num_images = 60000 ####60000\n",
    "\n",
    "images, bounding_boxes, centers = load_data(file_path, num_images)\n",
    "\n",
    "# Verify shapes\n",
    "print(f'Images shape: {images.shape}')\n",
    "print(f'Bounding boxes shape: {bounding_boxes.shape}')\n",
    "print(f'Centers shape: {centers.shape}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-18 16:11:38.474288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79078 MB memory:  -> device: 7, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:c9:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "image_normalized = (images + 1e-9) / 64\n",
    "# normalized_boxes = boxes / [1, 64, 64, 64, 64]\n",
    "\n",
    "images_np = image_normalized\n",
    "center_coordinates = centers / [1,64, 64]  # Normalizing the center coordinates\n",
    "probabilities = np.array(center_coordinates[:,:, :-2])\n",
    "probabilities = tf.expand_dims(probabilities, axis=1)\n",
    "center_coordinates_np = np.array(center_coordinates[:, :, 1:])\n",
    "center_coordinates_np = tf.expand_dims(center_coordinates_np, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f8c4c18f7d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b= np.random.randint(0,100)\n",
    "plt.imshow(images_np[b])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "dataset = tf.data.Dataset.from_tensor_slices((images_np, {'x_prob_reshape': probabilities, 'x_midpoints_reshape': center_coordinates_np}))\n",
    "dataset = dataset.shuffle(buffer_size=10000,reshuffle_each_iteration=True).batch(batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with h5py.File('/home/da886/ElectronCountingProject/final_combined_images_bboxes.h5', 'r') as hdf:\n",
    "#     ls = list(hdf.keys())\n",
    "#     images = hdf.get('images')\n",
    "#     # boxes = hdf.get('boxes')\n",
    "#     center_coordinates = hdf.get('center_coordinates')\n",
    "#     images = np.array(images)\n",
    "#     # boxes = np.array(boxes)\n",
    "#     center_coordinates = np.array(center_coordinates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.       , 0.8046875, 0.1640625],\n",
       "        [1.       , 0.8515625, 0.2109375],\n",
       "        [1.       , 0.1015625, 0.3046875],\n",
       "        ...,\n",
       "        [0.       , 0.       , 0.       ],\n",
       "        [0.       , 0.       , 0.       ],\n",
       "        [0.       , 0.       , 0.       ]],\n",
       "\n",
       "       [[1.       , 0.4140625, 0.4140625],\n",
       "        [1.       , 0.1015625, 0.4765625],\n",
       "        [1.       , 0.0859375, 0.2109375],\n",
       "        ...,\n",
       "        [0.       , 0.       , 0.       ],\n",
       "        [0.       , 0.       , 0.       ],\n",
       "        [0.       , 0.       , 0.       ]],\n",
       "\n",
       "       [[1.       , 0.0703125, 0.5859375],\n",
       "        [1.       , 0.3359375, 0.3359375],\n",
       "        [1.       , 0.7734375, 0.4140625],\n",
       "        ...,\n",
       "        [0.       , 0.       , 0.       ],\n",
       "        [0.       , 0.       , 0.       ],\n",
       "        [0.       , 0.       , 0.       ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.       , 0.2109375, 0.2265625],\n",
       "        [0.       , 0.       , 0.       ],\n",
       "        [0.       , 0.       , 0.       ],\n",
       "        ...,\n",
       "        [0.       , 0.       , 0.       ],\n",
       "        [0.       , 0.       , 0.       ],\n",
       "        [0.       , 0.       , 0.       ]],\n",
       "\n",
       "       [[1.       , 0.8671875, 0.0390625],\n",
       "        [1.       , 0.4453125, 0.1953125],\n",
       "        [1.       , 0.7890625, 0.1328125],\n",
       "        ...,\n",
       "        [0.       , 0.       , 0.       ],\n",
       "        [0.       , 0.       , 0.       ],\n",
       "        [0.       , 0.       , 0.       ]],\n",
       "\n",
       "       [[1.       , 0.5859375, 0.4921875],\n",
       "        [1.       , 0.9140625, 0.4609375],\n",
       "        [1.       , 0.5859375, 0.8984375],\n",
       "        ...,\n",
       "        [0.       , 0.       , 0.       ],\n",
       "        [0.       , 0.       , 0.       ],\n",
       "        [0.       , 0.       , 0.       ]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-15 22:00:23.976879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79078 MB memory:  -> device: 7, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:c9:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# image_normalized = (images + 1e-9) / 64\n",
    "# # normalized_boxes = boxes / [1, 64, 64, 64, 64]\n",
    "\n",
    "# images_np = image_normalized\n",
    "# center_coordinates = center_coordinates / [1,64, 64]  # Normalizing the center coordinates\n",
    "# probabilities = np.array(center_coordinates[:,:, :-2])\n",
    "# probabilities = tf.expand_dims(probabilities, axis=1)\n",
    "# center_coordinates_np = np.array(center_coordinates[:, :, 1:])\n",
    "# center_coordinates_np = tf.expand_dims(center_coordinates_np, axis=1)\n",
    "# batch_size = 128\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((images_np, {'x_prob_reshape': probabilities, 'x_midpoints_reshape': center_coordinates_np}))\n",
    "# # dataset = dataset.shuffle(buffer_size=10000).batch(batch_size)\n",
    "# total_items = len(images_np)\n",
    "# train_size = int(total_items * 0.9)\n",
    "# test_size = total_items - train_size \n",
    "\n",
    "#     # Splitting the dataset\n",
    "# train_dataset = dataset.take(train_size)\n",
    "# test_dataset = dataset.skip(train_size)\n",
    "\n",
    "\n",
    "# train_dataset = train_dataset.shuffle(buffer_size=train_size,reshuffle_each_iteration=True)\n",
    "# train_dataset = train_dataset.batch(batch_size)\n",
    "# test_dataset = test_dataset.batch(batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "input_shape = (64, 64, 1)\n",
    "num_classes = 12\n",
    "num_coordinates = 2\n",
    "\n",
    "\n",
    "x_input = layers.Input(shape=input_shape)\n",
    "x = layers.Conv2D(512, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.01))(x_input)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = layers.Conv2D(512, kernel_size=3, padding='same', activation='relu',kernel_regularizer=l2(0.01))(x)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = layers.BatchNormalization()(x) \n",
    "x = layers.Dropout(0.4)(x) \n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "# Probability output\n",
    "x_prob = layers.Dense(num_classes, activation='sigmoid', name='x_prob')(x)\n",
    "x_prob_reshape = layers.Reshape((-1, num_classes, 1), name='x_prob_reshape')(x_prob)\n",
    "\n",
    "# Bounding box output\n",
    "x_midpoints = layers.Dense(num_classes * num_coordinates, activation='sigmoid', name='x_midpoints')(x)\n",
    "x_midpoints_reshape = layers.Reshape((-1, num_classes, num_coordinates), name='x_midpoints_reshape')(x_midpoints)\n",
    "\n",
    "model = tf.keras.models.Model(x_input, [x_prob_reshape,x_midpoints_reshape])\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model(\"/home/da886/ElectronCountingProject/10electronhits10000imagesonelayerv3.keras\")  ##### from Jingru's dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras import layers, callbacks\n",
    "# reduce_lr = callbacks.ReduceLROnPlateau(monitor='loss', factor=0.1, patience=5, min_lr=3e-11, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable()\n",
    "class MaskedMeanSquaredError(tf.keras.losses.Loss):\n",
    "    def __init__(self, name=\"masked_mse_loss\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Create mask\n",
    "        mask = tf.reduce_sum(y_true, axis=-1) > 0\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        mask = tf.expand_dims(mask, axis=-1)\n",
    "\n",
    "        # Apply mask to the true and predicted values\n",
    "        y_true_masked = y_true * mask\n",
    "        y_pred_masked = y_pred * mask\n",
    "\n",
    "        # Calculate mean squared error\n",
    "        mse = tf.reduce_mean(tf.square(y_true_masked - y_pred_masked), axis=-1)\n",
    "\n",
    "        # Normalize the loss by the number of non-zero elements in the mask\n",
    "        mask_sum = tf.reduce_sum(mask)\n",
    "        masked_loss = tf.reduce_sum(mse) / mask_sum\n",
    "        return masked_loss\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class MaskedBinaryCrossentropy(tf.keras.losses.Loss):\n",
    "    def __init__(self, name=\"masked_binary_crossentropy\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Create mask\n",
    "        mask = tf.reduce_sum(y_true, axis=-1) > 0\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        mask = tf.expand_dims(mask, axis=-1)\n",
    "\n",
    "        # Apply mask to the true and predicted values\n",
    "        y_true_masked = y_true * mask\n",
    "        y_pred_masked = y_pred * mask\n",
    "\n",
    "        # Calculate binary cross-entropy\n",
    "        bce = tf.keras.losses.binary_crossentropy(y_true_masked, y_pred_masked)\n",
    "\n",
    "        # Normalize the loss by the number of non-zero elements in the mask\n",
    "        mask_sum = tf.reduce_sum(mask)\n",
    "        masked_loss = tf.reduce_sum(bce) / mask_sum\n",
    "        return masked_loss\n",
    "model.compile(optimizer= optimizer, loss= {'x_prob_reshape': MaskedBinaryCrossentropy(), 'x_midpoints_reshape':MaskedMeanSquaredError()})   \n",
    "# num_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, callbacks\n",
    "# Custom callback to save the model every 10 epochs\n",
    "class CustomModelCheckpoint(callbacks.Callback):\n",
    "    def __init__(self, save_freq, save_path):\n",
    "        super(CustomModelCheckpoint, self).__init__()\n",
    "        self.save_freq = save_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.save_freq == 0:\n",
    "            self.model.save(self.save_path.format(epoch=epoch + 1))\n",
    "            print(f\"Model saved at epoch {epoch + 1}\")\n",
    "\n",
    "save_freq = 5  # Save every 10 epochs\n",
    "save_path = \"/home/da886/ElectronCountingProject/Weights from my data/weights for 2 layers 10 electrons with regularizer/modelv1_epoch_{epoch:02d}.h5\"\n",
    "checkpoint_callback = CustomModelCheckpoint(save_freq=save_freq, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 35ms/step - loss: 0.0787\n",
      "Epoch 2/100\n",
      "\u001b[1m331/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 0.0768"
     ]
    }
   ],
   "source": [
    "model.fit(dataset,epochs =100,callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"/home/da886/ElectronCountingProject/Weights from my data/weights for 1 layer 10 electron hits/1layer60000imagesmodelv1.keras\")  ##### Jingrui's\n",
    "model = tf.keras.models.load_model(\"/home/da886/ElectronCountingProject/Weights from my data/weights for 1 layer 10 electron hits/1layer60000imagesmodelv1.keras\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721318608.449876 1453508 service.cc:146] XLA service 0x7fa6c803f250 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1721318608.449939 1453508 service.cc:154]   StreamExecutor device (0): NVIDIA A100 80GB PCIe, Compute Capability 8.0\n",
      "2024-07-18 16:03:28.456691: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-18 16:03:28.613666: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/13\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 1s/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-18 16:03:29.209881: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:762] The NVIDIA driver's CUDA version is 12.4 which is older than the ptxas CUDA version (12.5.82). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "I0000 00:00:1721318609.264054 1453508 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((images_np, {'x_prob_reshape': probabilities, 'x_midpoints_reshape': center_coordinates_np}))\n",
    "# test_dataset = dataset.take(train_size)\n",
    "dataset = dataset.batch(400)\n",
    "inputs,targets = next(iter(dataset))\n",
    "# inputs,targets = next(iter(test_dataset))\n",
    "output =model.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/objectdetection/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/objectdetection/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:429\u001b[0m, in \u001b[0;36mTensorFlowTrainer.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    428\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[0;32m--> 429\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    431\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_end(step, logs)\n",
      "File \u001b[0;32m~/anaconda3/envs/objectdetection/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/objectdetection/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/objectdetection/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:919\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m   \u001b[38;5;66;03m# If we did not create any variables the trace we have is good enough.\u001b[39;00m\n\u001b[1;32m    914\u001b[0m   filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    915\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(\n\u001b[1;32m    916\u001b[0m           bound_args\n\u001b[1;32m    917\u001b[0m       )\n\u001b[1;32m    918\u001b[0m   )\n\u001b[0;32m--> 919\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    920\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn_with_cond\u001b[39m(inner_args, inner_kwds):\n\u001b[1;32m    925\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Conditionally runs initialization if it's needed.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/objectdetection/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/objectdetection/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/objectdetection/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/objectdetection/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1567\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/envs/objectdetection/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n = model.evaluate(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 0.16341649\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# r = np.random.randint(0,100)\n",
    "tensor1 = tf.constant(targets['x_midpoints_reshape'], dtype=tf.float64)\n",
    "\n",
    "tensor2 = tf.constant(output[1], dtype=tf.float32)\n",
    "\n",
    "\n",
    "tensor2 = tf.cast(tensor2, tf.float64)\n",
    "\n",
    "\n",
    "mse_loss_fn = MaskedMeanSquaredError()\n",
    "mse_loss = mse_loss_fn(tensor1, tensor2)\n",
    "\n",
    "print(\"MSE Loss:\", mse_loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# t = np.random.randint(0,450)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m t\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 44\u001b[0m visualize_midpoints(\u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mconvert_to_tensor(inputs[t]), probabilities[t]\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39msqueeze(), tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(output[\u001b[38;5;241m1\u001b[39m][t,\u001b[38;5;241m0\u001b[39m,:,:])\u001b[38;5;241m*\u001b[39m[\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m64\u001b[39m]) \u001b[38;5;66;03m##myprediction   \u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize_midpoints2\u001b[39m(image, probability_vector, midpoints, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m    Visualizes midpoints on an image based on a probability vector.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m    None (displays the image with midpoints).\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_midpoints(image, probability_vector, midpoints, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Visualizes midpoints on an image based on a probability vector.\n",
    "\n",
    "    Parameters:\n",
    "    - image: A 3D tensor representing the image.\n",
    "    - probability_vector: A 1D tensor representing the probabilities associated with midpoints.\n",
    "    - midpoints: A 2D tensor representing the midpoint coordinates (x, y).\n",
    "    - threshold: Probability threshold for visualization.\n",
    "\n",
    "    Returns:\n",
    "    None (displays the image with midpoints).\n",
    "    \"\"\"\n",
    "    # Convert to NumPy arrays for easier handling\n",
    "    image_np = image\n",
    "    prob_vector_np = probability_vector\n",
    "    midpoints_np = midpoints#.numpy() if hasattr(midpoints, 'numpy') else midpoints\n",
    "\n",
    "    # Denormalize image if necessary (adjust based on your normalization method)\n",
    "    denormalized_image = image_np  # Modify if normalization was applied during training\n",
    "\n",
    "    # Visualize the image\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(denormalized_image, cmap='gray')\n",
    "    plt.title(\"Predicted Midpoint Visualization\")\n",
    "\n",
    "    # Plot midpoints based on probability threshold\n",
    "    for i, (y,x) in enumerate(midpoints_np):\n",
    "        prob = prob_vector_np[i]\n",
    "        if prob > threshold:\n",
    "            plt.scatter(y, x, color='red', s=5)\n",
    "            # if i == 0:  # Add label only once to avoid repetition in the legend\n",
    "            #     plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "# t = np.random.randint(0,450)\n",
    "t=1\n",
    "visualize_midpoints(tf.convert_to_tensor(inputs[t]), probabilities[t].numpy().squeeze(), tf.convert_to_tensor(output[1][t,0,:,:])*[64,64]) ##myprediction   \n",
    "\n",
    "def visualize_midpoints2(image, probability_vector, midpoints, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Visualizes midpoints on an image based on a probability vector.\n",
    "\n",
    "    Parameters:\n",
    "    - image: A 3D tensor representing the image.\n",
    "    - probability_vector: A 1D tensor representing the probabilities associated with midpoints.\n",
    "    - midpoints: A 2D tensor representing the midpoint coordinates (x, y).\n",
    "    - threshold: Probability threshold for visualization.\n",
    "\n",
    "    Returns:\n",
    "    None (displays the image with midpoints).\n",
    "    \"\"\"\n",
    "    # Convert to NumPy arrays for easier handling\n",
    "    image_np = image\n",
    "    prob_vector_np = probability_vector\n",
    "    midpoints_np = midpoints#.numpy() if hasattr(midpoints, 'numpy') else midpoints\n",
    "\n",
    "    # Denormalize image if necessary (adjust based on your normalization method)\n",
    "    denormalized_image = image_np  # Modify if normalization was applied during training\n",
    "\n",
    "    # Visualize the image\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(denormalized_image, cmap='gray')\n",
    "    plt.title(\"Ground Truth Midpoint Visualization\")\n",
    "\n",
    "    # Plot midpoints based on probability threshold\n",
    "    for i, (y,x) in enumerate(midpoints_np):\n",
    "        prob = prob_vector_np[i]\n",
    "        if prob > threshold:\n",
    "            plt.scatter(y, x, color='red', s=5)\n",
    "            # if i == 0:  # Add label only once to avoid repetition in the legend\n",
    "            #     plt.legend()\n",
    "\n",
    "    plt.show()     \n",
    "visualize_midpoints2(tf.convert_to_tensor(images[t]), probabilities[t].numpy().squeeze(), tf.convert_to_tensor(center_coordinates_np[t,0,:,:])*[64,64]) ##ground truth          \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5161742092761595, Recall: 1.0, F1 Score: 0.6808903701410244, MAE: 0.2807062132781524, MSE: 0.15852413947898508\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_model(predictions, ground_truths, probability_threshold=0.9, midpoint_diff_threshold=0.05):\n",
    "    pred_probabilities = predictions[0]\n",
    "    pred_midpoints = predictions[1]\n",
    "    gt_probabilities = ground_truths[0]\n",
    "    gt_midpoints = ground_truths[1]\n",
    "    \n",
    "    # Valid predictions are those above the probability threshold\n",
    "    valid_predictions = pred_probabilities > probability_threshold\n",
    "    no_predictions = pred_probabilities <= probability_threshold\n",
    "\n",
    "    # Calculate differences for midpoint evaluations\n",
    "    midpoint_diff = np.abs(pred_midpoints - gt_midpoints)\n",
    "    is_accurate_prediction = np.all(midpoint_diff <= midpoint_diff_threshold, axis=1)\n",
    "    \n",
    "    # True Positives: Valid predictions close enough to the ground truth\n",
    "    TP = np.sum(np.logical_and(valid_predictions, is_accurate_prediction))\n",
    "    \n",
    "    # False Positives: Valid predictions not close enough to the ground truth\n",
    "    FP = np.sum(np.logical_and(valid_predictions, ~is_accurate_prediction))\n",
    "    \n",
    "    # True Negatives: No predictions where no objects are actually present\n",
    "    TN = np.sum(np.logical_and(no_predictions, gt_probabilities <= probability_threshold))\n",
    "    \n",
    "    # False Negatives: No predictions where objects are actually present\n",
    "    FN = np.sum(np.logical_and(no_predictions, gt_probabilities > probability_threshold))\n",
    "    \n",
    "    # Metrics calculation\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    F1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    MAE = np.mean(midpoint_diff)\n",
    "    MSE = np.mean(np.square(midpoint_diff))\n",
    "    \n",
    "    return precision, recall, F1_score, MAE, MSE\n",
    "\n",
    "# Usage \n",
    "predictions = [output[0][0:800], output[1][0:800]]\n",
    "ground_truths = [targets['x_prob_reshape'][0:800], targets['x_midpoints_reshape'][0:800]]\n",
    "precision, recall, F1_score, MAE, MSE = evaluate_model(predictions, ground_truths)\n",
    "\n",
    "print(f\"Precision: {precision}, Recall: {recall}, F1 Score: {F1_score}, MAE: {MAE}, MSE: {MSE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "import tensorflow as tf  # Assuming TensorFlow is used for tensor conversion\n",
    "\n",
    "def pos_deviation(coords, truth):\n",
    "    \"\"\"\n",
    "    Calculate the distances between detected electron incident positions and the ground truth positions in units of pixels.\n",
    "    \"\"\"\n",
    "    distances = []\n",
    "    if len(coords) > 0:\n",
    "        assignment, distances = pairwise_distances_argmin_min(coords, truth)\n",
    "    return distances\n",
    "\n",
    "def end2end_evaluation(groundtruth, predicted, tolerance):\n",
    "    \"\"\"\n",
    "    Evaluate the model performance in detecting electron incident positions.\n",
    "\n",
    "    Args:\n",
    "    groundtruth: numpy array or TensorFlow tensor of ground truth electron positions.\n",
    "    predicted: numpy array or TensorFlow tensor of predicted electron positions.\n",
    "    tolerance: predictions with position error no larger than \"tolerance\" pixels will be selected as true positive.\n",
    "\n",
    "    Returns:\n",
    "    recall: recall of the prediction\n",
    "    precision: precision of the prediction\n",
    "    f1: F1 score\n",
    "    dce: detector conversion efficiency (ratio of detected electrons to ground truth electrons)\n",
    "    mae_position: mean position error (Euclidean distance) averaged over all detected electrons\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert TensorFlow tensors to NumPy arrays if necessary\n",
    "    if isinstance(groundtruth, tf.Tensor):\n",
    "        groundtruth = groundtruth.numpy()\n",
    "    if isinstance(predicted, tf.Tensor):\n",
    "        predicted = predicted.numpy()\n",
    "\n",
    "    # Flatten the coordinates for comparison\n",
    "    groundtruth_coords = groundtruth.reshape(-1, 2)\n",
    "    predicted_coords = predicted.reshape(-1, 2)\n",
    "\n",
    "    # Calculate the real total number of ground truth electrons\n",
    "    num_groundtruth_electrons = len(groundtruth_coords)\n",
    "\n",
    "    # Calculate the deviations between the predicted and ground truth positions\n",
    "    deviations = pos_deviation(predicted_coords, groundtruth_coords)\n",
    "\n",
    "    # Get the true positives, which have an error no larger than \"tolerance\" pixels\n",
    "    true_positives = np.sum(deviations <= tolerance)\n",
    "\n",
    "    # Calculate precision\n",
    "    precision = true_positives / len(predicted_coords) if len(predicted_coords) > 0 else 0\n",
    "\n",
    "    # Calculate recall\n",
    "    recall = true_positives / num_groundtruth_electrons if num_groundtruth_electrons > 0 else 0\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # Calculate detector conversion efficiency\n",
    "    dce = len(predicted_coords) / num_groundtruth_electrons if num_groundtruth_electrons > 0 else 0\n",
    "\n",
    "    # Calculate mean absolute error of positions\n",
    "    mae_position = deviations.mean() if len(deviations) > 0 else 0\n",
    "\n",
    "    # Print and return the evaluation metrics\n",
    "    print('Recall:', recall, 'Precision:', precision, 'F1:', f1, 'DCE:', dce, 'MAE of positions:', mae_position)\n",
    "    return recall, precision, f1, dce, mae_position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 1.0 Precision: 1.0 F1: 1.0 DCE: 1.0 MAE of positions: 0.008038402247415836\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "evaluation = end2end_evaluation(targets['x_midpoints_reshape'][0:600], output[1][0:600],0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
