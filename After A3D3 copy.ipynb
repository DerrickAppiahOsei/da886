{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 15:36:32.985954: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-24 15:36:32.998586: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-24 15:36:33.011175: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-24 15:36:33.014971: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-24 15:36:33.025843: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-24 15:36:33.622169: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 15:36:35.072266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79196 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:4e:00.0, compute capability: 8.0\n",
      "2024-10-24 15:36:35.073700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 79196 MB memory:  -> device: 1, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:4f:00.0, compute capability: 8.0\n",
      "2024-10-24 15:36:35.075035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 79196 MB memory:  -> device: 2, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:c5:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3,4,5\"  # Only GPUs 0 and 1 will be visible to TensorFlow\n",
    "\n",
    "import tensorflow as tf\n",
    "strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:3\", \"/gpu:4\",\"/gpu:5\"])\n",
    "# strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# DataLoader Class Definition\n",
    "# -----------------------------\n",
    "class DataLoader:\n",
    "    def __init__(self, h5_filename):\n",
    "        self.h5_filename = h5_filename\n",
    "        self.images, self.centers = self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        with h5py.File(self.h5_filename, 'r') as f:\n",
    "            images = np.array(f['images'])\n",
    "            centers = np.array(f['centers_training'])\n",
    "        return images, centers\n",
    "\n",
    "    def plot_image_with_centers(self, image_index=None):\n",
    "        if image_index is None:\n",
    "            image_index = np.random.randint(0, len(self.images))\n",
    "\n",
    "        image = self.images[image_index]\n",
    "        centers = self.centers[image_index]\n",
    "\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        valid_centers = centers[centers[:, 0] == 1]\n",
    "        for center in valid_centers:\n",
    "            plt.scatter(center[1], center[2], c='red', marker='o',s=5)  # center[1] is x and center[2] is y\n",
    "        plt.title('Image with Valid Centers Marked')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_centers(centers):\n",
    "        return centers[np.lexsort((centers[:, 0], centers[:, 1]))]\n",
    "\n",
    "    def normalize_data(self):\n",
    "        normalized_images = self.images / np.max(self.images)\n",
    "        sorted_centers = np.array([self.sort_centers(image_centers[:, 1:]) for image_centers in self.centers])\n",
    "        normalized_centers = sorted_centers / np.max(centers)\n",
    "\n",
    "        normalized_midpoints = tf.expand_dims(normalized_centers, axis=1)\n",
    "        return normalized_images, normalized_midpoints.numpy()\n",
    "\n",
    "    def split_data(self, train_size=0.8, random_state=42):\n",
    "        normalized_images, normalized_midpoints_np = self.normalize_data()\n",
    "        return train_test_split(normalized_images, normalized_midpoints_np, train_size=train_size, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Utility Function Definition\n",
    "# -----------------------------\n",
    "def plot_transposed_images_with_midpoints(dataset, image_indices=[0, 1, 2, 3]):\n",
    "    \"\"\"\n",
    "    Extracts multiple images and their midpoints from the given dataset, transposes the images, \n",
    "    corrects the midpoints, and plots the transposed images with the corrected midpoints.\n",
    "\n",
    "    Args:\n",
    "    - dataset (tf.data.Dataset): The dataset from which to extract the images and midpoints.\n",
    "    - image_indices (list): The indices of the images in the batch to visualize. Default is [0, 1, 2, 3].\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract a sample image batch and its corresponding midpoints from the dataset\n",
    "    sample_image_batch, sample_midpoints_batch = next(iter(dataset))\n",
    "\n",
    "    # Create a figure with 2x2 subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(6, 6))\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < len(image_indices):\n",
    "            # Select the specified image and corresponding midpoints from the batch\n",
    "            sample_image = np.array(sample_image_batch[image_indices[i]])\n",
    "            sample_midpoints = np.array(sample_midpoints_batch[image_indices[i]])\n",
    "\n",
    "            # Transpose the image\n",
    "            transposed_image = sample_image\n",
    "\n",
    "            # Correct the midpoints by swapping the x and y coordinates\n",
    "            transposed_midpoints_corrected = sample_midpoints[:, :, [1, 0]]\n",
    "\n",
    "            # Plot the transposed image with corrected midpoints\n",
    "            ax.imshow(transposed_image, cmap='gray')\n",
    "            ax.scatter(\n",
    "                transposed_midpoints_corrected[:, :, 0] * 31, \n",
    "                transposed_midpoints_corrected[:, :, 1] * 31, \n",
    "                c='red', marker='o', s=5\n",
    "            )\n",
    "            ax.set_title(f'Image {image_indices[i]} for this batch')\n",
    "        else:\n",
    "            ax.axis('off')  # If fewer than 4 images are requested, hide the unused subplots\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# Custom Loss and Callback Definitions\n",
    "# -----------------------------\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def custom_loss(exponent):\n",
    "    def loss(y_true, y_pred):\n",
    "        diff = tf.abs(y_true - y_pred)\n",
    "        powered_diff = tf.pow(diff, exponent)\n",
    "        return tf.reduce_mean(powered_diff)\n",
    "    return loss\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class DynamicExponentCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, initial_exponent, increment, update_frequency):\n",
    "        super().__init__()\n",
    "        self.exponent = initial_exponent\n",
    "        self.increment = increment\n",
    "        self.update_frequency = update_frequency\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.update_frequency == 0:\n",
    "            self.exponent += self.increment\n",
    "            print(f\"\\nEpoch {epoch + 1}: Increasing exponent to {self.exponent}\")\n",
    "            self.model.loss = self.custom_loss(self.exponent)\n",
    "\n",
    "    def custom_loss(self, exponent):\n",
    "        def loss(y_true, y_pred):\n",
    "            diff = tf.abs(y_true - y_pred)\n",
    "            powered_diff = tf.pow(diff, exponent)\n",
    "            return tf.reduce_mean(powered_diff)\n",
    "        return loss\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'initial_exponent': self.exponent,\n",
    "            'increment': self.increment,\n",
    "            'update_frequency': self.update_frequency,\n",
    "        })\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my NEW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, regularizers\n",
    "import tensorflow as tf\n",
    "\n",
    "class ModelBuilder:\n",
    "    def __init__(self, input_shape=(32, 32, 1), num_classes=5, num_coordinates=2, learning_rate=1e-3, weights_path=None, l1_reg=0.001, l2_reg=0.007):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.num_coordinates = num_coordinates\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l1_reg = l1_reg\n",
    "        self.l2_reg = l2_reg\n",
    "        self.model = self.build_model()\n",
    "\n",
    "        # Load weights if a path is provided\n",
    "        if weights_path is not None:\n",
    "            self.model.load_weights(weights_path)\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "\n",
    "    def build_model(self):\n",
    "        \n",
    "        l1 = regularizers.l1(self.l1_reg)\n",
    "        l2 = regularizers.l2(self.l2_reg)\n",
    "        \n",
    "    \n",
    "        x_input = layers.Input(shape=self.input_shape)\n",
    "        \n",
    "        x = layers.Conv2D(512, kernel_size=19, padding='same', activation='relu')(x_input)\n",
    "        \n",
    "        \n",
    "        x = layers.Flatten()(x)\n",
    "        \n",
    "        x_midpoints = layers.Dense(self.num_classes * self.num_coordinates, activation='sigmoid', name='x_midpoints')(x)\n",
    "        x_midpoints_reshape = layers.Reshape((-1, self.num_classes, self.num_coordinates), name='x_midpoints_reshape')(x_midpoints)\n",
    "        \n",
    "        return tf.keras.models.Model(x_input, x_midpoints_reshape)\n",
    "\n",
    "    def compile_model(self, loss_function):\n",
    "        self.model.compile(optimizer=self.optimizer, loss=loss_function)\n",
    "\n",
    "    def train_model(self, train_dataset, val_dataset, epochs, callbacks_list):\n",
    "        history = self.model.fit(train_dataset, epochs=epochs, validation_data=val_dataset, callbacks=callbacks_list)\n",
    "        return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Main Script Execution\n",
    "# -----------------------------\n",
    "\n",
    "# Load data\n",
    "# h5_filename = '/home/da886/Final Electron counting project/Images and Labels/30KFixed_Mixed_13.h5'             \n",
    "h5_filename = '/home/da886/Final Electron counting project/Images and Labels/13KFixed_Mixed_5_32by32_95indexFor19KernelNoNoise.h5'\n",
    "# h5_filename = '/home/da886/Final Electron counting project/Images and Labels/30KFixed_13_SparsespotsRandomIndex.h5'\n",
    "# h5_filename ='/home/da886/Final Electron counting project/Images and Labels/30KFixed-index6_13.h5'\n",
    "data_loader = DataLoader(h5_filename)\n",
    "images, centers = data_loader.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<matplotlib.image.AxesImage at 0x7f9dd45cf2c0>, 4088)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "l=np.random.randint(0,len(centers))\n",
    "data_loader.plot_image_with_centers(l)\n",
    "plt.imshow(images[l]),l\n",
    "# plt.grid(True),l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<matplotlib.image.AxesImage at 0x7f2fb41ad250>, 10273)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[l]),l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., 22.,  5.],\n",
       "       [ 1.,  5., 11.],\n",
       "       [ 1.,  1., 27.],\n",
       "       [ 1., 14., 25.],\n",
       "       [ 1., 29., 13.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.random.randint(0,len(centers))\n",
    "centers[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalize the data and split it into training and validation sets\n",
    "train_images, val_images, train_midpoints, val_midpoints = data_loader.split_data()\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_midpoints))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_midpoints))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.0, 1.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(train_images), np.max(train_images), np.min(train_midpoints), np.max(train_midpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images: (10400, 32, 32), Train Midpoints: (10400, 1, 5, 2)\n",
      "Validation Images: (2600, 32, 32), Validation Midpoints: (2600, 1, 5, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 1000\n",
    "train_dataset = train_dataset.shuffle(buffer_size=8000, reshuffle_each_iteration=True).batch(batch_size)\n",
    "val_dataset = val_dataset.shuffle(buffer_size=8000).batch(batch_size)\n",
    "\n",
    "# Check the shape of the datasets\n",
    "print(f'Train Images: {train_images.shape}, Train Midpoints: {train_midpoints.shape}')\n",
    "print(f'Validation Images: {val_images.shape}, Validation Midpoints: {val_midpoints.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_transposed_images_with_midpoints(train_dataset, image_indices=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set up learning rate scheduler\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.9, patience=10, verbose=1, mode='min', min_lr=5e-6\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">185,344</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">524288</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ x_midpoints (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,242,890</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ x_midpoints_reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │       \u001b[38;5;34m185,344\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m524288\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ x_midpoints (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │     \u001b[38;5;34m5,242,890\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ x_midpoints_reshape (\u001b[38;5;33mReshape\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m2\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,428,234</span> (20.71 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,428,234\u001b[0m (20.71 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,428,234</span> (20.71 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,428,234\u001b[0m (20.71 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# dynamic_exponent_callback = DynamicExponentCallback(2, 1, 400)\n",
    "\n",
    "\n",
    "with strategy.scope():\n",
    "    model_builder = ModelBuilder()\n",
    "\n",
    "    model_builder.build_model()\n",
    "\n",
    "    model_builder.model.summary()\n",
    "\n",
    "    model_builder.compile_model(loss_function=tf.keras.losses.MeanSquaredError()) \n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:20:02.041632: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2024-10-23 20:20:03.918175: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1729714803.985210  386753 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714804.000593  386756 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714804.005350  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714804.012768  386753 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714804.020272  386756 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714804.027797  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714804.035351  386753 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714804.051757  386756 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714804.068050  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714804.084278  386753 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714804.099271  386756 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714804.114150  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714804.128995  386753 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714804.156124  386756 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714804.183522  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714804.210393  386753 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714804.249489  386756 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714804.286540  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714804.323641  386753 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714804.343217  386756 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714804.353964  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714804.364421  386753 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714804.390753  386756 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714804.398951  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714804.406964  386753 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714804.468214  386756 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714804.526748  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714804.584459  386753 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714804.641477  386756 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714804.689237  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714805.048533  386756 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714805.054828  386757 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714805.067058  386756 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714805.073215  386757 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714805.079548  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714805.085978  386756 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714805.091458  386757 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714805.096772  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714805.103090  386756 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714805.109991  386757 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714805.116720  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714805.122101  386757 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714805.130377  386756 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714805.138635  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714805.145342  386757 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714805.154659  386756 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714805.163887  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714805.172097  386757 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714805.181749  386756 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714805.190985  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714805.200144  386757 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714805.348624  386756 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714805.493746  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714805.508461  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0788"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:20:06.275664: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2024-10-23 20:20:06.275792: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "W0000 00:00:1729714806.286960  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.289280  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.292597  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.299495  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.312487  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.318763  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.329906  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.345019  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.349467  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.352851  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.376627  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.399470  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.402039  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.405180  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.411889  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.416025  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.422121  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.433005  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.447922  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.452304  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.455628  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.478799  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.518600  386751 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.522013  386742 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.525553  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.529172  386742 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.532171  386751 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.535112  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.538118  386742 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.540656  386751 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.543207  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.545853  386742 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.548866  386751 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.552012  386742 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.555731  386751 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.559555  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.562576  386742 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.566586  386751 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.570622  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.574424  386742 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.578277  386751 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.582154  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.586159  386742 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.643974  386751 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0780"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1729714806.702767  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714806.709650  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.171308  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.184079  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.192797  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.201371  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.210582  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.219184  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.228471  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.238028  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.247112  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.256174  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.268144  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.278319  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.288531  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.298863  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.309217  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.319794  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.331820  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.343854  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.355839  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.367387  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.378521  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.389771  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.401460  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.412864  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.424537  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.440031  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.455279  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.470530  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.488031  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.505347  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.522750  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.537135  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.551442  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.565598  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.585710  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.605630  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.625648  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.639955  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.654252  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.668674  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.685446  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.702289  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.719133  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.734720  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.750613  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.766275  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.788245  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.810429  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.832561  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.862452  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.892508  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.922260  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714807.982307  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.042656  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.102837  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.106997  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.111091  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.115208  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.122049  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.128906  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.154913  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.161893  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.187926  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.225627  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.251706  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.289222  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.348153  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.387291  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.445631  386749 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.454686  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.511999  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.524876  386754 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "2024-10-23 20:20:08.811401: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "W0000 00:00:1729714808.844116  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.849647  386744 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.855429  386743 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.861177  386744 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.866599  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.871817  386744 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.877398  386743 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.882676  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.888342  386744 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.894763  386743 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.900424  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.906817  386744 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.913122  386743 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.919505  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.925862  386744 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.932772  386743 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.938945  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.945928  386744 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.952545  386743 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.959625  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.966373  386744 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.973557  386743 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.980255  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.987557  386744 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714808.996921  386743 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.004265  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.013663  386744 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.024404  386743 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.033787  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.044428  386744 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.053374  386743 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.063840  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.072699  386744 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.085215  386743 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.094208  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.106586  386744 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.115402  386743 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.127706  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.136547  386743 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.145389  386744 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.155583  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.165937  386743 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.176291  386744 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.186126  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.195691  386743 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.205217  386744 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.218501  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.231849  386743 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.245321  386744 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.263504  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.281782  386744 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.318070  386743 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.336062  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.372509  386744 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.375111  386743 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.411372  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.414137  386743 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.416757  386744 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.421099  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.425447  386743 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.429594  386744 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.445761  386743 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.461654  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.477556  386744 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.500523  386743 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.523382  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.546292  386744 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.581060  386743 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.617196  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 409ms/step - loss: 0.0771 - val_loss: 0.0500 - learning_rate: 0.0010\n",
      "Epoch 2/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1729714809.653715  386744 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.659480  386743 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729714809.665200  386752 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0440"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:20:10.649140: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0434 - val_loss: 0.0335 - learning_rate: 0.0010\n",
      "Epoch 3/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0308 - val_loss: 0.0285 - learning_rate: 0.0010\n",
      "Epoch 4/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:20:12.008718: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0265 - val_loss: 0.0274 - learning_rate: 0.0010\n",
      "Epoch 5/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0246 - val_loss: 0.0267 - learning_rate: 0.0010\n",
      "Epoch 6/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0233"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:20:15.264491: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0233 - val_loss: 0.0240 - learning_rate: 0.0010\n",
      "Epoch 7/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0224 - val_loss: 0.0244 - learning_rate: 0.0010\n",
      "Epoch 8/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0213 - val_loss: 0.0239 - learning_rate: 0.0010\n",
      "Epoch 9/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0206 - val_loss: 0.0229 - learning_rate: 0.0010\n",
      "Epoch 10/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0198 - val_loss: 0.0240 - learning_rate: 0.0010\n",
      "Epoch 11/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0193 - val_loss: 0.0228 - learning_rate: 0.0010\n",
      "Epoch 12/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:20:21.293172: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0185 - val_loss: 0.0232 - learning_rate: 0.0010\n",
      "Epoch 13/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - loss: 0.0180 - val_loss: 0.0228 - learning_rate: 0.0010\n",
      "Epoch 14/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - loss: 0.0179 - val_loss: 0.0230 - learning_rate: 0.0010\n",
      "Epoch 15/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - loss: 0.0172 - val_loss: 0.0225 - learning_rate: 0.0010\n",
      "Epoch 16/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0170 - val_loss: 0.0230 - learning_rate: 0.0010\n",
      "Epoch 17/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0164 - val_loss: 0.0214 - learning_rate: 0.0010\n",
      "Epoch 18/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0163 - val_loss: 0.0234 - learning_rate: 0.0010\n",
      "Epoch 19/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - loss: 0.0156 - val_loss: 0.0217 - learning_rate: 0.0010\n",
      "Epoch 20/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0153 - val_loss: 0.0211 - learning_rate: 0.0010\n",
      "Epoch 21/2000\n",
      "\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 257ms/step - loss: 0.0148"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:20:32.049989: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0148 - val_loss: 0.0215 - learning_rate: 0.0010\n",
      "Epoch 22/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0143"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:20:33.965224: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0143 - val_loss: 0.0221 - learning_rate: 0.0010\n",
      "Epoch 23/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0140 - val_loss: 0.0225 - learning_rate: 0.0010\n",
      "Epoch 24/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0137 - val_loss: 0.0224 - learning_rate: 0.0010\n",
      "Epoch 25/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0134 - val_loss: 0.0212 - learning_rate: 0.0010\n",
      "Epoch 26/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 0.0127 - val_loss: 0.0219 - learning_rate: 0.0010\n",
      "Epoch 27/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0123 - val_loss: 0.0224 - learning_rate: 0.0010\n",
      "Epoch 28/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - loss: 0.0119 - val_loss: 0.0218 - learning_rate: 0.0010\n",
      "Epoch 29/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0114 - val_loss: 0.0229 - learning_rate: 0.0010\n",
      "Epoch 30/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0111\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0112 - val_loss: 0.0220 - learning_rate: 0.0010\n",
      "Epoch 31/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0107 - val_loss: 0.0222 - learning_rate: 9.0000e-04\n",
      "Epoch 32/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0103 - val_loss: 0.0224 - learning_rate: 9.0000e-04\n",
      "Epoch 33/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0100 - val_loss: 0.0225 - learning_rate: 9.0000e-04\n",
      "Epoch 34/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 0.0095 - val_loss: 0.0216 - learning_rate: 9.0000e-04\n",
      "Epoch 35/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0093 - val_loss: 0.0224 - learning_rate: 9.0000e-04\n",
      "Epoch 36/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0088 - val_loss: 0.0214 - learning_rate: 9.0000e-04\n",
      "Epoch 37/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0084 - val_loss: 0.0225 - learning_rate: 9.0000e-04\n",
      "Epoch 38/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0078 - val_loss: 0.0219 - learning_rate: 9.0000e-04\n",
      "Epoch 39/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0077 - val_loss: 0.0223 - learning_rate: 9.0000e-04\n",
      "Epoch 40/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0071\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.0008100000384729356.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0071 - val_loss: 0.0219 - learning_rate: 9.0000e-04\n",
      "Epoch 41/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0067 - val_loss: 0.0221 - learning_rate: 8.1000e-04\n",
      "Epoch 42/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0063 - val_loss: 0.0218 - learning_rate: 8.1000e-04\n",
      "Epoch 43/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0060 - val_loss: 0.0220 - learning_rate: 8.1000e-04\n",
      "Epoch 44/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:20:58.832043: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0056 - val_loss: 0.0213 - learning_rate: 8.1000e-04\n",
      "Epoch 45/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0056 - val_loss: 0.0227 - learning_rate: 8.1000e-04\n",
      "Epoch 46/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0053"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:21:02.242390: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0053 - val_loss: 0.0225 - learning_rate: 8.1000e-04\n",
      "Epoch 47/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0051 - val_loss: 0.0230 - learning_rate: 8.1000e-04\n",
      "Epoch 48/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0048 - val_loss: 0.0223 - learning_rate: 8.1000e-04\n",
      "Epoch 49/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0045 - val_loss: 0.0220 - learning_rate: 8.1000e-04\n",
      "Epoch 50/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 0.0047 - val_loss: 0.0210 - learning_rate: 8.1000e-04\n",
      "Epoch 51/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0044 - val_loss: 0.0212 - learning_rate: 8.1000e-04\n",
      "Epoch 52/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0039 - val_loss: 0.0239 - learning_rate: 8.1000e-04\n",
      "Epoch 53/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0037 - val_loss: 0.0219 - learning_rate: 8.1000e-04\n",
      "Epoch 54/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0035 - val_loss: 0.0218 - learning_rate: 8.1000e-04\n",
      "Epoch 55/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0035 - val_loss: 0.0226 - learning_rate: 8.1000e-04\n",
      "Epoch 56/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0034 - val_loss: 0.0216 - learning_rate: 8.1000e-04\n",
      "Epoch 57/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0031 - val_loss: 0.0221 - learning_rate: 8.1000e-04\n",
      "Epoch 58/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0029 - val_loss: 0.0228 - learning_rate: 8.1000e-04\n",
      "Epoch 59/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0027 - val_loss: 0.0224 - learning_rate: 8.1000e-04\n",
      "Epoch 60/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0031\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 0.0007290000503417104.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0030 - val_loss: 0.0225 - learning_rate: 8.1000e-04\n",
      "Epoch 61/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0028 - val_loss: 0.0229 - learning_rate: 7.2900e-04\n",
      "Epoch 62/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 0.0023 - val_loss: 0.0228 - learning_rate: 7.2900e-04\n",
      "Epoch 63/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0022 - val_loss: 0.0220 - learning_rate: 7.2900e-04\n",
      "Epoch 64/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0021 - val_loss: 0.0225 - learning_rate: 7.2900e-04\n",
      "Epoch 65/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 0.0020 - val_loss: 0.0223 - learning_rate: 7.2900e-04\n",
      "Epoch 66/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0019 - val_loss: 0.0219 - learning_rate: 7.2900e-04\n",
      "Epoch 67/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0018 - val_loss: 0.0211 - learning_rate: 7.2900e-04\n",
      "Epoch 68/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 0.0017 - val_loss: 0.0214 - learning_rate: 7.2900e-04\n",
      "Epoch 69/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0019 - val_loss: 0.0225 - learning_rate: 7.2900e-04\n",
      "Epoch 70/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0017\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 0.0006561000715009868.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0017 - val_loss: 0.0225 - learning_rate: 7.2900e-04\n",
      "Epoch 71/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0016 - val_loss: 0.0235 - learning_rate: 6.5610e-04\n",
      "Epoch 72/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0015"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:21:33.043322: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0015 - val_loss: 0.0216 - learning_rate: 6.5610e-04\n",
      "Epoch 73/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0015 - val_loss: 0.0226 - learning_rate: 6.5610e-04\n",
      "Epoch 74/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0014 - val_loss: 0.0230 - learning_rate: 6.5610e-04\n",
      "Epoch 75/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0014 - val_loss: 0.0225 - learning_rate: 6.5610e-04\n",
      "Epoch 76/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0012 - val_loss: 0.0227 - learning_rate: 6.5610e-04\n",
      "Epoch 77/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0013 - val_loss: 0.0217 - learning_rate: 6.5610e-04\n",
      "Epoch 78/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0011 - val_loss: 0.0218 - learning_rate: 6.5610e-04\n",
      "Epoch 79/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0011 - val_loss: 0.0223 - learning_rate: 6.5610e-04\n",
      "Epoch 80/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0011\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 0.0005904900433961303.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0011 - val_loss: 0.0224 - learning_rate: 6.5610e-04\n",
      "Epoch 81/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 0.0011 - val_loss: 0.0231 - learning_rate: 5.9049e-04\n",
      "Epoch 82/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0010 - val_loss: 0.0225 - learning_rate: 5.9049e-04\n",
      "Epoch 83/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0010 - val_loss: 0.0236 - learning_rate: 5.9049e-04\n",
      "Epoch 84/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.4934e-04 - val_loss: 0.0224 - learning_rate: 5.9049e-04\n",
      "Epoch 85/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 9.3349e-04 - val_loss: 0.0229 - learning_rate: 5.9049e-04\n",
      "Epoch 86/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.0838e-04 - val_loss: 0.0227 - learning_rate: 5.9049e-04\n",
      "Epoch 87/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:21:49.736962: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 8.7975e-04 - val_loss: 0.0229 - learning_rate: 5.9049e-04\n",
      "Epoch 88/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 8.6377e-04 - val_loss: 0.0229 - learning_rate: 5.9049e-04\n",
      "Epoch 89/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 8.3943e-04 - val_loss: 0.0228 - learning_rate: 5.9049e-04\n",
      "Epoch 90/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 7.9098e-04\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 0.0005314410547725857.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 8.0060e-04 - val_loss: 0.0223 - learning_rate: 5.9049e-04\n",
      "Epoch 91/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 7.8144e-04 - val_loss: 0.0232 - learning_rate: 5.3144e-04\n",
      "Epoch 92/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.2906e-04 - val_loss: 0.0231 - learning_rate: 5.3144e-04\n",
      "Epoch 93/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 7.0198e-04 - val_loss: 0.0237 - learning_rate: 5.3144e-04\n",
      "Epoch 94/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.6905e-04 - val_loss: 0.0232 - learning_rate: 5.3144e-04\n",
      "Epoch 95/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.5784e-04 - val_loss: 0.0225 - learning_rate: 5.3144e-04\n",
      "Epoch 96/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.6513e-04 - val_loss: 0.0230 - learning_rate: 5.3144e-04\n",
      "Epoch 97/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.4671e-04 - val_loss: 0.0243 - learning_rate: 5.3144e-04\n",
      "Epoch 98/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:22:03.093870: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.7286e-04 - val_loss: 0.0227 - learning_rate: 5.3144e-04\n",
      "Epoch 99/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 6.1397e-04 - val_loss: 0.0221 - learning_rate: 5.3144e-04\n",
      "Epoch 100/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 6.0551e-04\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 0.00047829695977270604.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.0528e-04 - val_loss: 0.0224 - learning_rate: 5.3144e-04\n",
      "Epoch 101/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 5.9003e-04 - val_loss: 0.0236 - learning_rate: 4.7830e-04\n",
      "Epoch 102/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.4687e-04 - val_loss: 0.0229 - learning_rate: 4.7830e-04\n",
      "Epoch 103/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.4148e-04 - val_loss: 0.0230 - learning_rate: 4.7830e-04\n",
      "Epoch 104/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.2985e-04 - val_loss: 0.0229 - learning_rate: 4.7830e-04\n",
      "Epoch 105/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.1103e-04 - val_loss: 0.0232 - learning_rate: 4.7830e-04\n",
      "Epoch 106/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.0742e-04 - val_loss: 0.0242 - learning_rate: 4.7830e-04\n",
      "Epoch 107/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.0782e-04 - val_loss: 0.0243 - learning_rate: 4.7830e-04\n",
      "Epoch 108/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.8876e-04 - val_loss: 0.0231 - learning_rate: 4.7830e-04\n",
      "Epoch 109/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.0491e-04 - val_loss: 0.0230 - learning_rate: 4.7830e-04\n",
      "Epoch 110/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 5.0433e-04\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 0.0004304672533180565.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.0130e-04 - val_loss: 0.0234 - learning_rate: 4.7830e-04\n",
      "Epoch 111/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.6510e-04 - val_loss: 0.0234 - learning_rate: 4.3047e-04\n",
      "Epoch 112/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.3681e-04 - val_loss: 0.0226 - learning_rate: 4.3047e-04\n",
      "Epoch 113/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 4.3675e-04 - val_loss: 0.0237 - learning_rate: 4.3047e-04\n",
      "Epoch 114/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 4.2676e-04 - val_loss: 0.0237 - learning_rate: 4.3047e-04\n",
      "Epoch 115/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.2033e-04 - val_loss: 0.0233 - learning_rate: 4.3047e-04\n",
      "Epoch 116/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.1196e-04 - val_loss: 0.0245 - learning_rate: 4.3047e-04\n",
      "Epoch 117/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.1031e-04 - val_loss: 0.0236 - learning_rate: 4.3047e-04\n",
      "Epoch 118/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.9627e-04 - val_loss: 0.0233 - learning_rate: 4.3047e-04\n",
      "Epoch 119/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.8372e-04 - val_loss: 0.0232 - learning_rate: 4.3047e-04\n",
      "Epoch 120/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 3.6183e-04\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 0.00038742052274756136.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.6398e-04 - val_loss: 0.0236 - learning_rate: 4.3047e-04\n",
      "Epoch 121/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.6922e-04 - val_loss: 0.0231 - learning_rate: 3.8742e-04\n",
      "Epoch 122/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.5284e-04 - val_loss: 0.0239 - learning_rate: 3.8742e-04\n",
      "Epoch 123/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 3.5008e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:22:33.461684: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.5041e-04 - val_loss: 0.0233 - learning_rate: 3.8742e-04\n",
      "Epoch 124/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.4728e-04 - val_loss: 0.0235 - learning_rate: 3.8742e-04\n",
      "Epoch 125/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.4412e-04 - val_loss: 0.0234 - learning_rate: 3.8742e-04\n",
      "Epoch 126/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.4144e-04 - val_loss: 0.0228 - learning_rate: 3.8742e-04\n",
      "Epoch 127/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.3043e-04 - val_loss: 0.0241 - learning_rate: 3.8742e-04\n",
      "Epoch 128/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.3325e-04 - val_loss: 0.0239 - learning_rate: 3.8742e-04\n",
      "Epoch 129/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 3.2604e-04 - val_loss: 0.0229 - learning_rate: 3.8742e-04\n",
      "Epoch 130/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 3.0966e-04\n",
      "Epoch 130: ReduceLROnPlateau reducing learning rate to 0.0003486784757114947.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.1047e-04 - val_loss: 0.0245 - learning_rate: 3.8742e-04\n",
      "Epoch 131/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 3.1319e-04 - val_loss: 0.0236 - learning_rate: 3.4868e-04\n",
      "Epoch 132/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step - loss: 3.0227e-04 - val_loss: 0.0242 - learning_rate: 3.4868e-04\n",
      "Epoch 133/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9108e-04 - val_loss: 0.0239 - learning_rate: 3.4868e-04\n",
      "Epoch 134/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9131e-04 - val_loss: 0.0227 - learning_rate: 3.4868e-04\n",
      "Epoch 135/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.8556e-04 - val_loss: 0.0241 - learning_rate: 3.4868e-04\n",
      "Epoch 136/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9123e-04 - val_loss: 0.0232 - learning_rate: 3.4868e-04\n",
      "Epoch 137/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.7929e-04 - val_loss: 0.0249 - learning_rate: 3.4868e-04\n",
      "Epoch 138/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6856e-04 - val_loss: 0.0234 - learning_rate: 3.4868e-04\n",
      "Epoch 139/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.7560e-04 - val_loss: 0.0235 - learning_rate: 3.4868e-04\n",
      "Epoch 140/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 2.7290e-04\n",
      "Epoch 140: ReduceLROnPlateau reducing learning rate to 0.00031381062290165574.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.7162e-04 - val_loss: 0.0234 - learning_rate: 3.4868e-04\n",
      "Epoch 141/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.5786e-04 - val_loss: 0.0228 - learning_rate: 3.1381e-04\n",
      "Epoch 142/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5124e-04 - val_loss: 0.0232 - learning_rate: 3.1381e-04\n",
      "Epoch 143/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.5089e-04 - val_loss: 0.0241 - learning_rate: 3.1381e-04\n",
      "Epoch 144/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.5292e-04 - val_loss: 0.0222 - learning_rate: 3.1381e-04\n",
      "Epoch 145/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4446e-04 - val_loss: 0.0235 - learning_rate: 3.1381e-04\n",
      "Epoch 146/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.4425e-04 - val_loss: 0.0224 - learning_rate: 3.1381e-04\n",
      "Epoch 147/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 2.4360e-04 - val_loss: 0.0237 - learning_rate: 3.1381e-04\n",
      "Epoch 148/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.4131e-04 - val_loss: 0.0232 - learning_rate: 3.1381e-04\n",
      "Epoch 149/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:23:03.629939: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3602e-04 - val_loss: 0.0236 - learning_rate: 3.1381e-04\n",
      "Epoch 150/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 2.3643e-04\n",
      "Epoch 150: ReduceLROnPlateau reducing learning rate to 0.0002824295632308349.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3489e-04 - val_loss: 0.0233 - learning_rate: 3.1381e-04\n",
      "Epoch 151/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.2755e-04 - val_loss: 0.0250 - learning_rate: 2.8243e-04\n",
      "Epoch 152/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.2415e-04 - val_loss: 0.0226 - learning_rate: 2.8243e-04\n",
      "Epoch 153/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.2112e-04 - val_loss: 0.0232 - learning_rate: 2.8243e-04\n",
      "Epoch 154/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.2158e-04 - val_loss: 0.0230 - learning_rate: 2.8243e-04\n",
      "Epoch 155/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 2.1878e-04 - val_loss: 0.0244 - learning_rate: 2.8243e-04\n",
      "Epoch 156/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.1690e-04 - val_loss: 0.0244 - learning_rate: 2.8243e-04\n",
      "Epoch 157/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.1098e-04 - val_loss: 0.0240 - learning_rate: 2.8243e-04\n",
      "Epoch 158/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.1381e-04 - val_loss: 0.0240 - learning_rate: 2.8243e-04\n",
      "Epoch 159/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.0449e-04 - val_loss: 0.0242 - learning_rate: 2.8243e-04\n",
      "Epoch 160/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 2.0811e-04\n",
      "Epoch 160: ReduceLROnPlateau reducing learning rate to 0.00025418660952709616.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.0773e-04 - val_loss: 0.0232 - learning_rate: 2.8243e-04\n",
      "Epoch 161/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.0187e-04 - val_loss: 0.0242 - learning_rate: 2.5419e-04\n",
      "Epoch 162/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.9938e-04 - val_loss: 0.0232 - learning_rate: 2.5419e-04\n",
      "Epoch 163/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 1.9760e-04 - val_loss: 0.0242 - learning_rate: 2.5419e-04\n",
      "Epoch 164/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.9359e-04 - val_loss: 0.0240 - learning_rate: 2.5419e-04\n",
      "Epoch 165/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.9400e-04 - val_loss: 0.0242 - learning_rate: 2.5419e-04\n",
      "Epoch 166/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.9381e-04 - val_loss: 0.0243 - learning_rate: 2.5419e-04\n",
      "Epoch 167/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.8713e-04 - val_loss: 0.0234 - learning_rate: 2.5419e-04\n",
      "Epoch 168/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.9242e-04 - val_loss: 0.0235 - learning_rate: 2.5419e-04\n",
      "Epoch 169/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.8593e-04 - val_loss: 0.0231 - learning_rate: 2.5419e-04\n",
      "Epoch 170/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 1.8522e-04\n",
      "Epoch 170: ReduceLROnPlateau reducing learning rate to 0.00022876793809700757.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 121ms/step - loss: 1.8433e-04 - val_loss: 0.0234 - learning_rate: 2.5419e-04\n",
      "Epoch 171/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.8146e-04 - val_loss: 0.0234 - learning_rate: 2.2877e-04\n",
      "Epoch 172/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 1.8102e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:23:31.960395: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.8077e-04 - val_loss: 0.0237 - learning_rate: 2.2877e-04\n",
      "Epoch 173/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.7658e-04 - val_loss: 0.0244 - learning_rate: 2.2877e-04\n",
      "Epoch 174/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 1.7172e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:23:34.420506: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.7187e-04 - val_loss: 0.0233 - learning_rate: 2.2877e-04\n",
      "Epoch 175/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.7568e-04 - val_loss: 0.0222 - learning_rate: 2.2877e-04\n",
      "Epoch 176/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.6930e-04 - val_loss: 0.0234 - learning_rate: 2.2877e-04\n",
      "Epoch 177/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.6956e-04 - val_loss: 0.0246 - learning_rate: 2.2877e-04\n",
      "Epoch 178/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.6848e-04 - val_loss: 0.0238 - learning_rate: 2.2877e-04\n",
      "Epoch 179/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.6760e-04 - val_loss: 0.0239 - learning_rate: 2.2877e-04\n",
      "Epoch 180/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 1.6508e-04\n",
      "Epoch 180: ReduceLROnPlateau reducing learning rate to 0.00020589114428730683.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.6471e-04 - val_loss: 0.0241 - learning_rate: 2.2877e-04\n",
      "Epoch 181/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.6333e-04 - val_loss: 0.0240 - learning_rate: 2.0589e-04\n",
      "Epoch 182/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 1.6151e-04 - val_loss: 0.0238 - learning_rate: 2.0589e-04\n",
      "Epoch 183/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.6760e-04 - val_loss: 0.0242 - learning_rate: 2.0589e-04\n",
      "Epoch 184/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.6079e-04 - val_loss: 0.0233 - learning_rate: 2.0589e-04\n",
      "Epoch 185/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.5866e-04 - val_loss: 0.0234 - learning_rate: 2.0589e-04\n",
      "Epoch 186/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 1.5228e-04 - val_loss: 0.0237 - learning_rate: 2.0589e-04\n",
      "Epoch 187/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.5643e-04 - val_loss: 0.0234 - learning_rate: 2.0589e-04\n",
      "Epoch 188/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.5717e-04 - val_loss: 0.0251 - learning_rate: 2.0589e-04\n",
      "Epoch 189/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.5524e-04 - val_loss: 0.0240 - learning_rate: 2.0589e-04\n",
      "Epoch 190/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 1.5216e-04\n",
      "Epoch 190: ReduceLROnPlateau reducing learning rate to 0.00018530203378759326.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.5244e-04 - val_loss: 0.0236 - learning_rate: 2.0589e-04\n",
      "Epoch 191/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 1.5152e-04 - val_loss: 0.0251 - learning_rate: 1.8530e-04\n",
      "Epoch 192/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 1.4794e-04 - val_loss: 0.0241 - learning_rate: 1.8530e-04\n",
      "Epoch 193/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.4683e-04 - val_loss: 0.0250 - learning_rate: 1.8530e-04\n",
      "Epoch 194/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.4960e-04 - val_loss: 0.0233 - learning_rate: 1.8530e-04\n",
      "Epoch 195/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.4838e-04 - val_loss: 0.0240 - learning_rate: 1.8530e-04\n",
      "Epoch 196/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.4394e-04 - val_loss: 0.0248 - learning_rate: 1.8530e-04\n",
      "Epoch 197/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.4591e-04 - val_loss: 0.0236 - learning_rate: 1.8530e-04\n",
      "Epoch 198/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.4442e-04 - val_loss: 0.0237 - learning_rate: 1.8530e-04\n",
      "Epoch 199/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.4488e-04 - val_loss: 0.0251 - learning_rate: 1.8530e-04\n",
      "Epoch 200/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 1.4053e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:24:05.208809: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 200: ReduceLROnPlateau reducing learning rate to 0.00016677183302817866.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.4094e-04 - val_loss: 0.0244 - learning_rate: 1.8530e-04\n",
      "Epoch 201/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.3985e-04 - val_loss: 0.0244 - learning_rate: 1.6677e-04\n",
      "Epoch 202/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - loss: 1.3640e-04 - val_loss: 0.0240 - learning_rate: 1.6677e-04\n",
      "Epoch 203/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.4211e-04 - val_loss: 0.0241 - learning_rate: 1.6677e-04\n",
      "Epoch 204/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.3954e-04 - val_loss: 0.0235 - learning_rate: 1.6677e-04\n",
      "Epoch 205/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.3763e-04 - val_loss: 0.0235 - learning_rate: 1.6677e-04\n",
      "Epoch 206/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.3661e-04 - val_loss: 0.0244 - learning_rate: 1.6677e-04\n",
      "Epoch 207/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.3465e-04 - val_loss: 0.0240 - learning_rate: 1.6677e-04\n",
      "Epoch 208/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.3522e-04 - val_loss: 0.0248 - learning_rate: 1.6677e-04\n",
      "Epoch 209/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.3297e-04 - val_loss: 0.0238 - learning_rate: 1.6677e-04\n",
      "Epoch 210/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 1.3169e-04\n",
      "Epoch 210: ReduceLROnPlateau reducing learning rate to 0.00015009464841568844.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.3172e-04 - val_loss: 0.0233 - learning_rate: 1.6677e-04\n",
      "Epoch 211/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.3800e-04 - val_loss: 0.0248 - learning_rate: 1.5009e-04\n",
      "Epoch 212/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.2821e-04 - val_loss: 0.0249 - learning_rate: 1.5009e-04\n",
      "Epoch 213/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.3253e-04 - val_loss: 0.0245 - learning_rate: 1.5009e-04\n",
      "Epoch 214/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.2701e-04 - val_loss: 0.0245 - learning_rate: 1.5009e-04\n",
      "Epoch 215/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 1.2958e-04 - val_loss: 0.0250 - learning_rate: 1.5009e-04\n",
      "Epoch 216/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.2327e-04 - val_loss: 0.0242 - learning_rate: 1.5009e-04\n",
      "Epoch 217/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.2754e-04 - val_loss: 0.0234 - learning_rate: 1.5009e-04\n",
      "Epoch 218/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.2490e-04 - val_loss: 0.0238 - learning_rate: 1.5009e-04\n",
      "Epoch 219/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.2479e-04 - val_loss: 0.0238 - learning_rate: 1.5009e-04\n",
      "Epoch 220/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 1.2864e-04\n",
      "Epoch 220: ReduceLROnPlateau reducing learning rate to 0.0001350851875031367.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.2773e-04 - val_loss: 0.0240 - learning_rate: 1.5009e-04\n",
      "Epoch 221/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 1.2030e-04 - val_loss: 0.0247 - learning_rate: 1.3509e-04\n",
      "Epoch 222/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.2206e-04 - val_loss: 0.0228 - learning_rate: 1.3509e-04\n",
      "Epoch 223/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.1963e-04 - val_loss: 0.0241 - learning_rate: 1.3509e-04\n",
      "Epoch 224/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.1994e-04 - val_loss: 0.0243 - learning_rate: 1.3509e-04\n",
      "Epoch 225/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.2222e-04 - val_loss: 0.0241 - learning_rate: 1.3509e-04\n",
      "Epoch 226/2000\n",
      "\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 251ms/step - loss: 1.1221e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:24:35.348575: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.1918e-04 - val_loss: 0.0245 - learning_rate: 1.3509e-04\n",
      "Epoch 227/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.1635e-04 - val_loss: 0.0239 - learning_rate: 1.3509e-04\n",
      "Epoch 228/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.1901e-04 - val_loss: 0.0246 - learning_rate: 1.3509e-04\n",
      "Epoch 229/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.1484e-04 - val_loss: 0.0245 - learning_rate: 1.3509e-04\n",
      "Epoch 230/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 1.1689e-04\n",
      "Epoch 230: ReduceLROnPlateau reducing learning rate to 0.00012157666351413355.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.1668e-04 - val_loss: 0.0242 - learning_rate: 1.3509e-04\n",
      "Epoch 231/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.1477e-04 - val_loss: 0.0250 - learning_rate: 1.2158e-04\n",
      "Epoch 232/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 115ms/step - loss: 1.1632e-04 - val_loss: 0.0233 - learning_rate: 1.2158e-04\n",
      "Epoch 233/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 1.1589e-04 - val_loss: 0.0237 - learning_rate: 1.2158e-04\n",
      "Epoch 234/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.1404e-04 - val_loss: 0.0240 - learning_rate: 1.2158e-04\n",
      "Epoch 235/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.1222e-04 - val_loss: 0.0244 - learning_rate: 1.2158e-04\n",
      "Epoch 236/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.1286e-04 - val_loss: 0.0229 - learning_rate: 1.2158e-04\n",
      "Epoch 237/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.1121e-04 - val_loss: 0.0248 - learning_rate: 1.2158e-04\n",
      "Epoch 238/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.1227e-04 - val_loss: 0.0233 - learning_rate: 1.2158e-04\n",
      "Epoch 239/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.1075e-04 - val_loss: 0.0236 - learning_rate: 1.2158e-04\n",
      "Epoch 240/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 1.1144e-04\n",
      "Epoch 240: ReduceLROnPlateau reducing learning rate to 0.00010941899454337544.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.1105e-04 - val_loss: 0.0246 - learning_rate: 1.2158e-04\n",
      "Epoch 241/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.1024e-04 - val_loss: 0.0242 - learning_rate: 1.0942e-04\n",
      "Epoch 242/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 1.0902e-04 - val_loss: 0.0248 - learning_rate: 1.0942e-04\n",
      "Epoch 243/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.0887e-04 - val_loss: 0.0244 - learning_rate: 1.0942e-04\n",
      "Epoch 244/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.0861e-04 - val_loss: 0.0246 - learning_rate: 1.0942e-04\n",
      "Epoch 245/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.0924e-04 - val_loss: 0.0240 - learning_rate: 1.0942e-04\n",
      "Epoch 246/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.0930e-04 - val_loss: 0.0239 - learning_rate: 1.0942e-04\n",
      "Epoch 247/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.0741e-04 - val_loss: 0.0244 - learning_rate: 1.0942e-04\n",
      "Epoch 248/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.0742e-04 - val_loss: 0.0246 - learning_rate: 1.0942e-04\n",
      "Epoch 249/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.0447e-04 - val_loss: 0.0234 - learning_rate: 1.0942e-04\n",
      "Epoch 250/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 1.0408e-04\n",
      "Epoch 250: ReduceLROnPlateau reducing learning rate to 9.847709443420172e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.0383e-04 - val_loss: 0.0240 - learning_rate: 1.0942e-04\n",
      "Epoch 251/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 1.0812e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:25:06.200541: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 1.0742e-04 - val_loss: 0.0238 - learning_rate: 9.8477e-05\n",
      "Epoch 252/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.0569e-04 - val_loss: 0.0243 - learning_rate: 9.8477e-05\n",
      "Epoch 253/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 1.0398e-04 - val_loss: 0.0246 - learning_rate: 9.8477e-05\n",
      "Epoch 254/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 1.0235e-04 - val_loss: 0.0245 - learning_rate: 9.8477e-05\n",
      "Epoch 255/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.0392e-04 - val_loss: 0.0250 - learning_rate: 9.8477e-05\n",
      "Epoch 256/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.0345e-04 - val_loss: 0.0241 - learning_rate: 9.8477e-05\n",
      "Epoch 257/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.0052e-04 - val_loss: 0.0240 - learning_rate: 9.8477e-05\n",
      "Epoch 258/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 121ms/step - loss: 1.0400e-04 - val_loss: 0.0243 - learning_rate: 9.8477e-05\n",
      "Epoch 259/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 9.9760e-05 - val_loss: 0.0239 - learning_rate: 9.8477e-05\n",
      "Epoch 260/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 1.0166e-04\n",
      "Epoch 260: ReduceLROnPlateau reducing learning rate to 8.862938630045391e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.0145e-04 - val_loss: 0.0244 - learning_rate: 9.8477e-05\n",
      "Epoch 261/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.0340e-04 - val_loss: 0.0233 - learning_rate: 8.8629e-05\n",
      "Epoch 262/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.0100e-04 - val_loss: 0.0237 - learning_rate: 8.8629e-05\n",
      "Epoch 263/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 1.0171e-04 - val_loss: 0.0242 - learning_rate: 8.8629e-05\n",
      "Epoch 264/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 9.8964e-05 - val_loss: 0.0240 - learning_rate: 8.8629e-05\n",
      "Epoch 265/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.8820e-05 - val_loss: 0.0238 - learning_rate: 8.8629e-05\n",
      "Epoch 266/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 9.8461e-05 - val_loss: 0.0241 - learning_rate: 8.8629e-05\n",
      "Epoch 267/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 9.5574e-05 - val_loss: 0.0233 - learning_rate: 8.8629e-05\n",
      "Epoch 268/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 9.8208e-05 - val_loss: 0.0246 - learning_rate: 8.8629e-05\n",
      "Epoch 269/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.7649e-05 - val_loss: 0.0244 - learning_rate: 8.8629e-05\n",
      "Epoch 270/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 1.0075e-04\n",
      "Epoch 270: ReduceLROnPlateau reducing learning rate to 7.976644701557234e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.0000e-04 - val_loss: 0.0246 - learning_rate: 8.8629e-05\n",
      "Epoch 271/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 9.5838e-05 - val_loss: 0.0239 - learning_rate: 7.9766e-05\n",
      "Epoch 272/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 9.5596e-05 - val_loss: 0.0235 - learning_rate: 7.9766e-05\n",
      "Epoch 273/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 9.6798e-05 - val_loss: 0.0250 - learning_rate: 7.9766e-05\n",
      "Epoch 274/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.2997e-05 - val_loss: 0.0243 - learning_rate: 7.9766e-05\n",
      "Epoch 275/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.4655e-05 - val_loss: 0.0238 - learning_rate: 7.9766e-05\n",
      "Epoch 276/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 9.4292e-05 - val_loss: 0.0244 - learning_rate: 7.9766e-05\n",
      "Epoch 277/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:25:36.404169: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 9.7171e-05 - val_loss: 0.0243 - learning_rate: 7.9766e-05\n",
      "Epoch 278/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.5129e-05 - val_loss: 0.0252 - learning_rate: 7.9766e-05\n",
      "Epoch 279/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 9.3824e-05 - val_loss: 0.0245 - learning_rate: 7.9766e-05\n",
      "Epoch 280/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 9.3815e-05\n",
      "Epoch 280: ReduceLROnPlateau reducing learning rate to 7.178980231401511e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 9.3586e-05 - val_loss: 0.0244 - learning_rate: 7.9766e-05\n",
      "Epoch 281/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 9.4184e-05 - val_loss: 0.0241 - learning_rate: 7.1790e-05\n",
      "Epoch 282/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 9.4239e-05 - val_loss: 0.0241 - learning_rate: 7.1790e-05\n",
      "Epoch 283/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 9.2206e-05 - val_loss: 0.0239 - learning_rate: 7.1790e-05\n",
      "Epoch 284/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - loss: 8.9781e-05 - val_loss: 0.0244 - learning_rate: 7.1790e-05\n",
      "Epoch 285/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.1333e-05 - val_loss: 0.0243 - learning_rate: 7.1790e-05\n",
      "Epoch 286/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.3038e-05 - val_loss: 0.0247 - learning_rate: 7.1790e-05\n",
      "Epoch 287/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 9.2205e-05 - val_loss: 0.0251 - learning_rate: 7.1790e-05\n",
      "Epoch 288/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.1512e-05 - val_loss: 0.0236 - learning_rate: 7.1790e-05\n",
      "Epoch 289/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.1245e-05 - val_loss: 0.0244 - learning_rate: 7.1790e-05\n",
      "Epoch 290/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 8.9769e-05\n",
      "Epoch 290: ReduceLROnPlateau reducing learning rate to 6.461082011810504e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.9419e-05 - val_loss: 0.0235 - learning_rate: 7.1790e-05\n",
      "Epoch 291/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.8525e-05 - val_loss: 0.0244 - learning_rate: 6.4611e-05\n",
      "Epoch 292/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 8.9504e-05 - val_loss: 0.0246 - learning_rate: 6.4611e-05\n",
      "Epoch 293/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 8.9464e-05 - val_loss: 0.0244 - learning_rate: 6.4611e-05\n",
      "Epoch 294/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 8.8173e-05 - val_loss: 0.0250 - learning_rate: 6.4611e-05\n",
      "Epoch 295/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.8540e-05 - val_loss: 0.0244 - learning_rate: 6.4611e-05\n",
      "Epoch 296/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 8.8538e-05 - val_loss: 0.0245 - learning_rate: 6.4611e-05\n",
      "Epoch 297/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 8.6490e-05 - val_loss: 0.0232 - learning_rate: 6.4611e-05\n",
      "Epoch 298/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.8594e-05 - val_loss: 0.0248 - learning_rate: 6.4611e-05\n",
      "Epoch 299/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 8.6170e-05 - val_loss: 0.0254 - learning_rate: 6.4611e-05\n",
      "Epoch 300/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 8.9034e-05\n",
      "Epoch 300: ReduceLROnPlateau reducing learning rate to 5.8149741380475466e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 8.9356e-05 - val_loss: 0.0245 - learning_rate: 6.4611e-05\n",
      "Epoch 301/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 8.9605e-05 - val_loss: 0.0253 - learning_rate: 5.8150e-05\n",
      "Epoch 302/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:26:06.450721: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.7684e-05 - val_loss: 0.0251 - learning_rate: 5.8150e-05\n",
      "Epoch 303/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 8.7890e-05 - val_loss: 0.0246 - learning_rate: 5.8150e-05\n",
      "Epoch 304/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 8.4338e-05 - val_loss: 0.0236 - learning_rate: 5.8150e-05\n",
      "Epoch 305/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 8.5658e-05 - val_loss: 0.0250 - learning_rate: 5.8150e-05\n",
      "Epoch 306/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 8.5403e-05 - val_loss: 0.0233 - learning_rate: 5.8150e-05\n",
      "Epoch 307/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.6041e-05 - val_loss: 0.0253 - learning_rate: 5.8150e-05\n",
      "Epoch 308/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 8.4629e-05 - val_loss: 0.0235 - learning_rate: 5.8150e-05\n",
      "Epoch 309/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 8.5587e-05 - val_loss: 0.0243 - learning_rate: 5.8150e-05\n",
      "Epoch 310/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 8.6488e-05\n",
      "Epoch 310: ReduceLROnPlateau reducing learning rate to 5.233476658759173e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.5941e-05 - val_loss: 0.0245 - learning_rate: 5.8150e-05\n",
      "Epoch 311/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 8.8076e-05 - val_loss: 0.0243 - learning_rate: 5.2335e-05\n",
      "Epoch 312/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 8.4875e-05 - val_loss: 0.0245 - learning_rate: 5.2335e-05\n",
      "Epoch 313/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 8.3554e-05 - val_loss: 0.0240 - learning_rate: 5.2335e-05\n",
      "Epoch 314/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 8.5045e-05 - val_loss: 0.0237 - learning_rate: 5.2335e-05\n",
      "Epoch 315/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.6276e-05 - val_loss: 0.0251 - learning_rate: 5.2335e-05\n",
      "Epoch 316/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.1159e-05 - val_loss: 0.0240 - learning_rate: 5.2335e-05\n",
      "Epoch 317/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 8.2479e-05 - val_loss: 0.0255 - learning_rate: 5.2335e-05\n",
      "Epoch 318/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.4851e-05 - val_loss: 0.0227 - learning_rate: 5.2335e-05\n",
      "Epoch 319/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.4827e-05 - val_loss: 0.0259 - learning_rate: 5.2335e-05\n",
      "Epoch 320/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 8.2468e-05\n",
      "Epoch 320: ReduceLROnPlateau reducing learning rate to 4.7101289601414466e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 8.2471e-05 - val_loss: 0.0247 - learning_rate: 5.2335e-05\n",
      "Epoch 321/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.0401e-05 - val_loss: 0.0247 - learning_rate: 4.7101e-05\n",
      "Epoch 322/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.3166e-05 - val_loss: 0.0237 - learning_rate: 4.7101e-05\n",
      "Epoch 323/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.0335e-05 - val_loss: 0.0241 - learning_rate: 4.7101e-05\n",
      "Epoch 324/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.2618e-05 - val_loss: 0.0250 - learning_rate: 4.7101e-05\n",
      "Epoch 325/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 8.3185e-05 - val_loss: 0.0248 - learning_rate: 4.7101e-05\n",
      "Epoch 326/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 8.1170e-05 - val_loss: 0.0243 - learning_rate: 4.7101e-05\n",
      "Epoch 327/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:26:36.557862: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.8068e-05 - val_loss: 0.0241 - learning_rate: 4.7101e-05\n",
      "Epoch 328/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 8.2313e-05 - val_loss: 0.0248 - learning_rate: 4.7101e-05\n",
      "Epoch 329/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.0673e-05 - val_loss: 0.0236 - learning_rate: 4.7101e-05\n",
      "Epoch 330/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 8.0319e-05\n",
      "Epoch 330: ReduceLROnPlateau reducing learning rate to 4.239116096869111e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 8.0059e-05 - val_loss: 0.0254 - learning_rate: 4.7101e-05\n",
      "Epoch 331/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.0117e-05 - val_loss: 0.0243 - learning_rate: 4.2391e-05\n",
      "Epoch 332/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.9437e-05 - val_loss: 0.0238 - learning_rate: 4.2391e-05\n",
      "Epoch 333/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 8.0637e-05 - val_loss: 0.0245 - learning_rate: 4.2391e-05\n",
      "Epoch 334/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.8794e-05 - val_loss: 0.0244 - learning_rate: 4.2391e-05\n",
      "Epoch 335/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.9049e-05 - val_loss: 0.0245 - learning_rate: 4.2391e-05\n",
      "Epoch 336/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.1108e-05 - val_loss: 0.0242 - learning_rate: 4.2391e-05\n",
      "Epoch 337/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.9578e-05 - val_loss: 0.0237 - learning_rate: 4.2391e-05\n",
      "Epoch 338/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.7795e-05 - val_loss: 0.0239 - learning_rate: 4.2391e-05\n",
      "Epoch 339/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.9509e-05 - val_loss: 0.0237 - learning_rate: 4.2391e-05\n",
      "Epoch 340/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 7.9896e-05\n",
      "Epoch 340: ReduceLROnPlateau reducing learning rate to 3.815204618149437e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.9444e-05 - val_loss: 0.0244 - learning_rate: 4.2391e-05\n",
      "Epoch 341/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.8323e-05 - val_loss: 0.0248 - learning_rate: 3.8152e-05\n",
      "Epoch 342/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.9803e-05 - val_loss: 0.0252 - learning_rate: 3.8152e-05\n",
      "Epoch 343/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.8163e-05 - val_loss: 0.0246 - learning_rate: 3.8152e-05\n",
      "Epoch 344/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:26:56.552650: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.8738e-05 - val_loss: 0.0238 - learning_rate: 3.8152e-05\n",
      "Epoch 345/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 7.6960e-05 - val_loss: 0.0236 - learning_rate: 3.8152e-05\n",
      "Epoch 346/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 7.7896e-05 - val_loss: 0.0248 - learning_rate: 3.8152e-05\n",
      "Epoch 347/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.8248e-05 - val_loss: 0.0247 - learning_rate: 3.8152e-05\n",
      "Epoch 348/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.9422e-05 - val_loss: 0.0238 - learning_rate: 3.8152e-05\n",
      "Epoch 349/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.5960e-05 - val_loss: 0.0247 - learning_rate: 3.8152e-05\n",
      "Epoch 350/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 7.7895e-05\n",
      "Epoch 350: ReduceLROnPlateau reducing learning rate to 3.4336842873017304e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.7530e-05 - val_loss: 0.0238 - learning_rate: 3.8152e-05\n",
      "Epoch 351/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.4752e-05 - val_loss: 0.0246 - learning_rate: 3.4337e-05\n",
      "Epoch 352/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:27:06.665309: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.6271e-05 - val_loss: 0.0244 - learning_rate: 3.4337e-05\n",
      "Epoch 353/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.5628e-05 - val_loss: 0.0248 - learning_rate: 3.4337e-05\n",
      "Epoch 354/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.5239e-05 - val_loss: 0.0251 - learning_rate: 3.4337e-05\n",
      "Epoch 355/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 7.6827e-05 - val_loss: 0.0249 - learning_rate: 3.4337e-05\n",
      "Epoch 356/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.5867e-05 - val_loss: 0.0250 - learning_rate: 3.4337e-05\n",
      "Epoch 357/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.7300e-05 - val_loss: 0.0239 - learning_rate: 3.4337e-05\n",
      "Epoch 358/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.3021e-05 - val_loss: 0.0245 - learning_rate: 3.4337e-05\n",
      "Epoch 359/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.4733e-05 - val_loss: 0.0247 - learning_rate: 3.4337e-05\n",
      "Epoch 360/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 7.6431e-05\n",
      "Epoch 360: ReduceLROnPlateau reducing learning rate to 3.0903160222806036e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.5964e-05 - val_loss: 0.0245 - learning_rate: 3.4337e-05\n",
      "Epoch 361/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.6434e-05 - val_loss: 0.0247 - learning_rate: 3.0903e-05\n",
      "Epoch 362/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 7.5460e-05 - val_loss: 0.0237 - learning_rate: 3.0903e-05\n",
      "Epoch 363/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.4379e-05 - val_loss: 0.0248 - learning_rate: 3.0903e-05\n",
      "Epoch 364/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.5895e-05 - val_loss: 0.0247 - learning_rate: 3.0903e-05\n",
      "Epoch 365/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.2305e-05 - val_loss: 0.0245 - learning_rate: 3.0903e-05\n",
      "Epoch 366/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.3752e-05 - val_loss: 0.0246 - learning_rate: 3.0903e-05\n",
      "Epoch 367/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.5559e-05 - val_loss: 0.0242 - learning_rate: 3.0903e-05\n",
      "Epoch 368/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.4095e-05 - val_loss: 0.0239 - learning_rate: 3.0903e-05\n",
      "Epoch 369/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.3999e-05 - val_loss: 0.0245 - learning_rate: 3.0903e-05\n",
      "Epoch 370/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 7.4505e-05\n",
      "Epoch 370: ReduceLROnPlateau reducing learning rate to 2.7812844200525434e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.3958e-05 - val_loss: 0.0240 - learning_rate: 3.0903e-05\n",
      "Epoch 371/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.5186e-05 - val_loss: 0.0246 - learning_rate: 2.7813e-05\n",
      "Epoch 372/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 7.4825e-05 - val_loss: 0.0240 - learning_rate: 2.7813e-05\n",
      "Epoch 373/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.5221e-05 - val_loss: 0.0243 - learning_rate: 2.7813e-05\n",
      "Epoch 374/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.3864e-05 - val_loss: 0.0243 - learning_rate: 2.7813e-05\n",
      "Epoch 375/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.2412e-05 - val_loss: 0.0243 - learning_rate: 2.7813e-05\n",
      "Epoch 376/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.3486e-05 - val_loss: 0.0250 - learning_rate: 2.7813e-05\n",
      "Epoch 377/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 7.4317e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:27:37.397567: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.3982e-05 - val_loss: 0.0240 - learning_rate: 2.7813e-05\n",
      "Epoch 378/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.0958e-05 - val_loss: 0.0248 - learning_rate: 2.7813e-05\n",
      "Epoch 379/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.1556e-05 - val_loss: 0.0244 - learning_rate: 2.7813e-05\n",
      "Epoch 380/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 7.3514e-05\n",
      "Epoch 380: ReduceLROnPlateau reducing learning rate to 2.5031560107890984e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.3282e-05 - val_loss: 0.0246 - learning_rate: 2.7813e-05\n",
      "Epoch 381/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.1491e-05 - val_loss: 0.0242 - learning_rate: 2.5032e-05\n",
      "Epoch 382/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.1216e-05 - val_loss: 0.0245 - learning_rate: 2.5032e-05\n",
      "Epoch 383/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.9516e-05 - val_loss: 0.0244 - learning_rate: 2.5032e-05\n",
      "Epoch 384/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.3579e-05 - val_loss: 0.0242 - learning_rate: 2.5032e-05\n",
      "Epoch 385/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.0855e-05 - val_loss: 0.0244 - learning_rate: 2.5032e-05\n",
      "Epoch 386/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.1584e-05 - val_loss: 0.0244 - learning_rate: 2.5032e-05\n",
      "Epoch 387/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.0051e-05 - val_loss: 0.0237 - learning_rate: 2.5032e-05\n",
      "Epoch 388/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.4265e-05 - val_loss: 0.0254 - learning_rate: 2.5032e-05\n",
      "Epoch 389/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.1075e-05 - val_loss: 0.0255 - learning_rate: 2.5032e-05\n",
      "Epoch 390/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 6.9830e-05\n",
      "Epoch 390: ReduceLROnPlateau reducing learning rate to 2.2528404588229024e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.9956e-05 - val_loss: 0.0241 - learning_rate: 2.5032e-05\n",
      "Epoch 391/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.2378e-05 - val_loss: 0.0239 - learning_rate: 2.2528e-05\n",
      "Epoch 392/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.2172e-05 - val_loss: 0.0236 - learning_rate: 2.2528e-05\n",
      "Epoch 393/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.9495e-05 - val_loss: 0.0250 - learning_rate: 2.2528e-05\n",
      "Epoch 394/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.1544e-05 - val_loss: 0.0247 - learning_rate: 2.2528e-05\n",
      "Epoch 395/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.1244e-05 - val_loss: 0.0247 - learning_rate: 2.2528e-05\n",
      "Epoch 396/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 6.8346e-05 - val_loss: 0.0247 - learning_rate: 2.2528e-05\n",
      "Epoch 397/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 7.0120e-05 - val_loss: 0.0243 - learning_rate: 2.2528e-05\n",
      "Epoch 398/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.0963e-05 - val_loss: 0.0253 - learning_rate: 2.2528e-05\n",
      "Epoch 399/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 7.1034e-05 - val_loss: 0.0239 - learning_rate: 2.2528e-05\n",
      "Epoch 400/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 7.2783e-05\n",
      "Epoch 400: ReduceLROnPlateau reducing learning rate to 2.0275563474569936e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.2475e-05 - val_loss: 0.0246 - learning_rate: 2.2528e-05\n",
      "Epoch 401/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 7.0704e-05 - val_loss: 0.0246 - learning_rate: 2.0276e-05\n",
      "Epoch 402/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.9509e-05 - val_loss: 0.0250 - learning_rate: 2.0276e-05\n",
      "Epoch 403/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:28:07.478123: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.0144e-05 - val_loss: 0.0241 - learning_rate: 2.0276e-05\n",
      "Epoch 404/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.0790e-05 - val_loss: 0.0237 - learning_rate: 2.0276e-05\n",
      "Epoch 405/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.9200e-05 - val_loss: 0.0237 - learning_rate: 2.0276e-05\n",
      "Epoch 406/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.1509e-05 - val_loss: 0.0241 - learning_rate: 2.0276e-05\n",
      "Epoch 407/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.9001e-05 - val_loss: 0.0249 - learning_rate: 2.0276e-05\n",
      "Epoch 408/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.8598e-05 - val_loss: 0.0257 - learning_rate: 2.0276e-05\n",
      "Epoch 409/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.0404e-05 - val_loss: 0.0254 - learning_rate: 2.0276e-05\n",
      "Epoch 410/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 6.9365e-05\n",
      "Epoch 410: ReduceLROnPlateau reducing learning rate to 1.8248007290821987e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.9224e-05 - val_loss: 0.0249 - learning_rate: 2.0276e-05\n",
      "Epoch 411/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.8738e-05 - val_loss: 0.0244 - learning_rate: 1.8248e-05\n",
      "Epoch 412/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.7506e-05 - val_loss: 0.0245 - learning_rate: 1.8248e-05\n",
      "Epoch 413/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.1552e-05 - val_loss: 0.0236 - learning_rate: 1.8248e-05\n",
      "Epoch 414/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 6.8888e-05 - val_loss: 0.0244 - learning_rate: 1.8248e-05\n",
      "Epoch 415/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.1131e-05 - val_loss: 0.0246 - learning_rate: 1.8248e-05\n",
      "Epoch 416/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.7756e-05 - val_loss: 0.0227 - learning_rate: 1.8248e-05\n",
      "Epoch 417/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.7642e-05 - val_loss: 0.0236 - learning_rate: 1.8248e-05\n",
      "Epoch 418/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.7566e-05 - val_loss: 0.0254 - learning_rate: 1.8248e-05\n",
      "Epoch 419/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.8957e-05 - val_loss: 0.0258 - learning_rate: 1.8248e-05\n",
      "Epoch 420/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 6.9676e-05\n",
      "Epoch 420: ReduceLROnPlateau reducing learning rate to 1.6423206398030745e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.9462e-05 - val_loss: 0.0240 - learning_rate: 1.8248e-05\n",
      "Epoch 421/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.0132e-05 - val_loss: 0.0244 - learning_rate: 1.6423e-05\n",
      "Epoch 422/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.7441e-05 - val_loss: 0.0247 - learning_rate: 1.6423e-05\n",
      "Epoch 423/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.8695e-05 - val_loss: 0.0226 - learning_rate: 1.6423e-05\n",
      "Epoch 424/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.8010e-05 - val_loss: 0.0235 - learning_rate: 1.6423e-05\n",
      "Epoch 425/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.7419e-05 - val_loss: 0.0259 - learning_rate: 1.6423e-05\n",
      "Epoch 426/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.7041e-05 - val_loss: 0.0248 - learning_rate: 1.6423e-05\n",
      "Epoch 427/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 6.7447e-05 - val_loss: 0.0247 - learning_rate: 1.6423e-05\n",
      "Epoch 428/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 6.8207e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:28:38.027184: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 6.7801e-05 - val_loss: 0.0236 - learning_rate: 1.6423e-05\n",
      "Epoch 429/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.9078e-05 - val_loss: 0.0254 - learning_rate: 1.6423e-05\n",
      "Epoch 430/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 6.5229e-05\n",
      "Epoch 430: ReduceLROnPlateau reducing learning rate to 1.4780885430809576e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.5254e-05 - val_loss: 0.0239 - learning_rate: 1.6423e-05\n",
      "Epoch 431/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 6.5758e-05 - val_loss: 0.0243 - learning_rate: 1.4781e-05\n",
      "Epoch 432/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.6791e-05 - val_loss: 0.0227 - learning_rate: 1.4781e-05\n",
      "Epoch 433/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.7190e-05 - val_loss: 0.0236 - learning_rate: 1.4781e-05\n",
      "Epoch 434/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 6.3240e-05 - val_loss: 0.0255 - learning_rate: 1.4781e-05\n",
      "Epoch 435/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.6878e-05 - val_loss: 0.0248 - learning_rate: 1.4781e-05\n",
      "Epoch 436/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.5860e-05 - val_loss: 0.0245 - learning_rate: 1.4781e-05\n",
      "Epoch 437/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 6.6540e-05 - val_loss: 0.0246 - learning_rate: 1.4781e-05\n",
      "Epoch 438/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.7125e-05 - val_loss: 0.0249 - learning_rate: 1.4781e-05\n",
      "Epoch 439/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.5755e-05 - val_loss: 0.0245 - learning_rate: 1.4781e-05\n",
      "Epoch 440/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 6.4674e-05\n",
      "Epoch 440: ReduceLROnPlateau reducing learning rate to 1.3302796560310526e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.4761e-05 - val_loss: 0.0241 - learning_rate: 1.4781e-05\n",
      "Epoch 441/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 6.7413e-05 - val_loss: 0.0246 - learning_rate: 1.3303e-05\n",
      "Epoch 442/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.7517e-05 - val_loss: 0.0246 - learning_rate: 1.3303e-05\n",
      "Epoch 443/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.4512e-05 - val_loss: 0.0243 - learning_rate: 1.3303e-05\n",
      "Epoch 444/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.7597e-05 - val_loss: 0.0255 - learning_rate: 1.3303e-05\n",
      "Epoch 445/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.5914e-05 - val_loss: 0.0247 - learning_rate: 1.3303e-05\n",
      "Epoch 446/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 6.6657e-05 - val_loss: 0.0245 - learning_rate: 1.3303e-05\n",
      "Epoch 447/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 6.4334e-05 - val_loss: 0.0248 - learning_rate: 1.3303e-05\n",
      "Epoch 448/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.5919e-05 - val_loss: 0.0242 - learning_rate: 1.3303e-05\n",
      "Epoch 449/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.6248e-05 - val_loss: 0.0248 - learning_rate: 1.3303e-05\n",
      "Epoch 450/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 6.6722e-05\n",
      "Epoch 450: ReduceLROnPlateau reducing learning rate to 1.1972517313552089e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.6625e-05 - val_loss: 0.0246 - learning_rate: 1.3303e-05\n",
      "Epoch 451/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.6241e-05 - val_loss: 0.0248 - learning_rate: 1.1973e-05\n",
      "Epoch 452/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.4231e-05 - val_loss: 0.0240 - learning_rate: 1.1973e-05\n",
      "Epoch 453/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 6.4653e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:29:08.162791: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.4460e-05 - val_loss: 0.0238 - learning_rate: 1.1973e-05\n",
      "Epoch 454/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.6210e-05 - val_loss: 0.0248 - learning_rate: 1.1973e-05\n",
      "Epoch 455/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.7076e-05 - val_loss: 0.0245 - learning_rate: 1.1973e-05\n",
      "Epoch 456/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.5598e-05 - val_loss: 0.0241 - learning_rate: 1.1973e-05\n",
      "Epoch 457/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.5601e-05 - val_loss: 0.0249 - learning_rate: 1.1973e-05\n",
      "Epoch 458/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.4730e-05 - val_loss: 0.0243 - learning_rate: 1.1973e-05\n",
      "Epoch 459/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.5144e-05 - val_loss: 0.0250 - learning_rate: 1.1973e-05\n",
      "Epoch 460/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 6.6618e-05\n",
      "Epoch 460: ReduceLROnPlateau reducing learning rate to 1.077526558219688e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.6130e-05 - val_loss: 0.0236 - learning_rate: 1.1973e-05\n",
      "Epoch 461/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.2646e-05 - val_loss: 0.0240 - learning_rate: 1.0775e-05\n",
      "Epoch 462/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.5585e-05 - val_loss: 0.0256 - learning_rate: 1.0775e-05\n",
      "Epoch 463/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.3642e-05 - val_loss: 0.0243 - learning_rate: 1.0775e-05\n",
      "Epoch 464/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.5774e-05 - val_loss: 0.0234 - learning_rate: 1.0775e-05\n",
      "Epoch 465/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.6571e-05 - val_loss: 0.0242 - learning_rate: 1.0775e-05\n",
      "Epoch 466/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.3624e-05 - val_loss: 0.0242 - learning_rate: 1.0775e-05\n",
      "Epoch 467/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.4796e-05 - val_loss: 0.0253 - learning_rate: 1.0775e-05\n",
      "Epoch 468/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.3116e-05 - val_loss: 0.0252 - learning_rate: 1.0775e-05\n",
      "Epoch 469/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.4792e-05 - val_loss: 0.0247 - learning_rate: 1.0775e-05\n",
      "Epoch 470/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 6.4070e-05\n",
      "Epoch 470: ReduceLROnPlateau reducing learning rate to 9.697739187686238e-06.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 6.4222e-05 - val_loss: 0.0242 - learning_rate: 1.0775e-05\n",
      "Epoch 471/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.3069e-05 - val_loss: 0.0236 - learning_rate: 9.6977e-06\n",
      "Epoch 472/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.2630e-05 - val_loss: 0.0230 - learning_rate: 9.6977e-06\n",
      "Epoch 473/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.6715e-05 - val_loss: 0.0250 - learning_rate: 9.6977e-06\n",
      "Epoch 474/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.4521e-05 - val_loss: 0.0243 - learning_rate: 9.6977e-06\n",
      "Epoch 475/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.4310e-05 - val_loss: 0.0241 - learning_rate: 9.6977e-06\n",
      "Epoch 476/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 6.6071e-05 - val_loss: 0.0240 - learning_rate: 9.6977e-06\n",
      "Epoch 477/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.6153e-05 - val_loss: 0.0244 - learning_rate: 9.6977e-06\n",
      "Epoch 478/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.4002e-05 - val_loss: 0.0240 - learning_rate: 9.6977e-06\n",
      "Epoch 479/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:29:38.171115: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.6196e-05 - val_loss: 0.0254 - learning_rate: 9.6977e-06\n",
      "Epoch 480/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 6.3210e-05\n",
      "Epoch 480: ReduceLROnPlateau reducing learning rate to 8.727965268917615e-06.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.3215e-05 - val_loss: 0.0234 - learning_rate: 9.6977e-06\n",
      "Epoch 481/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 6.6246e-05 - val_loss: 0.0249 - learning_rate: 8.7280e-06\n",
      "Epoch 482/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 6.4771e-05 - val_loss: 0.0252 - learning_rate: 8.7280e-06\n",
      "Epoch 483/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.4045e-05 - val_loss: 0.0242 - learning_rate: 8.7280e-06\n",
      "Epoch 484/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.7428e-05 - val_loss: 0.0238 - learning_rate: 8.7280e-06\n",
      "Epoch 485/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.4088e-05 - val_loss: 0.0241 - learning_rate: 8.7280e-06\n",
      "Epoch 486/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.4230e-05 - val_loss: 0.0247 - learning_rate: 8.7280e-06\n",
      "Epoch 487/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.5002e-05 - val_loss: 0.0250 - learning_rate: 8.7280e-06\n",
      "Epoch 488/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.4055e-05 - val_loss: 0.0249 - learning_rate: 8.7280e-06\n",
      "Epoch 489/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.3214e-05 - val_loss: 0.0244 - learning_rate: 8.7280e-06\n",
      "Epoch 490/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 6.2877e-05\n",
      "Epoch 490: ReduceLROnPlateau reducing learning rate to 7.855168496462283e-06.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.3011e-05 - val_loss: 0.0250 - learning_rate: 8.7280e-06\n",
      "Epoch 491/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.1086e-05 - val_loss: 0.0240 - learning_rate: 7.8552e-06\n",
      "Epoch 492/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.3756e-05 - val_loss: 0.0241 - learning_rate: 7.8552e-06\n",
      "Epoch 493/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.2592e-05 - val_loss: 0.0245 - learning_rate: 7.8552e-06\n",
      "Epoch 494/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.6454e-05 - val_loss: 0.0250 - learning_rate: 7.8552e-06\n",
      "Epoch 495/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.3681e-05 - val_loss: 0.0248 - learning_rate: 7.8552e-06\n",
      "Epoch 496/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.2319e-05 - val_loss: 0.0249 - learning_rate: 7.8552e-06\n",
      "Epoch 497/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.2512e-05 - val_loss: 0.0250 - learning_rate: 7.8552e-06\n",
      "Epoch 498/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.3904e-05 - val_loss: 0.0248 - learning_rate: 7.8552e-06\n",
      "Epoch 499/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.3854e-05 - val_loss: 0.0238 - learning_rate: 7.8552e-06\n",
      "Epoch 500/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 6.4938e-05\n",
      "Epoch 500: ReduceLROnPlateau reducing learning rate to 7.069651564961533e-06.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.4743e-05 - val_loss: 0.0247 - learning_rate: 7.8552e-06\n",
      "Epoch 501/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.5248e-05 - val_loss: 0.0251 - learning_rate: 7.0697e-06\n",
      "Epoch 502/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.4349e-05 - val_loss: 0.0253 - learning_rate: 7.0697e-06\n",
      "Epoch 503/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.2392e-05 - val_loss: 0.0238 - learning_rate: 7.0697e-06\n",
      "Epoch 504/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 6.3900e-05 - val_loss: 0.0253 - learning_rate: 7.0697e-06\n",
      "Epoch 505/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:30:08.761077: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.1993e-05 - val_loss: 0.0247 - learning_rate: 7.0697e-06\n",
      "Epoch 506/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.2447e-05 - val_loss: 0.0247 - learning_rate: 7.0697e-06\n",
      "Epoch 507/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.3952e-05 - val_loss: 0.0251 - learning_rate: 7.0697e-06\n",
      "Epoch 508/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.4835e-05 - val_loss: 0.0251 - learning_rate: 7.0697e-06\n",
      "Epoch 509/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.3367e-05 - val_loss: 0.0250 - learning_rate: 7.0697e-06\n",
      "Epoch 510/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 6.1665e-05\n",
      "Epoch 510: ReduceLROnPlateau reducing learning rate to 6.362686326610856e-06.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.1673e-05 - val_loss: 0.0240 - learning_rate: 7.0697e-06\n",
      "Epoch 511/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.2027e-05 - val_loss: 0.0232 - learning_rate: 6.3627e-06\n",
      "Epoch 512/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.1857e-05 - val_loss: 0.0240 - learning_rate: 6.3627e-06\n",
      "Epoch 513/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.0565e-05 - val_loss: 0.0247 - learning_rate: 6.3627e-06\n",
      "Epoch 514/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.2941e-05 - val_loss: 0.0238 - learning_rate: 6.3627e-06\n",
      "Epoch 515/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.2716e-05 - val_loss: 0.0260 - learning_rate: 6.3627e-06\n",
      "Epoch 516/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.3025e-05 - val_loss: 0.0247 - learning_rate: 6.3627e-06\n",
      "Epoch 517/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.1373e-05 - val_loss: 0.0240 - learning_rate: 6.3627e-06\n",
      "Epoch 518/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 6.0665e-05 - val_loss: 0.0244 - learning_rate: 6.3627e-06\n",
      "Epoch 519/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.2916e-05 - val_loss: 0.0242 - learning_rate: 6.3627e-06\n",
      "Epoch 520/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 6.3172e-05\n",
      "Epoch 520: ReduceLROnPlateau reducing learning rate to 5.726417612095247e-06.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.2743e-05 - val_loss: 0.0249 - learning_rate: 6.3627e-06\n",
      "Epoch 521/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.1752e-05 - val_loss: 0.0242 - learning_rate: 5.7264e-06\n",
      "Epoch 522/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 6.4738e-05 - val_loss: 0.0245 - learning_rate: 5.7264e-06\n",
      "Epoch 523/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.3266e-05 - val_loss: 0.0248 - learning_rate: 5.7264e-06\n",
      "Epoch 524/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.3167e-05 - val_loss: 0.0250 - learning_rate: 5.7264e-06\n",
      "Epoch 525/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.0479e-05 - val_loss: 0.0234 - learning_rate: 5.7264e-06\n",
      "Epoch 526/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 6.1712e-05 - val_loss: 0.0254 - learning_rate: 5.7264e-06\n",
      "Epoch 527/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 6.2056e-05 - val_loss: 0.0247 - learning_rate: 5.7264e-06\n",
      "Epoch 528/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 6.1341e-05 - val_loss: 0.0249 - learning_rate: 5.7264e-06\n",
      "Epoch 529/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.1515e-05 - val_loss: 0.0239 - learning_rate: 5.7264e-06\n",
      "Epoch 530/2000\n",
      "\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 252ms/step - loss: 5.9356e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:30:38.901838: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 6.0528e-05\n",
      "Epoch 530: ReduceLROnPlateau reducing learning rate to 5.15377605552203e-06.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.0644e-05 - val_loss: 0.0240 - learning_rate: 5.7264e-06\n",
      "Epoch 531/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.1954e-05 - val_loss: 0.0232 - learning_rate: 5.1538e-06\n",
      "Epoch 532/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.2098e-05 - val_loss: 0.0238 - learning_rate: 5.1538e-06\n",
      "Epoch 533/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.0110e-05 - val_loss: 0.0256 - learning_rate: 5.1538e-06\n",
      "Epoch 534/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.2545e-05 - val_loss: 0.0247 - learning_rate: 5.1538e-06\n",
      "Epoch 535/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 6.3773e-05 - val_loss: 0.0234 - learning_rate: 5.1538e-06\n",
      "Epoch 536/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.1710e-05 - val_loss: 0.0248 - learning_rate: 5.1538e-06\n",
      "Epoch 537/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.2804e-05 - val_loss: 0.0250 - learning_rate: 5.1538e-06\n",
      "Epoch 538/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.1796e-05 - val_loss: 0.0242 - learning_rate: 5.1538e-06\n",
      "Epoch 539/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.9884e-05 - val_loss: 0.0242 - learning_rate: 5.1538e-06\n",
      "Epoch 540/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 6.3799e-05\n",
      "Epoch 540: ReduceLROnPlateau reducing learning rate to 5e-06.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.2944e-05 - val_loss: 0.0241 - learning_rate: 5.1538e-06\n",
      "Epoch 541/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.1602e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 542/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.3939e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 543/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.4465e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 544/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.0267e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 545/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.4451e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 546/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.1499e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 547/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.0739e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 548/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - loss: 6.2343e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 549/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.1828e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 550/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.0362e-05 - val_loss: 0.0232 - learning_rate: 5.0000e-06\n",
      "Epoch 551/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.9798e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 552/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.1389e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 553/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 6.0491e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 554/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.0474e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 555/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 6.3683e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:31:09.736001: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.2928e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 556/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.0358e-05 - val_loss: 0.0233 - learning_rate: 5.0000e-06\n",
      "Epoch 557/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 5.9827e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 558/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.2631e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 559/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.2929e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 560/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.0072e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 561/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.9748e-05 - val_loss: 0.0237 - learning_rate: 5.0000e-06\n",
      "Epoch 562/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.9331e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 563/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.1330e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 564/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.2292e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 565/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.1919e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 566/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.0817e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 567/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.1642e-05 - val_loss: 0.0261 - learning_rate: 5.0000e-06\n",
      "Epoch 568/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.0014e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 569/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.0879e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 570/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.1861e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 571/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.2025e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 572/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 6.1329e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 573/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.0272e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 574/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.9100e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 575/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.2466e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 576/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.0387e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 577/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.1278e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 578/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.9869e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 579/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.7561e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 580/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.2299e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 581/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 6.2447e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:31:40.288162: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.2058e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 582/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.1951e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 583/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.9580e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 584/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.0544e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 585/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.0027e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 586/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - loss: 5.7910e-05 - val_loss: 0.0235 - learning_rate: 5.0000e-06\n",
      "Epoch 587/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.7495e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 588/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.0351e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 589/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.8249e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 590/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.8754e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 591/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.0061e-05 - val_loss: 0.0238 - learning_rate: 5.0000e-06\n",
      "Epoch 592/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.0519e-05 - val_loss: 0.0229 - learning_rate: 5.0000e-06\n",
      "Epoch 593/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.0668e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 594/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.9467e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 595/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.8462e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 596/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 6.0565e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 597/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.9798e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 598/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.9746e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 599/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.8536e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 600/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.9918e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 601/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.0609e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 602/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.9392e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 603/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.9025e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 604/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.7084e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 605/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.1223e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 606/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.8565e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 607/2000\n",
      "\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 287ms/step - loss: 6.4863e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:32:10.301914: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.9374e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 608/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.9966e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 609/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.9323e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 610/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 6.1523e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 611/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.8330e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 612/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.7610e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 613/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.9836e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 614/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.7861e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 615/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.8987e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 616/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.9607e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 617/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - loss: 6.0291e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 618/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 5.9724e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 619/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.0243e-05 - val_loss: 0.0234 - learning_rate: 5.0000e-06\n",
      "Epoch 620/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.7168e-05 - val_loss: 0.0232 - learning_rate: 5.0000e-06\n",
      "Epoch 621/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.8925e-05 - val_loss: 0.0233 - learning_rate: 5.0000e-06\n",
      "Epoch 622/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.9424e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 623/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.8781e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 624/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.9742e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 625/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.7299e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 626/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 5.8310e-05 - val_loss: 0.0237 - learning_rate: 5.0000e-06\n",
      "Epoch 627/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.8469e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 628/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.9839e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 629/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.9660e-05 - val_loss: 0.0236 - learning_rate: 5.0000e-06\n",
      "Epoch 630/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.6895e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 631/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 5.6206e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 632/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:32:40.333391: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 6.0282e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 633/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.9174e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 634/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.2249e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 635/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.8208e-05 - val_loss: 0.0230 - learning_rate: 5.0000e-06\n",
      "Epoch 636/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.7491e-05 - val_loss: 0.0259 - learning_rate: 5.0000e-06\n",
      "Epoch 637/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.0540e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 638/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.7956e-05 - val_loss: 0.0237 - learning_rate: 5.0000e-06\n",
      "Epoch 639/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.0985e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 640/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.8887e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 641/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.7050e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 642/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.6711e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 643/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.7499e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 644/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.8969e-05 - val_loss: 0.0237 - learning_rate: 5.0000e-06\n",
      "Epoch 645/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.0355e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 646/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.8678e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 647/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 121ms/step - loss: 5.6922e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 648/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.7979e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 649/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.0724e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 650/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.8206e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 651/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.7100e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 652/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.7198e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 653/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.7233e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 654/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.8643e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 655/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.7050e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 656/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 5.7434e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 657/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 5.7511e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:33:11.139467: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.7090e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 658/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.8270e-05 - val_loss: 0.0236 - learning_rate: 5.0000e-06\n",
      "Epoch 659/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.5861e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 660/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.7611e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 661/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 5.7356e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 662/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.7627e-05 - val_loss: 0.0233 - learning_rate: 5.0000e-06\n",
      "Epoch 663/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.7629e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 664/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.9042e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 665/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.7555e-05 - val_loss: 0.0259 - learning_rate: 5.0000e-06\n",
      "Epoch 666/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.7577e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 667/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.7237e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 668/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.6455e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 669/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.7018e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 670/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.6572e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 671/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.0561e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 672/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.7329e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 673/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - loss: 5.6806e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 674/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.5831e-05 - val_loss: 0.0229 - learning_rate: 5.0000e-06\n",
      "Epoch 675/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.7578e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 676/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.8042e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 677/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.6827e-05 - val_loss: 0.0237 - learning_rate: 5.0000e-06\n",
      "Epoch 678/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.6542e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 679/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 5.4676e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 680/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.7273e-05 - val_loss: 0.0258 - learning_rate: 5.0000e-06\n",
      "Epoch 681/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.8096e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 682/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.4552e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 683/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:33:41.164184: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.5408e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 684/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.6748e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 685/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.7881e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 686/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.5998e-05 - val_loss: 0.0235 - learning_rate: 5.0000e-06\n",
      "Epoch 687/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:33:45.762843: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.5503e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 688/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.6215e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 689/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.6234e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 690/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.5536e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 691/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.7063e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 692/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.5112e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 693/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.6730e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 694/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.7547e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 695/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.3690e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 696/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.5004e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 697/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 5.3824e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 698/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.4372e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 699/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step - loss: 5.6286e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 700/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.7403e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 701/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 5.4296e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 702/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.5197e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 703/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.4257e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 704/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.6468e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 705/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 5.6365e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 706/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.6485e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 707/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.3098e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 708/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 5.5410e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:34:11.960588: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.4994e-05 - val_loss: 0.0265 - learning_rate: 5.0000e-06\n",
      "Epoch 709/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.6070e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 710/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.7369e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 711/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.5614e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 712/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.7392e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 713/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.6074e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 714/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.6022e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 715/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.4989e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 716/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.4071e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 717/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.5594e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 718/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.3484e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 719/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.5164e-05 - val_loss: 0.0233 - learning_rate: 5.0000e-06\n",
      "Epoch 720/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.3362e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 721/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.4018e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 722/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.6298e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 723/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.4236e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 724/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.3996e-05 - val_loss: 0.0238 - learning_rate: 5.0000e-06\n",
      "Epoch 725/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.4198e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 726/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.6036e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 727/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.6121e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 728/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 5.5924e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 729/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.3031e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 730/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.5276e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 731/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.4019e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 732/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.4903e-05 - val_loss: 0.0236 - learning_rate: 5.0000e-06\n",
      "Epoch 733/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.3499e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 734/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 5.4976e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:34:42.749010: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.4763e-05 - val_loss: 0.0238 - learning_rate: 5.0000e-06\n",
      "Epoch 735/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 5.4002e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 736/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.4579e-05 - val_loss: 0.0235 - learning_rate: 5.0000e-06\n",
      "Epoch 737/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.3957e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 738/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.5103e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 739/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.4848e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 740/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.3348e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 741/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.5296e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 742/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.4817e-05 - val_loss: 0.0236 - learning_rate: 5.0000e-06\n",
      "Epoch 743/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.6397e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 744/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.5470e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 745/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.4959e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 746/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.3222e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 747/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.4802e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 748/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.3467e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 749/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.5996e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 750/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 5.2257e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 751/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.4735e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 752/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.3465e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 753/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.4681e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 754/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.2066e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 755/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.4911e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 756/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.2611e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 757/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.4645e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 758/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.2119e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 759/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.1194e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 760/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 5.2420e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:35:13.484780: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.2456e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 761/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.3942e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 762/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.0786e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 763/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.3686e-05 - val_loss: 0.0234 - learning_rate: 5.0000e-06\n",
      "Epoch 764/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.4631e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 765/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.2823e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 766/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.3299e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 767/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.1084e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 768/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.2393e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 769/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.1917e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 770/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.2640e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 771/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.2518e-05 - val_loss: 0.0234 - learning_rate: 5.0000e-06\n",
      "Epoch 772/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.3714e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 773/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.3297e-05 - val_loss: 0.0235 - learning_rate: 5.0000e-06\n",
      "Epoch 774/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.0696e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 775/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.2698e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 776/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.0381e-05 - val_loss: 0.0233 - learning_rate: 5.0000e-06\n",
      "Epoch 777/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.4031e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 778/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.3193e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 779/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.2220e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 780/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.2165e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 781/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.1613e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 782/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.0908e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 783/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.2955e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 784/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.2848e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 785/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.0458e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 786/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:35:43.503717: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.1685e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 787/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.3927e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 788/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.4078e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 789/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.3175e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 790/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.3361e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 791/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.3826e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 792/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.1820e-05 - val_loss: 0.0259 - learning_rate: 5.0000e-06\n",
      "Epoch 793/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.0852e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 794/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.3319e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 795/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.2312e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 796/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.3015e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 797/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.2776e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 798/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.0923e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 799/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.0666e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 800/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.1745e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 801/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.2764e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 802/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.2593e-05 - val_loss: 0.0237 - learning_rate: 5.0000e-06\n",
      "Epoch 803/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.1090e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 804/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.1104e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 805/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.9943e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 806/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.1167e-05 - val_loss: 0.0258 - learning_rate: 5.0000e-06\n",
      "Epoch 807/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.1699e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 808/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.0377e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 809/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.1650e-05 - val_loss: 0.0263 - learning_rate: 5.0000e-06\n",
      "Epoch 810/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.0240e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 811/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:36:13.545707: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.1086e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 812/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.1981e-05 - val_loss: 0.0260 - learning_rate: 5.0000e-06\n",
      "Epoch 813/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.2631e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 814/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.0003e-05 - val_loss: 0.0233 - learning_rate: 5.0000e-06\n",
      "Epoch 815/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.0766e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 816/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.1077e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 817/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.2566e-05 - val_loss: 0.0238 - learning_rate: 5.0000e-06\n",
      "Epoch 818/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.0530e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 819/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.0978e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 820/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.1795e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 821/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.1499e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 822/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.0499e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 823/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.0989e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 824/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.8983e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 825/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.1838e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 826/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.1085e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 827/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.0851e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 828/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.2720e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 829/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.0286e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 830/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 4.9502e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 831/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.0136e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 832/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.8993e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 833/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.0912e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 834/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.0586e-05 - val_loss: 0.0238 - learning_rate: 5.0000e-06\n",
      "Epoch 835/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.0759e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 836/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 4.9984e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:36:44.092050: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.0092e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 837/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.0072e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 838/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.1505e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 839/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.9430e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 840/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.0762e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 841/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 5.1024e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 842/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.1311e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 843/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.9572e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 844/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.8578e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 845/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.9462e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 846/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.0751e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 847/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.8839e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 848/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.0172e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 849/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.9819e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 850/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.1539e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 851/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.0319e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 852/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.9505e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 853/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.8243e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 854/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.7645e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 855/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.9024e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 856/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.9121e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 857/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 5.0243e-05 - val_loss: 0.0261 - learning_rate: 5.0000e-06\n",
      "Epoch 858/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.0840e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 859/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.1257e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 860/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.8422e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 861/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.0012e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 862/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:37:14.100624: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.0303e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 863/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.8395e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 864/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.0113e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 865/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.9544e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 866/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.0315e-05 - val_loss: 0.0238 - learning_rate: 5.0000e-06\n",
      "Epoch 867/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.0779e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 868/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.9557e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 869/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.9135e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 870/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.8393e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 871/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.7508e-05 - val_loss: 0.0233 - learning_rate: 5.0000e-06\n",
      "Epoch 872/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.7695e-05 - val_loss: 0.0233 - learning_rate: 5.0000e-06\n",
      "Epoch 873/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.7567e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 874/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.9208e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 875/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 4.8421e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 876/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.9517e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 877/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.9856e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 878/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.9033e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 879/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.9292e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 880/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.9113e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 881/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.8742e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 882/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 4.9613e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 883/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.8756e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 884/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.8105e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 885/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.9397e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 886/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 5.0223e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 887/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:37:44.228276: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.7328e-05 - val_loss: 0.0237 - learning_rate: 5.0000e-06\n",
      "Epoch 888/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.8489e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 889/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.8530e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 890/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.9477e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 891/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.8652e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 892/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.5436e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 893/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.8322e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 894/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.9397e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 895/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.7778e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 896/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.7811e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 897/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.7896e-05 - val_loss: 0.0238 - learning_rate: 5.0000e-06\n",
      "Epoch 898/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.8561e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 899/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.6936e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 900/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 4.8553e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 901/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.7267e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 902/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.6130e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 903/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.7529e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 904/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.9023e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 905/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.8168e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 906/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.9083e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 907/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.6176e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 908/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.8606e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 909/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.8716e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 910/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.7394e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 911/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.9118e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 912/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 4.8837e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:38:14.722226: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 4.8804e-05 - val_loss: 0.0232 - learning_rate: 5.0000e-06\n",
      "Epoch 913/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.7488e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 914/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5710e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 915/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.6207e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 916/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.8232e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 917/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.7911e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 918/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.9501e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 919/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.6163e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 920/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.7696e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 921/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.6231e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 922/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.6690e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 923/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.8119e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 924/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.5991e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 925/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.7727e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 926/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.6507e-05 - val_loss: 0.0237 - learning_rate: 5.0000e-06\n",
      "Epoch 927/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.8370e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 928/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5537e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 929/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.5851e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 930/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5478e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 931/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.7369e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 932/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.6403e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 933/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.5811e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 934/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.6778e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 935/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.6675e-05 - val_loss: 0.0234 - learning_rate: 5.0000e-06\n",
      "Epoch 936/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.6984e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 937/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.9142e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 938/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:38:44.762282: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5768e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 939/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.8069e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 940/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.7692e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 941/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.6117e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 942/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.6486e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 943/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5394e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 944/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.5696e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 945/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 4.4390e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 946/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.7858e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 947/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.7022e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 948/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5914e-05 - val_loss: 0.0236 - learning_rate: 5.0000e-06\n",
      "Epoch 949/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5166e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 950/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.7631e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 951/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.6112e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 952/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.4658e-05 - val_loss: 0.0258 - learning_rate: 5.0000e-06\n",
      "Epoch 953/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.6778e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 954/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.6666e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 955/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5384e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 956/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.5936e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 957/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.6523e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 958/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.7137e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 959/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.4772e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 960/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.6547e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 961/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.5316e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 962/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5029e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 963/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 4.4730e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:39:15.742715: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - loss: 4.4551e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 964/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.5823e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 965/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 4.5536e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 966/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.6062e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 967/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.6248e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 968/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.6862e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 969/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.4333e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 970/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.4351e-05 - val_loss: 0.0258 - learning_rate: 5.0000e-06\n",
      "Epoch 971/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.4226e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 972/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5465e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 973/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.4180e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 974/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.2372e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 975/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.4979e-05 - val_loss: 0.0235 - learning_rate: 5.0000e-06\n",
      "Epoch 976/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.6470e-05 - val_loss: 0.0238 - learning_rate: 5.0000e-06\n",
      "Epoch 977/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.5005e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 978/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.6418e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 979/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5689e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 980/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.6602e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 981/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.4209e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 982/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5073e-05 - val_loss: 0.0237 - learning_rate: 5.0000e-06\n",
      "Epoch 983/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.5548e-05 - val_loss: 0.0236 - learning_rate: 5.0000e-06\n",
      "Epoch 984/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5791e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 985/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.4532e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 986/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.4799e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 987/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.4533e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 988/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5267e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 989/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 4.4059e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:39:46.546746: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.4054e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 990/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5514e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 991/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.2986e-05 - val_loss: 0.0236 - learning_rate: 5.0000e-06\n",
      "Epoch 992/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.6291e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 993/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.3990e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 994/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.5068e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 995/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.4497e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 996/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 4.3872e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 997/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.3176e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 998/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.4360e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 999/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.3878e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1000/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.5069e-05 - val_loss: 0.0262 - learning_rate: 5.0000e-06\n",
      "Epoch 1001/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - loss: 4.7091e-05 - val_loss: 0.0238 - learning_rate: 5.0000e-06\n",
      "Epoch 1002/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 4.4441e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1003/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.3543e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1004/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.3066e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1005/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.5839e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 1006/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.5257e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1007/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.3841e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1008/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5721e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1009/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.3040e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 1010/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.4587e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1011/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5012e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1012/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.3836e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1013/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.2990e-05 - val_loss: 0.0260 - learning_rate: 5.0000e-06\n",
      "Epoch 1014/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.3564e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1015/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:40:16.559071: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.3854e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1016/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.4380e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1017/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.4264e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1018/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.4349e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1019/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.3692e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 1020/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.2657e-05 - val_loss: 0.0238 - learning_rate: 5.0000e-06\n",
      "Epoch 1021/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.3914e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1022/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.3795e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1023/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.2360e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1024/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.2163e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1025/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.3577e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1026/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.2730e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1027/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.2166e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1028/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.3069e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1029/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.3436e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1030/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.3153e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1031/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.4815e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1032/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.3965e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1033/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - loss: 4.4694e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1034/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.2609e-05 - val_loss: 0.0238 - learning_rate: 5.0000e-06\n",
      "Epoch 1035/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.2606e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 1036/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.2737e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 1037/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.2463e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1038/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.1098e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1039/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.2476e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1040/2000\n",
      "\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 249ms/step - loss: 4.6119e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:40:46.655235: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.3527e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1041/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.2260e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 1042/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.4695e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1043/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.4212e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 1044/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.3052e-05 - val_loss: 0.0238 - learning_rate: 5.0000e-06\n",
      "Epoch 1045/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 4.3045e-05 - val_loss: 0.0237 - learning_rate: 5.0000e-06\n",
      "Epoch 1046/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.1941e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1047/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.2062e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1048/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.3355e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1049/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.2853e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1050/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.3607e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1051/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.3647e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1052/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.3321e-05 - val_loss: 0.0238 - learning_rate: 5.0000e-06\n",
      "Epoch 1053/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.3171e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1054/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.2826e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1055/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.2507e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 1056/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.1965e-05 - val_loss: 0.0258 - learning_rate: 5.0000e-06\n",
      "Epoch 1057/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.1901e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1058/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.2237e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1059/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.1784e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1060/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.2580e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1061/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.1648e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1062/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.2247e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 1063/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - loss: 4.1937e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1064/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.2539e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 1065/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 4.1216e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:41:17.421946: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.1008e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 1066/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.2818e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1067/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.2545e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1068/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.2102e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 1069/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.3390e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1070/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.2821e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1071/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.0779e-05 - val_loss: 0.0260 - learning_rate: 5.0000e-06\n",
      "Epoch 1072/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 4.2515e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 1073/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.2774e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1074/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.1625e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1075/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.1381e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1076/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.2515e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1077/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.1873e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1078/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.1847e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1079/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.3161e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1080/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.1579e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 1081/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.2966e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1082/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.2179e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1083/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.2348e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1084/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.0704e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1085/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.1765e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1086/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.0424e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 1087/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.1564e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1088/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.1256e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1089/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step - loss: 4.1171e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1090/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.1601e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1091/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:41:47.539487: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.0346e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1092/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.1213e-05 - val_loss: 0.0258 - learning_rate: 5.0000e-06\n",
      "Epoch 1093/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.2495e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1094/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.1933e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1095/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.1021e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 1096/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.2731e-05 - val_loss: 0.0259 - learning_rate: 5.0000e-06\n",
      "Epoch 1097/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.0702e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1098/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.0820e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1099/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.1982e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1100/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 4.1751e-05 - val_loss: 0.0238 - learning_rate: 5.0000e-06\n",
      "Epoch 1101/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.1199e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1102/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.0909e-05 - val_loss: 0.0232 - learning_rate: 5.0000e-06\n",
      "Epoch 1103/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.0761e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 1104/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.1286e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 1105/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.1872e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1106/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.0156e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1107/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.0682e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1108/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.0770e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 1109/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.1476e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1110/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.0194e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1111/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.0479e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1112/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.1198e-05 - val_loss: 0.0233 - learning_rate: 5.0000e-06\n",
      "Epoch 1113/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.0738e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1114/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.9820e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1115/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step - loss: 3.9986e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1116/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 4.1932e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:42:18.291634: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.1375e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1117/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.1572e-05 - val_loss: 0.0258 - learning_rate: 5.0000e-06\n",
      "Epoch 1118/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.0639e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1119/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.0636e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1120/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.0634e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 1121/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.9790e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1122/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.0565e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1123/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.1892e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 1124/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.1029e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1125/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 4.1623e-05 - val_loss: 0.0258 - learning_rate: 5.0000e-06\n",
      "Epoch 1126/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.0169e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1127/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.0124e-05 - val_loss: 0.0231 - learning_rate: 5.0000e-06\n",
      "Epoch 1128/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 4.0675e-05 - val_loss: 0.0259 - learning_rate: 5.0000e-06\n",
      "Epoch 1129/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.9873e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1130/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.9183e-05 - val_loss: 0.0259 - learning_rate: 5.0000e-06\n",
      "Epoch 1131/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.9840e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1132/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.0859e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1133/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.8477e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1134/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.8187e-05 - val_loss: 0.0266 - learning_rate: 5.0000e-06\n",
      "Epoch 1135/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.9513e-05 - val_loss: 0.0258 - learning_rate: 5.0000e-06\n",
      "Epoch 1136/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.0027e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1137/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.0326e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1138/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.8940e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1139/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.1826e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1140/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.0003e-05 - val_loss: 0.0232 - learning_rate: 5.0000e-06\n",
      "Epoch 1141/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 4.0225e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1142/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:42:48.470936: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.9610e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1143/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.9327e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 1144/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.8496e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1145/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.8227e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1146/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.9387e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1147/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.0311e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1148/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.9564e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1149/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.9007e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1150/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.0031e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1151/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.0449e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1152/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.9697e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1153/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.1722e-05 - val_loss: 0.0238 - learning_rate: 5.0000e-06\n",
      "Epoch 1154/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.8586e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 1155/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.9736e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1156/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.1010e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1157/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.0141e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1158/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.8469e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1159/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.9410e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1160/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.9069e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1161/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 3.8425e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1162/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.0230e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1163/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.9578e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1164/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.8605e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1165/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.9538e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1166/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.9614e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1167/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 3.9778e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:43:19.044817: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.9655e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1168/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.9342e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 1169/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.0219e-05 - val_loss: 0.0237 - learning_rate: 5.0000e-06\n",
      "Epoch 1170/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.9972e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1171/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.0172e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 1172/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.9516e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1173/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.9713e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1174/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.8895e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1175/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.8231e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 1176/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.7553e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1177/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.8410e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1178/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.9848e-05 - val_loss: 0.0260 - learning_rate: 5.0000e-06\n",
      "Epoch 1179/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.0287e-05 - val_loss: 0.0233 - learning_rate: 5.0000e-06\n",
      "Epoch 1180/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.7607e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1181/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.7914e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1182/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.7937e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1183/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.9341e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1184/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.7679e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1185/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.8305e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1186/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.6423e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 1187/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.7449e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1188/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.8036e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1189/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.9335e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1190/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.8973e-05 - val_loss: 0.0261 - learning_rate: 5.0000e-06\n",
      "Epoch 1191/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.8906e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1192/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.0425e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1193/2000\n",
      "\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 280ms/step - loss: 3.5721e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:43:49.239782: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.6510e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1194/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.0287e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1195/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.8051e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1196/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.8340e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1197/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.9146e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1198/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.7956e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1199/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.7937e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1200/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.8359e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 1201/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.7636e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1202/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.8210e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1203/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.8766e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1204/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.8407e-05 - val_loss: 0.0259 - learning_rate: 5.0000e-06\n",
      "Epoch 1205/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.8719e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1206/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.8650e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1207/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.8307e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1208/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.7211e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1209/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.7396e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 1210/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.8158e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1211/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.6828e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 1212/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.7962e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1213/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.8126e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1214/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.8725e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1215/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.9725e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1216/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.7436e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1217/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.7012e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1218/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 3.9133e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:44:19.830870: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.8827e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1219/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.6449e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1220/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.6864e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1221/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.6499e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1222/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.8403e-05 - val_loss: 0.0234 - learning_rate: 5.0000e-06\n",
      "Epoch 1223/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.7755e-05 - val_loss: 0.0266 - learning_rate: 5.0000e-06\n",
      "Epoch 1224/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.6355e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1225/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.7832e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1226/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.8229e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 1227/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.7047e-05 - val_loss: 0.0236 - learning_rate: 5.0000e-06\n",
      "Epoch 1228/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.7582e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1229/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.9377e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1230/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.7599e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1231/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.6407e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1232/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.7313e-05 - val_loss: 0.0237 - learning_rate: 5.0000e-06\n",
      "Epoch 1233/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.7832e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1234/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.6256e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1235/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.7344e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1236/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.7325e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1237/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.8177e-05 - val_loss: 0.0258 - learning_rate: 5.0000e-06\n",
      "Epoch 1238/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.6546e-05 - val_loss: 0.0233 - learning_rate: 5.0000e-06\n",
      "Epoch 1239/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.8389e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1240/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.6063e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1241/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.5224e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1242/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.7088e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1243/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 3.5335e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:44:49.834606: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.5705e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1244/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.7579e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1245/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.5799e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1246/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.7283e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1247/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.6514e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 1248/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.6048e-05 - val_loss: 0.0261 - learning_rate: 5.0000e-06\n",
      "Epoch 1249/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.6123e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1250/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.5275e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1251/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.5602e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1252/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.7152e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1253/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.5642e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1254/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.7236e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1255/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.7103e-05 - val_loss: 0.0236 - learning_rate: 5.0000e-06\n",
      "Epoch 1256/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.6888e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 1257/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.5813e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1258/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.8285e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 1259/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.7124e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1260/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.6713e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 1261/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.6034e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1262/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.6479e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1263/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.6857e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1264/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.5437e-05 - val_loss: 0.0259 - learning_rate: 5.0000e-06\n",
      "Epoch 1265/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.5939e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1266/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.6036e-05 - val_loss: 0.0261 - learning_rate: 5.0000e-06\n",
      "Epoch 1267/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.5921e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1268/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 3.6594e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:45:19.865147: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.6512e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1269/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.5232e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1270/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.7101e-05 - val_loss: 0.0238 - learning_rate: 5.0000e-06\n",
      "Epoch 1271/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.5385e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 1272/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.6094e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1273/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.4605e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1274/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.5290e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1275/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.6878e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1276/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.5482e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1277/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.5282e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1278/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.5440e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1279/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.6392e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1280/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.5513e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1281/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 3.6029e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1282/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.3818e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1283/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.6754e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1284/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.6413e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1285/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.6106e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1286/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.5275e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1287/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.4303e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1288/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.5644e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1289/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.4582e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1290/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.3663e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1291/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.5791e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1292/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.6594e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1293/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.6966e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1294/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 3.7227e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:45:50.588297: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.6755e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1295/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.6008e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1296/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.5223e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1297/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.4734e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1298/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.5072e-05 - val_loss: 0.0258 - learning_rate: 5.0000e-06\n",
      "Epoch 1299/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.4905e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1300/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.6168e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1301/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.5154e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1302/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.5783e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1303/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.5673e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1304/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.5030e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1305/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.4448e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 1306/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.4500e-05 - val_loss: 0.0234 - learning_rate: 5.0000e-06\n",
      "Epoch 1307/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.5055e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 1308/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.5835e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1309/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.5736e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1310/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.5595e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1311/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.4055e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1312/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.6484e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1313/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.4361e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1314/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.5414e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1315/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.4201e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1316/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.5483e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1317/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.5752e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1318/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.4087e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1319/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 3.5403e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:46:20.627614: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.5360e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1320/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.5296e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1321/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.4969e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 1322/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.4037e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1323/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.3094e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1324/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.4297e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1325/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.4252e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1326/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.5432e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1327/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.3665e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1328/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.5649e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1329/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.4221e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1330/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.4004e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1331/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.5258e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1332/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.4938e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1333/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.3918e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 1334/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.4030e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1335/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.4942e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1336/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.4815e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 1337/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.5128e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 1338/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.4737e-05 - val_loss: 0.0259 - learning_rate: 5.0000e-06\n",
      "Epoch 1339/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.4233e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1340/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.5865e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1341/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.4385e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1342/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.3496e-05 - val_loss: 0.0238 - learning_rate: 5.0000e-06\n",
      "Epoch 1343/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.4392e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 1344/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 3.3606e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:46:50.638069: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.3670e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1345/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.4211e-05 - val_loss: 0.0234 - learning_rate: 5.0000e-06\n",
      "Epoch 1346/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.4062e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 1347/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.4170e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1348/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.5252e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1349/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.3648e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1350/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.3845e-05 - val_loss: 0.0238 - learning_rate: 5.0000e-06\n",
      "Epoch 1351/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.4872e-05 - val_loss: 0.0234 - learning_rate: 5.0000e-06\n",
      "Epoch 1352/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.3763e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1353/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.3860e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1354/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.4480e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 1355/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.4541e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1356/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.3696e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1357/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.2281e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1358/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.3625e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1359/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.3764e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1360/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.4158e-05 - val_loss: 0.0261 - learning_rate: 5.0000e-06\n",
      "Epoch 1361/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.4956e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1362/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.3788e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 1363/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2673e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1364/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.2892e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1365/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.4041e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1366/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 3.3939e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1367/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.3270e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1368/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.3646e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1369/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.3517e-05 - val_loss: 0.0232 - learning_rate: 5.0000e-06\n",
      "Epoch 1370/2000\n",
      "\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 264ms/step - loss: 3.6154e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:47:20.667034: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.4554e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1371/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.3693e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1372/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:47:22.890012: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.2390e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1373/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.3207e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 1374/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.4093e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1375/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.3202e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 1376/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.3593e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1377/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.4689e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1378/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.3824e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1379/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step - loss: 3.2683e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1380/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.4537e-05 - val_loss: 0.0258 - learning_rate: 5.0000e-06\n",
      "Epoch 1381/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2235e-05 - val_loss: 0.0237 - learning_rate: 5.0000e-06\n",
      "Epoch 1382/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1684e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1383/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.3504e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1384/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.3851e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1385/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.4164e-05 - val_loss: 0.0259 - learning_rate: 5.0000e-06\n",
      "Epoch 1386/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.4381e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1387/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.4423e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 1388/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 3.3698e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1389/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.2197e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1390/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.3080e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1391/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2380e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1392/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.3906e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1393/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.3677e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 1394/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2557e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1395/2000\n",
      "\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 270ms/step - loss: 3.6575e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:47:50.781818: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.3541e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1396/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.3221e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1397/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.2511e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1398/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.3842e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1399/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.2090e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1400/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.4362e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1401/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.2550e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1402/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.3572e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 1403/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2593e-05 - val_loss: 0.0258 - learning_rate: 5.0000e-06\n",
      "Epoch 1404/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.2461e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1405/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.3735e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 1406/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.4427e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1407/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.2480e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1408/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.3018e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1409/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.2325e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1410/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.2180e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1411/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 113ms/step - loss: 3.1624e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1412/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 251ms/step - loss: 3.2996e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1413/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.4276e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1414/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1679e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1415/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 127ms/step - loss: 3.2547e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1416/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - loss: 3.2588e-05 - val_loss: 0.0258 - learning_rate: 5.0000e-06\n",
      "Epoch 1417/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 3.1310e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:48:21.240127: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step - loss: 3.1409e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1418/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 253ms/step - loss: 3.3500e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1419/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.3062e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1420/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.3825e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1421/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.1930e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1422/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.3252e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1423/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.3805e-05 - val_loss: 0.0260 - learning_rate: 5.0000e-06\n",
      "Epoch 1424/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 3.2078e-05 - val_loss: 0.0260 - learning_rate: 5.0000e-06\n",
      "Epoch 1425/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1225e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 1426/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 3.2651e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 1427/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.2313e-05 - val_loss: 0.0259 - learning_rate: 5.0000e-06\n",
      "Epoch 1428/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2012e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1429/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.2404e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1430/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.2518e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 1431/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.2569e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1432/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.2683e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1433/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.1393e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1434/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.1671e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1435/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.1814e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1436/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.2094e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1437/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.2505e-05 - val_loss: 0.0261 - learning_rate: 5.0000e-06\n",
      "Epoch 1438/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.0733e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1439/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.2898e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1440/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1575e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1441/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 3.1276e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:48:51.667523: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1168e-05 - val_loss: 0.0238 - learning_rate: 5.0000e-06\n",
      "Epoch 1442/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1973e-05 - val_loss: 0.0259 - learning_rate: 5.0000e-06\n",
      "Epoch 1443/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.2286e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1444/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.2442e-05 - val_loss: 0.0233 - learning_rate: 5.0000e-06\n",
      "Epoch 1445/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.1301e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 1446/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.3203e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 1447/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.1319e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1448/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - loss: 3.2028e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1449/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.3104e-05 - val_loss: 0.0262 - learning_rate: 5.0000e-06\n",
      "Epoch 1450/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.1736e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1451/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2252e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1452/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.2633e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1453/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1775e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1454/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1893e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 1455/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.1547e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1456/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 3.2096e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 1457/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.2320e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1458/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.1101e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1459/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.1986e-05 - val_loss: 0.0233 - learning_rate: 5.0000e-06\n",
      "Epoch 1460/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.0889e-05 - val_loss: 0.0261 - learning_rate: 5.0000e-06\n",
      "Epoch 1461/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1993e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1462/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1299e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1463/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.0703e-05 - val_loss: 0.0259 - learning_rate: 5.0000e-06\n",
      "Epoch 1464/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.1241e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1465/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.1163e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1466/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 3.1829e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:49:21.818197: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1676e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 1467/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2031e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1468/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1388e-05 - val_loss: 0.0261 - learning_rate: 5.0000e-06\n",
      "Epoch 1469/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.0983e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1470/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.1123e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1471/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.1090e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1472/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.1583e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1473/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.2573e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 1474/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.1492e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1475/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2717e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1476/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.1015e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1477/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.0832e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1478/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - loss: 3.1509e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 1479/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1060e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1480/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.1238e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1481/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2170e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1482/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.0336e-05 - val_loss: 0.0263 - learning_rate: 5.0000e-06\n",
      "Epoch 1483/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.0782e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1484/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.0152e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1485/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.9136e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1486/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1660e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1487/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1685e-05 - val_loss: 0.0262 - learning_rate: 5.0000e-06\n",
      "Epoch 1488/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.0904e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1489/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.0405e-05 - val_loss: 0.0258 - learning_rate: 5.0000e-06\n",
      "Epoch 1490/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.1157e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1491/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 3.1388e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1492/2000\n",
      "\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 249ms/step - loss: 3.7249e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:49:51.989494: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.2371e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1493/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9924e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1494/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.1864e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1495/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.0551e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1496/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9606e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 1497/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1253e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1498/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.1123e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1499/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.0135e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1500/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.0035e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1501/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.0623e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1502/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.9591e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1503/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.0545e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 1504/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 135ms/step - loss: 3.0022e-05 - val_loss: 0.0264 - learning_rate: 5.0000e-06\n",
      "Epoch 1505/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - loss: 3.1688e-05 - val_loss: 0.0263 - learning_rate: 5.0000e-06\n",
      "Epoch 1506/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.1482e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1507/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.0793e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1508/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 2.9842e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1509/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 2.9943e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1510/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.9787e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1511/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 3.0778e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1512/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - loss: 3.0078e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1513/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 3.0365e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1514/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - loss: 3.0280e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1515/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.0881e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1516/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 3.0220e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:50:22.459031: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 3.0182e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1517/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - loss: 2.9614e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1518/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - loss: 2.9700e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1519/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 3.1665e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 1520/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 3.0815e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1521/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - loss: 2.8786e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1522/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.0488e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1523/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 3.1327e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1524/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 2.9499e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1525/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 2.9751e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1526/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.9489e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1527/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 3.0219e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1528/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.9684e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1529/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.9284e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1530/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 122ms/step - loss: 3.1205e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1531/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 3.0481e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1532/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.0581e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1533/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.9876e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 1534/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.0126e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1535/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.9769e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1536/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 3.0776e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 1537/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 3.0634e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 1538/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.9750e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1539/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.0622e-05 - val_loss: 0.0259 - learning_rate: 5.0000e-06\n",
      "Epoch 1540/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 2.9356e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1541/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 2.9345e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:50:53.145367: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9146e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1542/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.0177e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1543/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - loss: 2.9987e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1544/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 2.9626e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1545/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - loss: 2.9728e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1546/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.9285e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 1547/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.9464e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1548/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.0151e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1549/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 3.0309e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1550/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.8684e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1551/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.9461e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1552/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9479e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 1553/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - loss: 3.0450e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1554/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.9603e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1555/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - loss: 2.8569e-05 - val_loss: 0.0259 - learning_rate: 5.0000e-06\n",
      "Epoch 1556/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - loss: 2.9521e-05 - val_loss: 0.0258 - learning_rate: 5.0000e-06\n",
      "Epoch 1557/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.8321e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1558/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.9463e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1559/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.9321e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1560/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 3.0414e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1561/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.8813e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1562/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.7955e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1563/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 2.8754e-05 - val_loss: 0.0259 - learning_rate: 5.0000e-06\n",
      "Epoch 1564/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.9811e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 1565/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.9225e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1566/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.8272e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1567/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:51:23.684203: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.8515e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1568/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 2.9482e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1569/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.9563e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1570/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.9762e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1571/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.9234e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1572/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9494e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1573/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.9406e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1574/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.0091e-05 - val_loss: 0.0261 - learning_rate: 5.0000e-06\n",
      "Epoch 1575/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.9107e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1576/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.0231e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1577/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 2.9082e-05 - val_loss: 0.0237 - learning_rate: 5.0000e-06\n",
      "Epoch 1578/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.8774e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1579/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.9668e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 1580/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.8508e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 1581/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.8369e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 1582/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.8420e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1583/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.9094e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1584/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.8453e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1585/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.9043e-05 - val_loss: 0.0260 - learning_rate: 5.0000e-06\n",
      "Epoch 1586/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.8464e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 1587/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.8083e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1588/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9562e-05 - val_loss: 0.0260 - learning_rate: 5.0000e-06\n",
      "Epoch 1589/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.8872e-05 - val_loss: 0.0265 - learning_rate: 5.0000e-06\n",
      "Epoch 1590/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.0046e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1591/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 2.7938e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:51:53.903192: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.8073e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1592/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.9869e-05 - val_loss: 0.0259 - learning_rate: 5.0000e-06\n",
      "Epoch 1593/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.8467e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1594/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.8634e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1595/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.8606e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1596/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - loss: 2.8056e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1597/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 2.8593e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1598/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.9302e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1599/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.9110e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1600/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.8463e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 1601/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.9305e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1602/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.8048e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1603/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.8289e-05 - val_loss: 0.0258 - learning_rate: 5.0000e-06\n",
      "Epoch 1604/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 2.8877e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1605/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.9135e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 1606/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.8414e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 1607/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.9340e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1608/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.9189e-05 - val_loss: 0.0261 - learning_rate: 5.0000e-06\n",
      "Epoch 1609/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.8137e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1610/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - loss: 2.9324e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1611/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.8894e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1612/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.9451e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 1613/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.8917e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1614/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.8179e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1615/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.8145e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1616/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 2.7399e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:52:24.703616: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.7481e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1617/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.8432e-05 - val_loss: 0.0262 - learning_rate: 5.0000e-06\n",
      "Epoch 1618/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.8837e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1619/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.7960e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1620/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.7572e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1621/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.7947e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1622/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.9373e-05 - val_loss: 0.0238 - learning_rate: 5.0000e-06\n",
      "Epoch 1623/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.7045e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1624/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.8477e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1625/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.8028e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 1626/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.8302e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 1627/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.7385e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1628/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.7676e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1629/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9687e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1630/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.7741e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 1631/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.7590e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1632/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.7577e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1633/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.8507e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1634/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.8700e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1635/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.7320e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1636/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.8155e-05 - val_loss: 0.0258 - learning_rate: 5.0000e-06\n",
      "Epoch 1637/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.8766e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1638/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.7399e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1639/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 2.9424e-05 - val_loss: 0.0263 - learning_rate: 5.0000e-06\n",
      "Epoch 1640/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.7544e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1641/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 2.8490e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:52:55.270418: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.8447e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1642/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.7042e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 1643/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.7239e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1644/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.7818e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1645/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.7893e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 1646/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.8514e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1647/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.8337e-05 - val_loss: 0.0259 - learning_rate: 5.0000e-06\n",
      "Epoch 1648/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.7493e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1649/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.8058e-05 - val_loss: 0.0262 - learning_rate: 5.0000e-06\n",
      "Epoch 1650/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.7970e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1651/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.8416e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1652/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.7499e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 1653/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.7537e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1654/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.7490e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1655/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.7323e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1656/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.8701e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 1657/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.8423e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 1658/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 2.7243e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1659/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 2.7039e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 1660/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.8442e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1661/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.7767e-05 - val_loss: 0.0262 - learning_rate: 5.0000e-06\n",
      "Epoch 1662/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.6208e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1663/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.8173e-05 - val_loss: 0.0237 - learning_rate: 5.0000e-06\n",
      "Epoch 1664/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.7264e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1665/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.7809e-05 - val_loss: 0.0259 - learning_rate: 5.0000e-06\n",
      "Epoch 1666/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 2.7597e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:53:25.481126: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.7587e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 1667/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.7514e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1668/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.7342e-05 - val_loss: 0.0236 - learning_rate: 5.0000e-06\n",
      "Epoch 1669/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.7432e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 1670/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.7327e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1671/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.7606e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1672/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.8061e-05 - val_loss: 0.0262 - learning_rate: 5.0000e-06\n",
      "Epoch 1673/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.8118e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1674/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.7177e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1675/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.6733e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1676/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6188e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1677/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.7761e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1678/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.7644e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 1679/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 2.6872e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1680/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.6476e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 1681/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - loss: 2.7161e-05 - val_loss: 0.0260 - learning_rate: 5.0000e-06\n",
      "Epoch 1682/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6782e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1683/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.7698e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1684/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.7608e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1685/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.7396e-05 - val_loss: 0.0259 - learning_rate: 5.0000e-06\n",
      "Epoch 1686/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.7665e-05 - val_loss: 0.0263 - learning_rate: 5.0000e-06\n",
      "Epoch 1687/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.7653e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1688/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.7096e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1689/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5629e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 1690/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - loss: 2.6918e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1691/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 2.8613e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:53:56.026248: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.8156e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1692/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.7724e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1693/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6114e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1694/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 2.6510e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1695/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.6360e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1696/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5912e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1697/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5804e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1698/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.7680e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1699/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6695e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1700/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.7589e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1701/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.7143e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1702/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6950e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1703/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.7261e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 1704/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6222e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1705/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5562e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 1706/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6824e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1707/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - loss: 2.6316e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1708/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.6362e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 1709/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.6647e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1710/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.7121e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1711/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6909e-05 - val_loss: 0.0262 - learning_rate: 5.0000e-06\n",
      "Epoch 1712/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.6300e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1713/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6925e-05 - val_loss: 0.0261 - learning_rate: 5.0000e-06\n",
      "Epoch 1714/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 2.6753e-05 - val_loss: 0.0259 - learning_rate: 5.0000e-06\n",
      "Epoch 1715/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.6799e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 1716/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 2.6146e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:54:26.568174: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.6261e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1717/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.7526e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1718/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6243e-05 - val_loss: 0.0258 - learning_rate: 5.0000e-06\n",
      "Epoch 1719/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.7241e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1720/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.6722e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1721/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.5345e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1722/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.5780e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1723/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 2.6080e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1724/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.6335e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1725/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6329e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1726/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.7256e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1727/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.7197e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 1728/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5752e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1729/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5461e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1730/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6196e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1731/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6062e-05 - val_loss: 0.0259 - learning_rate: 5.0000e-06\n",
      "Epoch 1732/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6055e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1733/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6401e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1734/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.7090e-05 - val_loss: 0.0261 - learning_rate: 5.0000e-06\n",
      "Epoch 1735/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5911e-05 - val_loss: 0.0234 - learning_rate: 5.0000e-06\n",
      "Epoch 1736/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.6427e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1737/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6171e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1738/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.6253e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1739/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5466e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1740/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.6329e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 1741/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 2.6783e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:54:56.761884: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6562e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1742/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6807e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1743/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5176e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 1744/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.6051e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1745/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6786e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1746/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5606e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1747/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5341e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1748/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6517e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1749/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 2.7028e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1750/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6505e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1751/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5805e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1752/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.5596e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1753/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5580e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1754/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6265e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 1755/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5309e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1756/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 2.5252e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1757/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.6142e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1758/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.4745e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 1759/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5776e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1760/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6199e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1761/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6364e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1762/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6825e-05 - val_loss: 0.0259 - learning_rate: 5.0000e-06\n",
      "Epoch 1763/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6884e-05 - val_loss: 0.0234 - learning_rate: 5.0000e-06\n",
      "Epoch 1764/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5509e-05 - val_loss: 0.0258 - learning_rate: 5.0000e-06\n",
      "Epoch 1765/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6940e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1766/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 2.5788e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:55:27.107768: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.5790e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1767/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.7024e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1768/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5209e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1769/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.5004e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1770/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.5298e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1771/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6393e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1772/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.6085e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1773/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5193e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1774/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5992e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1775/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5221e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1776/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5013e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 1777/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.5215e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1778/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4969e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1779/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4868e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1780/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6136e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1781/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.5369e-05 - val_loss: 0.0258 - learning_rate: 5.0000e-06\n",
      "Epoch 1782/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5166e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1783/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4874e-05 - val_loss: 0.0261 - learning_rate: 5.0000e-06\n",
      "Epoch 1784/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5320e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1785/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5854e-05 - val_loss: 0.0258 - learning_rate: 5.0000e-06\n",
      "Epoch 1786/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5187e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1787/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4862e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1788/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.5249e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1789/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.4946e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1790/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5404e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1791/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.4713e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1792/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:55:57.130154: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5174e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 1793/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4472e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1794/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 121ms/step - loss: 2.5789e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1795/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5019e-05 - val_loss: 0.0235 - learning_rate: 5.0000e-06\n",
      "Epoch 1796/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5346e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 1797/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5855e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1798/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 2.5325e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 1799/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4791e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1800/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4894e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 1801/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5764e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 1802/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.5265e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1803/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5381e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1804/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3848e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1805/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4879e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1806/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3541e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1807/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4652e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1808/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5427e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1809/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5083e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1810/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4059e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1811/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.5304e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1812/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4909e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 1813/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5350e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 1814/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5972e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1815/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.4119e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1816/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 2.5214e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:56:27.406273: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5150e-05 - val_loss: 0.0268 - learning_rate: 5.0000e-06\n",
      "Epoch 1817/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.4460e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1818/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4975e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1819/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5326e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1820/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - loss: 2.5363e-05 - val_loss: 0.0234 - learning_rate: 5.0000e-06\n",
      "Epoch 1821/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5558e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1822/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.5473e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1823/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4882e-05 - val_loss: 0.0236 - learning_rate: 5.0000e-06\n",
      "Epoch 1824/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.5114e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1825/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.5529e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1826/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4148e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1827/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5181e-05 - val_loss: 0.0237 - learning_rate: 5.0000e-06\n",
      "Epoch 1828/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5168e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1829/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.4137e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1830/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4940e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1831/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5635e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1832/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - loss: 2.4430e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1833/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5050e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1834/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4889e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1835/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.4470e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1836/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5247e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1837/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5064e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1838/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3922e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1839/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.4572e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1840/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 2.4710e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1841/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 2.4579e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:56:57.627830: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4458e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1842/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5460e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1843/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5629e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1844/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.4068e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1845/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4794e-05 - val_loss: 0.0265 - learning_rate: 5.0000e-06\n",
      "Epoch 1846/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4985e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1847/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4339e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1848/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4797e-05 - val_loss: 0.0267 - learning_rate: 5.0000e-06\n",
      "Epoch 1849/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.4685e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 1850/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4516e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 1851/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3796e-05 - val_loss: 0.0259 - learning_rate: 5.0000e-06\n",
      "Epoch 1852/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4881e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1853/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.4005e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1854/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3227e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1855/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3785e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1856/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4044e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1857/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3655e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1858/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.4751e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1859/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4518e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1860/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5027e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1861/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4553e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1862/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.4478e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1863/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.3460e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1864/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - loss: 2.4086e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1865/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.4968e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1866/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 2.3227e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:57:27.775668: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3418e-05 - val_loss: 0.0240 - learning_rate: 5.0000e-06\n",
      "Epoch 1867/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3696e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1868/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3544e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1869/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.5246e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1870/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3591e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1871/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3255e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1872/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.4372e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1873/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4034e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1874/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4817e-05 - val_loss: 0.0234 - learning_rate: 5.0000e-06\n",
      "Epoch 1875/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.2835e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1876/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.4535e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1877/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3808e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 1878/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.3514e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1879/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4438e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1880/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3540e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1881/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3932e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 1882/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3714e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1883/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.4538e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1884/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3687e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 1885/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4545e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1886/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3634e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1887/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.3722e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1888/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.3603e-05 - val_loss: 0.0260 - learning_rate: 5.0000e-06\n",
      "Epoch 1889/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4077e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 1890/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.4605e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1891/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4325e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1892/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:57:57.981786: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3608e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1893/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.2745e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1894/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step - loss: 2.3069e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1895/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4603e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1896/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3300e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1897/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3874e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1898/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.4778e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1899/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.2936e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1900/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.2991e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 1901/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5280e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1902/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4277e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1903/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.3638e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1904/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4322e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1905/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.3501e-05 - val_loss: 0.0247 - learning_rate: 5.0000e-06\n",
      "Epoch 1906/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3464e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1907/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.3334e-05 - val_loss: 0.0259 - learning_rate: 5.0000e-06\n",
      "Epoch 1908/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.4243e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1909/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.2936e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1910/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3576e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1911/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.3604e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 1912/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.3673e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1913/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3015e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1914/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.3793e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1915/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.2809e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1916/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3718e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1917/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:58:28.184848: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3636e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 1918/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.2838e-05 - val_loss: 0.0242 - learning_rate: 5.0000e-06\n",
      "Epoch 1919/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3198e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1920/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - loss: 2.2993e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1921/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3270e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1922/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4420e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1923/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3500e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1924/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3628e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1925/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.2903e-05 - val_loss: 0.0241 - learning_rate: 5.0000e-06\n",
      "Epoch 1926/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4043e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1927/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4134e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1928/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.2732e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1929/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4312e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1930/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.2608e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1931/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3062e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1932/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3065e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1933/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.2421e-05 - val_loss: 0.0262 - learning_rate: 5.0000e-06\n",
      "Epoch 1934/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.2817e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1935/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3463e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1936/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.2215e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1937/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 2.3149e-05 - val_loss: 0.0259 - learning_rate: 5.0000e-06\n",
      "Epoch 1938/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3463e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1939/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3141e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1940/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3193e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1941/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4484e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1942/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:58:58.238052: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.2530e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1943/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.2940e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1944/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.2612e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1945/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4659e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1946/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 122ms/step - loss: 2.3048e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1947/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3095e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1948/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3493e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1949/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.2979e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1950/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3008e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1951/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.2672e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1952/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.1910e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1953/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.3238e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 1954/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3586e-05 - val_loss: 0.0258 - learning_rate: 5.0000e-06\n",
      "Epoch 1955/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.2946e-05 - val_loss: 0.0263 - learning_rate: 5.0000e-06\n",
      "Epoch 1956/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.3014e-05 - val_loss: 0.0239 - learning_rate: 5.0000e-06\n",
      "Epoch 1957/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.3236e-05 - val_loss: 0.0246 - learning_rate: 5.0000e-06\n",
      "Epoch 1958/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.1908e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1959/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3862e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 1960/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.2612e-05 - val_loss: 0.0255 - learning_rate: 5.0000e-06\n",
      "Epoch 1961/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3499e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1962/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.1881e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1963/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.2864e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1964/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.2056e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1965/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3560e-05 - val_loss: 0.0259 - learning_rate: 5.0000e-06\n",
      "Epoch 1966/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.2299e-05 - val_loss: 0.0260 - learning_rate: 5.0000e-06\n",
      "Epoch 1967/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:59:28.316048: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.2238e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1968/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.2149e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1969/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.2728e-05 - val_loss: 0.0245 - learning_rate: 5.0000e-06\n",
      "Epoch 1970/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.2801e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1971/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.2078e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1972/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.2927e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1973/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.2111e-05 - val_loss: 0.0258 - learning_rate: 5.0000e-06\n",
      "Epoch 1974/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.2627e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1975/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.2317e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1976/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.3195e-05 - val_loss: 0.0260 - learning_rate: 5.0000e-06\n",
      "Epoch 1977/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.2903e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1978/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.2374e-05 - val_loss: 0.0254 - learning_rate: 5.0000e-06\n",
      "Epoch 1979/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.1876e-05 - val_loss: 0.0253 - learning_rate: 5.0000e-06\n",
      "Epoch 1980/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3055e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1981/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.1926e-05 - val_loss: 0.0244 - learning_rate: 5.0000e-06\n",
      "Epoch 1982/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.2612e-05 - val_loss: 0.0268 - learning_rate: 5.0000e-06\n",
      "Epoch 1983/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.2003e-05 - val_loss: 0.0258 - learning_rate: 5.0000e-06\n",
      "Epoch 1984/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.2887e-05 - val_loss: 0.0265 - learning_rate: 5.0000e-06\n",
      "Epoch 1985/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.2294e-05 - val_loss: 0.0251 - learning_rate: 5.0000e-06\n",
      "Epoch 1986/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.2365e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 1987/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.2244e-05 - val_loss: 0.0257 - learning_rate: 5.0000e-06\n",
      "Epoch 1988/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.1691e-05 - val_loss: 0.0261 - learning_rate: 5.0000e-06\n",
      "Epoch 1989/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.1969e-05 - val_loss: 0.0256 - learning_rate: 5.0000e-06\n",
      "Epoch 1990/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.1723e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1991/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.1783e-05 - val_loss: 0.0258 - learning_rate: 5.0000e-06\n",
      "Epoch 1992/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:59:58.448287: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.2453e-05 - val_loss: 0.0248 - learning_rate: 5.0000e-06\n",
      "Epoch 1993/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.2219e-05 - val_loss: 0.0243 - learning_rate: 5.0000e-06\n",
      "Epoch 1994/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.2180e-05 - val_loss: 0.0250 - learning_rate: 5.0000e-06\n",
      "Epoch 1995/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.1523e-05 - val_loss: 0.0252 - learning_rate: 5.0000e-06\n",
      "Epoch 1996/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.2420e-05 - val_loss: 0.0265 - learning_rate: 5.0000e-06\n",
      "Epoch 1997/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.2312e-05 - val_loss: 0.0238 - learning_rate: 5.0000e-06\n",
      "Epoch 1998/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.2473e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n",
      "Epoch 1999/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.2544e-05 - val_loss: 0.0261 - learning_rate: 5.0000e-06\n",
      "Epoch 2000/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.1588e-05 - val_loss: 0.0249 - learning_rate: 5.0000e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model with the custom callback\n",
    "history = model_builder.train_model(\n",
    "    train_dataset, \n",
    "    val_dataset, \n",
    "    epochs=2000,\n",
    "    callbacks_list=[lr_scheduler]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Extract the losses from the history object\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "\n",
    "train_loss_x_midpoints = history.history.get('x_midpoints_reshape_loss', train_loss)\n",
    "val_loss_x_midpoints = history.history.get('val_x_midpoints_reshape_loss', val_loss)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2,1)\n",
    "plt.plot(train_loss_x_midpoints, label='Train Loss x_midpoints_reshape')\n",
    "plt.plot(val_loss_x_midpoints, label='Validation Loss x_midpoints_reshape')\n",
    "plt.xlabel('Epochs')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss ')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_builder.model.save(\"/home/da886/Analysis/13KFixed_Mixed_5_32by32_95indexFor19KernelNoNoise.keras\")\n",
    "# loaded_model = tf.keras.models.load_model(\n",
    "# \"/home/da886/Analysis/30KFixed_13_SparsespotsrandomSPOTS.keras\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # Define the function for visualizing midpoints\n",
    "# def visualize_midpoints(image, midpoints, title=\"Predicted Midpoint Visualization\"):\n",
    "#     \"\"\"\n",
    "#     Visualizes midpoints on an image without using a probability vector.\n",
    "\n",
    "#     Parameters:\n",
    "#     - image: A 3D tensor representing the image.\n",
    "#     - midpoints: A 2D tensor representing the midpoint coordinates (x, y).\n",
    "#     - title: The title of the plot.\n",
    "\n",
    "#     Returns:\n",
    "#     None (displays the image with midpoints).\n",
    "#     \"\"\"\n",
    "#     # Convert to NumPy arrays for easier handling\n",
    "#     image_np = image\n",
    "#     midpoints_np = midpoints\n",
    "\n",
    "#     # Denormalize image if necessary (adjust based on your normalization method)\n",
    "#     denormalized_image = image_np  # Modify if normalization was applied during training\n",
    "\n",
    "#     # Visualize the image\n",
    "#     plt.figure(figsize=(4, 4))\n",
    "#     plt.imshow(denormalized_image, cmap='gray')\n",
    "#     plt.title(title)\n",
    "\n",
    "#     # Plot midpoints directly, only if they are not (0, 0)\n",
    "#     for i, (x, y) in enumerate(midpoints_np):\n",
    "#         if x >= 0 and y >= 0:  # Only plot if the point is not (0, 0)\n",
    "#             plt.scatter(x, y, color='red', s=5)\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "# # Create the validation dataset\n",
    "# # val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_midpoints))\n",
    "# # val_dataset = val_dataset.batch(800)\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_midpoints))\n",
    "# train_dataset = train_dataset.batch(800)\n",
    "# # inputs,targets = next(iter(train_dataset))\n",
    "# # outputs = model_builder.model.predict(inputs)\n",
    "# # # Initialize lists to collect the data\n",
    "# all_images = []\n",
    "# all_true_midpoints = []\n",
    "# all_pred_midpoints = []\n",
    "\n",
    "# # # Loop through each batch in the validation dataset, predict, and collect results\n",
    "# # for i, (data_batch, midpoints_batch) in enumerate(val_dataset):\n",
    "\n",
    "# for i, (data_batch, midpoints_batch) in enumerate(train_dataset):\n",
    "#     print(f\"Processing batch {i + 1}, batch shape: {data_batch.shape}\")\n",
    "    \n",
    "#     # Get the model predictions\n",
    "#     predictions =model_builder.model.predict(data_batch)\n",
    "\n",
    "#     # Extend the lists to store data from each batch\n",
    "#     all_images.extend(data_batch.numpy())  # Store all images\n",
    "#     all_true_midpoints.extend(midpoints_batch.numpy())  # Store all true midpoints\n",
    "#     all_pred_midpoints.extend(predictions)  # Store all predicted midpoints\n",
    "\n",
    "# # Convert lists to arrays for easier indexing\n",
    "# all_images = np.array(all_images)\n",
    "# all_true_midpoints = np.array(all_true_midpoints)\n",
    "# all_pred_midpoints = np.array(all_pred_midpoints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1, batch shape: (800, 32, 32)\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 21:10:15.477756: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 2, batch shape: (800, 32, 32)\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Processing batch 3, batch shape: (800, 32, 32)\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Processing batch 4, batch shape: (200, 32, 32)\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1729717816.560441  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.560863  386737 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.561087  386720 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.562130  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.562438  386737 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.562841  386720 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.563223  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.563797  386737 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.564093  386720 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.564453  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.564764  386737 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.565532  386720 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.565869  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.566350  386737 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.566740  386720 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.567130  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.567680  386737 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.568149  386720 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.568644  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.569128  386737 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.569928  386720 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.570320  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.570919  386737 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.571475  386720 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.572014  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.572591  386737 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.573123  386720 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.573689  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.574345  386737 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.574934  386720 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.575552  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.576169  386737 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.576774  386720 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.577376  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.577982  386737 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.578660  386720 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.579258  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.579981  386737 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.580681  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.581345  386720 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.582045  386737 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.582485  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.583195  386720 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.583887  386737 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.584440  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.585185  386720 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.586003  386737 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.586535  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.587152  386720 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.587826  386737 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.588506  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.589217  386720 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.589812  386737 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.590524  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.591202  386720 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.591807  386737 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.592989  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.593633  386720 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.594277  386737 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.595092  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.595937  386720 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.596769  386737 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.597377  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.597833  386720 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.598712  386737 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.599234  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.599519  386720 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.599932  386737 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.600180  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.600823  386720 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.601313  386737 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.601646  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.602606  386720 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.603080  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.603619  386737 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.604606  386720 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.605629  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.607058  386737 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.607650  386720 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.608301  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.609067  386737 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.609662  386720 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.611116  386758 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.611790  386737 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.612897  386720 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.613731  386737 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.614581  386720 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729717816.615059  386737 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the function for visualizing midpoints\n",
    "def visualize_midpoints_with_gt(image, true_midpoints, pred_midpoints, title=\"Predicted vs GT Midpoints\"):\n",
    "    \"\"\"\n",
    "    Visualizes ground truth and predicted midpoints on an image and draws lines to connect them.\n",
    "\n",
    "    Parameters:\n",
    "    - image: A 3D tensor representing the image.\n",
    "    - true_midpoints: A 2D tensor representing the ground truth midpoint coordinates (x, y).\n",
    "    - pred_midpoints: A 2D tensor representing the predicted midpoint coordinates (x, y).\n",
    "    - title: The title of the plot.\n",
    "\n",
    "    Returns:\n",
    "    None (displays the image with midpoints and lines).\n",
    "    \"\"\"\n",
    "    # Convert to NumPy arrays for easier handling\n",
    "    image_np = image\n",
    "\n",
    "    # Ensure that midpoints are in the shape (num_points, 2) for both true and predicted midpoints\n",
    "    true_midpoints_np = np.reshape(true_midpoints, (-1, 2))\n",
    "    pred_midpoints_np = np.reshape(pred_midpoints, (-1, 2))\n",
    "\n",
    "    # Visualize the image\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(image_np, cmap='gray')\n",
    "    plt.title(title)\n",
    "\n",
    "    # Plot both ground truth and predicted midpoints\n",
    "    for i, ((gt_x, gt_y), (pred_x, pred_y)) in enumerate(zip(true_midpoints_np, pred_midpoints_np)):\n",
    "        if gt_x >= 0 and gt_y >= 0:  # Only plot if the GT point is valid\n",
    "            plt.scatter(gt_x, gt_y, color='blue', label='Ground Truth' if i == 0 else \"\", s=30)\n",
    "            plt.scatter(pred_x, pred_y, color='red', label='Prediction' if i == 0 else \"\", s=30)\n",
    "\n",
    "            # Draw a line connecting the GT and predicted points\n",
    "            plt.plot([gt_x, pred_x], [gt_y, pred_y], color='green', linewidth=1)\n",
    "\n",
    "    # Add legend only once\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Create the training dataset\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_midpoints))\n",
    "# train_dataset = train_dataset.batch(800)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_midpoints))\n",
    "val_dataset = val_dataset.batch(800)\n",
    "# Initialize lists to collect the data\n",
    "all_images = []\n",
    "all_true_midpoints = []\n",
    "all_pred_midpoints = []\n",
    "\n",
    "# Loop through each batch in the training dataset, predict, and collect results\n",
    "# for i, (data_batch, midpoints_batch) in enumerate(train_dataset):\n",
    "for i, (data_batch, midpoints_batch) in enumerate(val_dataset):\n",
    "    print(f\"Processing batch {i + 1}, batch shape: {data_batch.shape}\")\n",
    "    \n",
    "    # Get the model predictions\n",
    "    predictions = model_builder.model.predict(data_batch)\n",
    "\n",
    "    # Extend the lists to store data from each batch\n",
    "    all_images.extend(data_batch.numpy())  # Store all images\n",
    "    all_true_midpoints.extend(midpoints_batch.numpy())  # Store all true midpoints\n",
    "    all_pred_midpoints.extend(predictions)  # Store all predicted midpoints\n",
    "\n",
    "# Convert lists to arrays for easier indexing\n",
    "all_images = np.array(all_images)\n",
    "all_true_midpoints = np.array(all_true_midpoints)\n",
    "all_pred_midpoints = np.array(all_pred_midpoints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index_to_visualize = np.random.randint(0, len(all_images))\n",
    "# Visualize the results for the first sample (you can change the index to visualize others)\n",
    "visualize_midpoints_with_gt(all_images[index_to_visualize ], all_true_midpoints[index_to_visualize ]*np.max(centers), all_pred_midpoints[index_to_visualize ]*np.max(centers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_midpoints))\n",
    "# train_dataset = train_dataset.batch(800)\n",
    "# inputs,targets = next(iter(train_dataset))\n",
    "# outputs = model_builder.model.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9600, 32, 32), (9600, 1, 13, 2), (9600, 1, 13, 2))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images.shape,all_pred_midpoints.shape,all_true_midpoints.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Select an index to visualize from the entire dataset\n",
    "index_to_visualize = np.random.randint(0, len(all_images))\n",
    "\n",
    "# index_to_visualize = 11548\n",
    "# Visualize the selected image with predicted and true midpoints\n",
    "visualize_midpoints(all_images[index_to_visualize], all_pred_midpoints[index_to_visualize, 0, :, :] * np.max(centers), title=\"Predicted Midpoints\")\n",
    "visualize_midpoints(all_images[index_to_visualize], all_true_midpoints[index_to_visualize, 0, :, :] * np.max(centers), title=\"Ground Truth Midpoints\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.843155e-06, 30.997658)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(all_pred_midpoints)*np.max(centers),np.max(all_pred_midpoints)*np.max(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 31.0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(all_true_midpoints)*np.max(centers),np.max(all_true_midpoints)*np.max(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.0, 31.0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(train_images), np.max(train_images), np.min(train_midpoints)*np.max(centers), np.max(train_midpoints)*np.max(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 4.2590027,  3.908059 ],\n",
       "         [27.263153 ,  4.3831873],\n",
       "         [ 5.8743353,  6.6574388],\n",
       "         [14.020337 ,  8.760142 ],\n",
       "         [ 4.1754484, 10.7192545],\n",
       "         [ 4.123699 , 11.2907295],\n",
       "         [22.326601 , 11.401616 ],\n",
       "         [ 7.203351 , 12.493043 ],\n",
       "         [ 5.814977 , 12.613149 ],\n",
       "         [21.202332 , 14.556165 ],\n",
       "         [ 8.0557995, 17.444569 ],\n",
       "         [ 8.055367 , 18.544151 ],\n",
       "         [ 5.2744675, 26.596989 ]]], dtype=float32),\n",
       " array([[[ 4.,  3.],\n",
       "         [28.,  6.],\n",
       "         [ 6.,  7.],\n",
       "         [14.,  9.],\n",
       "         [ 4., 10.],\n",
       "         [ 4., 10.],\n",
       "         [22., 11.],\n",
       "         [ 7., 12.],\n",
       "         [ 6., 14.],\n",
       "         [21., 15.],\n",
       "         [ 8., 17.],\n",
       "         [ 8., 18.],\n",
       "         [ 5., 28.]]], dtype=float32))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pred_midpoints[2]*np.max(centers),all_true_midpoints[2]*np.max(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1., 35., 12.],\n",
       "        [ 1., 63., 52.],\n",
       "        [ 1., 11., 43.],\n",
       "        ...,\n",
       "        [ 1., 10., 33.],\n",
       "        [ 1., 11., 48.],\n",
       "        [ 1., 59., 49.]],\n",
       "\n",
       "       [[ 1.,  5., 24.],\n",
       "        [ 1., 34., 55.],\n",
       "        [ 1., 42., 29.],\n",
       "        ...,\n",
       "        [ 1.,  2., 55.],\n",
       "        [ 1., 13., 17.],\n",
       "        [ 1., 45.,  8.]],\n",
       "\n",
       "       [[ 1., 38., 36.],\n",
       "        [ 1., 11., 33.],\n",
       "        [ 1., 40., 40.],\n",
       "        ...,\n",
       "        [ 1., 45.,  5.],\n",
       "        [ 1.,  8.,  7.],\n",
       "        [ 1., 55., 37.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1., 50., 18.],\n",
       "        [ 1., 20., 19.],\n",
       "        [ 1., 63.,  8.],\n",
       "        ...,\n",
       "        [ 1., 32., 44.],\n",
       "        [ 1., 63., 24.],\n",
       "        [ 1., 51., 52.]],\n",
       "\n",
       "       [[ 1.,  6., 33.],\n",
       "        [ 1., 23., 59.],\n",
       "        [ 1., 13., 48.],\n",
       "        ...,\n",
       "        [ 1., 26., 62.],\n",
       "        [ 1., 20., 42.],\n",
       "        [ 1., 53.,  2.]],\n",
       "\n",
       "       [[ 1., 24., 40.],\n",
       "        [ 1., 30., 43.],\n",
       "        [ 1., 38., 55.],\n",
       "        ...,\n",
       "        [ 1., 31., 56.],\n",
       "        [ 1., 21., 50.],\n",
       "        [ 1., 15.,  6.]]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[2],)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize variables to track the min and max MSE\n",
    "min_mse = float('inf')\n",
    "max_mse = float('-inf')\n",
    "min_mse_index = -1\n",
    "max_mse_index = -1\n",
    "\n",
    "# Loop through each prediction to calculate the MSE\n",
    "for i in range(len(all_pred_midpoints)):\n",
    "    mse = np.mean((all_pred_midpoints[i] - all_true_midpoints[i]) **2)\n",
    "    \n",
    "    if mse < min_mse:\n",
    "        min_mse = mse\n",
    "        min_mse_index = i\n",
    "    \n",
    "    if mse > max_mse:\n",
    "        max_mse = mse\n",
    "        max_mse_index = i\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot an image with its centers\n",
    "def plot_image_with_centers(image, true_center, predicted_center, title):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(image.squeeze(), cmap='gray')  # Display the image\n",
    "\n",
    "    # Plot the actual center (Groundtruth)\n",
    "    plt.scatter(true_center[:, 0], true_center[:, 1], color='green', label='Groundtruth', s=10)\n",
    "\n",
    "    # Plot the predicted center\n",
    "    plt.scatter(predicted_center[:, 0], predicted_center[:, 1], color='red', label='Predictions', s=10)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plotting the image with the least MSE\n",
    "plot_image_with_centers(all_images[min_mse_index],\n",
    "                        all_true_midpoints[min_mse_index][0] * np.max(centers),  # Adjust for scaling if needed\n",
    "                        all_pred_midpoints[min_mse_index][0] * np.max(centers),  # Adjust for scaling if needed\n",
    "                        f'Image with Least MSE. MSE: {min_mse:.4f}')\n",
    "\n",
    "# Plotting the image with the largest MSE\n",
    "plot_image_with_centers(all_images[max_mse_index],\n",
    "                        all_true_midpoints[max_mse_index][0] * np.max(centers),  # Adjust for scaling if needed\n",
    "                        all_pred_midpoints[max_mse_index][0] * np.max(centers),  # Adjust for scaling if needed\n",
    "                        f'Image with Largest MSE. MSE: {max_mse:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5266"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_mse_index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
