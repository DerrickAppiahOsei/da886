{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:13:31.416249: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-23 01:13:31.428835: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-23 01:13:31.441372: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-23 01:13:31.445158: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-23 01:13:31.456030: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-23 01:13:32.055236: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:13:33.575108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79196 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:4e:00.0, compute capability: 8.0\n",
      "2024-10-23 01:13:33.576649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 79196 MB memory:  -> device: 1, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:4f:00.0, compute capability: 8.0\n",
      "2024-10-23 01:13:33.577998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 79196 MB memory:  -> device: 2, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:c5:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3,4,5\"  # Only GPUs 0 and 1 will be visible to TensorFlow\n",
    "\n",
    "import tensorflow as tf\n",
    "strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:3\", \"/gpu:4\",\"/gpu:5\"])\n",
    "# strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# DataLoader Class Definition\n",
    "# -----------------------------\n",
    "class DataLoader:\n",
    "    def __init__(self, h5_filename):\n",
    "        self.h5_filename = h5_filename\n",
    "        self.images, self.centers = self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        with h5py.File(self.h5_filename, 'r') as f:\n",
    "            images = np.array(f['images'])\n",
    "            centers = np.array(f['centers_training'])\n",
    "        return images, centers\n",
    "\n",
    "    def plot_image_with_centers(self, image_index=None):\n",
    "        if image_index is None:\n",
    "            image_index = np.random.randint(0, len(self.images))\n",
    "\n",
    "        image = self.images[image_index]\n",
    "        centers = self.centers[image_index]\n",
    "\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        valid_centers = centers[centers[:, 0] == 1]\n",
    "        for center in valid_centers:\n",
    "            plt.scatter(center[1], center[2], c='red', marker='o',s=5)  # center[1] is x and center[2] is y\n",
    "        plt.title('Image with Valid Centers Marked')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_centers(centers):\n",
    "        return centers[np.lexsort((centers[:, 0], centers[:, 1]))]\n",
    "\n",
    "    def normalize_data(self):\n",
    "        normalized_images = self.images / np.max(self.images)\n",
    "        sorted_centers = np.array([self.sort_centers(image_centers[:, 1:]) for image_centers in self.centers])\n",
    "        normalized_centers = sorted_centers / np.max(centers)\n",
    "\n",
    "        normalized_midpoints = tf.expand_dims(normalized_centers, axis=1)\n",
    "        return normalized_images, normalized_midpoints.numpy()\n",
    "\n",
    "    def split_data(self, train_size=0.8, random_state=42):\n",
    "        normalized_images, normalized_midpoints_np = self.normalize_data()\n",
    "        return train_test_split(normalized_images, normalized_midpoints_np, train_size=train_size, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Utility Function Definition\n",
    "# -----------------------------\n",
    "def plot_transposed_images_with_midpoints(dataset, image_indices=[0, 1, 2, 3]):\n",
    "    \"\"\"\n",
    "    Extracts multiple images and their midpoints from the given dataset, transposes the images, \n",
    "    corrects the midpoints, and plots the transposed images with the corrected midpoints.\n",
    "\n",
    "    Args:\n",
    "    - dataset (tf.data.Dataset): The dataset from which to extract the images and midpoints.\n",
    "    - image_indices (list): The indices of the images in the batch to visualize. Default is [0, 1, 2, 3].\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract a sample image batch and its corresponding midpoints from the dataset\n",
    "    sample_image_batch, sample_midpoints_batch = next(iter(dataset))\n",
    "\n",
    "    # Create a figure with 2x2 subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(6, 6))\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < len(image_indices):\n",
    "            # Select the specified image and corresponding midpoints from the batch\n",
    "            sample_image = np.array(sample_image_batch[image_indices[i]])\n",
    "            sample_midpoints = np.array(sample_midpoints_batch[image_indices[i]])\n",
    "\n",
    "            # Transpose the image\n",
    "            transposed_image = sample_image.T\n",
    "\n",
    "            # Correct the midpoints by swapping the x and y coordinates\n",
    "            transposed_midpoints_corrected = sample_midpoints[:, :, [1, 0]]\n",
    "\n",
    "            # Plot the transposed image with corrected midpoints\n",
    "            ax.imshow(transposed_image, cmap='gray')\n",
    "            ax.scatter(\n",
    "                transposed_midpoints_corrected[:, :, 0] * 31, \n",
    "                transposed_midpoints_corrected[:, :, 1] * 31, \n",
    "                c='red', marker='o', s=5\n",
    "            )\n",
    "            ax.set_title(f'Image {image_indices[i]} for this batch')\n",
    "        else:\n",
    "            ax.axis('off')  # If fewer than 4 images are requested, hide the unused subplots\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# Custom Loss and Callback Definitions\n",
    "# -----------------------------\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def custom_loss(exponent):\n",
    "    def loss(y_true, y_pred):\n",
    "        diff = tf.abs(y_true - y_pred)\n",
    "        powered_diff = tf.pow(diff, exponent)\n",
    "        return tf.reduce_mean(powered_diff)\n",
    "    return loss\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class DynamicExponentCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, initial_exponent, increment, update_frequency):\n",
    "        super().__init__()\n",
    "        self.exponent = initial_exponent\n",
    "        self.increment = increment\n",
    "        self.update_frequency = update_frequency\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.update_frequency == 0:\n",
    "            self.exponent += self.increment\n",
    "            print(f\"\\nEpoch {epoch + 1}: Increasing exponent to {self.exponent}\")\n",
    "            self.model.loss = self.custom_loss(self.exponent)\n",
    "\n",
    "    def custom_loss(self, exponent):\n",
    "        def loss(y_true, y_pred):\n",
    "            diff = tf.abs(y_true - y_pred)\n",
    "            powered_diff = tf.pow(diff, exponent)\n",
    "            return tf.reduce_mean(powered_diff)\n",
    "        return loss\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'initial_exponent': self.exponent,\n",
    "            'increment': self.increment,\n",
    "            'update_frequency': self.update_frequency,\n",
    "        })\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my NEW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, regularizers\n",
    "import tensorflow as tf\n",
    "\n",
    "class ModelBuilder:\n",
    "    def __init__(self, input_shape=(32, 32, 1), num_classes=5, num_coordinates=2, learning_rate=1e-3, weights_path=None, l1_reg=0.001, l2_reg=0.01):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.num_coordinates = num_coordinates\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l1_reg = l1_reg\n",
    "        self.l2_reg = l2_reg\n",
    "        self.model = self.build_model()\n",
    "\n",
    "        # Load weights if a path is provided\n",
    "        if weights_path is not None:\n",
    "            self.model.load_weights(weights_path)\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "\n",
    "    def build_model(self):\n",
    "        \n",
    "        l1 = regularizers.l1(self.l1_reg)\n",
    "        l2 = regularizers.l2(self.l2_reg)\n",
    "        \n",
    "        # # CBAM Attention Block\n",
    "        # def cbam_block(input_tensor, reduction_ratio=16):\n",
    "        #     \"\"\"CBAM block, which includes channel and spatial attention\"\"\"\n",
    "        #     # Channel Attention\n",
    "        #     channel_attention = layers.GlobalAveragePooling2D()(input_tensor)\n",
    "        #     channel_attention = layers.Reshape((1, 1, input_tensor.shape[-1]))(channel_attention)\n",
    "        #     channel_attention = layers.Dense(input_tensor.shape[-1] // reduction_ratio, activation='relu')(channel_attention)\n",
    "        #     channel_attention = layers.Dense(input_tensor.shape[-1], activation='sigmoid')(channel_attention)\n",
    "        #     x = layers.Multiply()([input_tensor, channel_attention])\n",
    "\n",
    "        #     # Spatial Attention\n",
    "        #     avg_pool = layers.Lambda(lambda x: tf.reduce_mean(x, axis=-1, keepdims=True))(x)\n",
    "        #     max_pool = layers.Lambda(lambda x: tf.reduce_max(x, axis=-1, keepdims=True))(x)\n",
    "        #     concat = layers.Concatenate(axis=-1)([avg_pool, max_pool])\n",
    "        #     spatial_attention = layers.Conv2D(1, kernel_size=7, padding='same', activation='sigmoid')(concat)\n",
    "        #     x = layers.Multiply()([x, spatial_attention])\n",
    "\n",
    "        #     return x\n",
    "    \n",
    "        x_input = layers.Input(shape=self.input_shape)\n",
    "        \n",
    "        x = layers.Conv2D(512, kernel_size=19, padding='same', activation='relu')(x_input)\n",
    "       \n",
    "        \n",
    "        x = layers.Flatten()(x)\n",
    "        \n",
    "        x_midpoints = layers.Dense(self.num_classes * self.num_coordinates, activation='sigmoid', name='x_midpoints')(x)\n",
    "        x_midpoints_reshape = layers.Reshape((-1, self.num_classes, self.num_coordinates), name='x_midpoints_reshape')(x_midpoints)\n",
    "        \n",
    "        return tf.keras.models.Model(x_input, x_midpoints_reshape)\n",
    "\n",
    "    def compile_model(self, loss_function):\n",
    "        self.model.compile(optimizer=self.optimizer, loss=loss_function)\n",
    "\n",
    "    def train_model(self, train_dataset, val_dataset, epochs, callbacks_list):\n",
    "        history = self.model.fit(train_dataset, epochs=epochs, validation_data=val_dataset, callbacks=callbacks_list)\n",
    "        return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Main Script Execution\n",
    "# -----------------------------\n",
    "\n",
    "# Load data\n",
    "# h5_filename = '/home/da886/Final Electron counting project/Images and Labels/30KFixed_Mixed_13.h5'             \n",
    "h5_filename = '/home/da886/Final Electron counting project/Images and Labels/13KFixed_Mixed_5_32by32_95indexFor19kernel.h5'\n",
    "# h5_filename = '/home/da886/Final Electron counting project/Images and Labels/30KFixed_13_SparsespotsRandomIndex.h5'\n",
    "# h5_filename ='/home/da886/Final Electron counting project/Images and Labels/30KFixed-index6_13.h5'\n",
    "data_loader = DataLoader(h5_filename)\n",
    "images, centers = data_loader.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<matplotlib.image.AxesImage at 0x7efc546ff2c0>, 8071)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "l=np.random.randint(0,len(centers))\n",
    "data_loader.plot_image_with_centers(l)\n",
    "plt.imshow(images[l]),l\n",
    "# plt.grid(True),l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<matplotlib.image.AxesImage at 0x7f17b8157d90>, 5501)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[l]),l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  6.,  2.],\n",
       "       [ 1., 27., 30.],\n",
       "       [ 1., 27., 13.],\n",
       "       [ 1., 27., 31.],\n",
       "       [ 1.,  9.,  5.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.random.randint(0,len(centers))\n",
    "centers[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalize the data and split it into training and validation sets\n",
    "train_images, val_images, train_midpoints, val_midpoints = data_loader.split_data()\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_midpoints))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_midpoints))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.0, 1.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(train_images), np.max(train_images), np.min(train_midpoints), np.max(train_midpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images: (10400, 32, 32), Train Midpoints: (10400, 1, 5, 2)\n",
      "Validation Images: (2600, 32, 32), Validation Midpoints: (2600, 1, 5, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 1000\n",
    "train_dataset = train_dataset.shuffle(buffer_size=8000, reshuffle_each_iteration=True).batch(batch_size)\n",
    "val_dataset = val_dataset.shuffle(buffer_size=8000).batch(batch_size)\n",
    "\n",
    "# Check the shape of the datasets\n",
    "print(f'Train Images: {train_images.shape}, Train Midpoints: {train_midpoints.shape}')\n",
    "print(f'Validation Images: {val_images.shape}, Validation Midpoints: {val_midpoints.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_transposed_images_with_midpoints(train_dataset, image_indices=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set up learning rate scheduler\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.9, patience=10, verbose=1, mode='min', min_lr=7e-6\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">185,344</span> \n",
       "\n",
       " flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">524288</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " x_midpoints (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,242,890</span> \n",
       "\n",
       " x_midpoints_reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " input_layer (\u001b[38;5;33mInputLayer\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " conv2d (\u001b[38;5;33mConv2D\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m512\u001b[0m)           \u001b[38;5;34m185,344\u001b[0m \n",
       "\n",
       " flatten (\u001b[38;5;33mFlatten\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m524288\u001b[0m)                      \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " x_midpoints (\u001b[38;5;33mDense\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  \u001b[38;5;34m5,242,890\u001b[0m \n",
       "\n",
       " x_midpoints_reshape (\u001b[38;5;33mReshape\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m2\u001b[0m)                     \u001b[38;5;34m0\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,428,234</span> (20.71 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,428,234\u001b[0m (20.71 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,428,234</span> (20.71 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,428,234\u001b[0m (20.71 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# dynamic_exponent_callback = DynamicExponentCallback(2, 1, 400)\n",
    "\n",
    "\n",
    "with strategy.scope():\n",
    "    model_builder = ModelBuilder()\n",
    "\n",
    "    model_builder.build_model()\n",
    "\n",
    "    model_builder.model.summary()\n",
    "\n",
    "    model_builder.compile_model(loss_function=tf.keras.losses.MeanSquaredError()) \n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:14:41.580546: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2024-10-23 01:14:43.491110: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1729646083.568214 1761512 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646083.577845 1761523 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646083.582629 1761519 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646083.591707 1761512 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646083.599423 1761519 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646083.606969 1761523 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646083.614697 1761512 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646083.631375 1761519 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646083.647545 1761523 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646083.664097 1761512 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646083.678994 1761519 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646083.693963 1761523 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646083.709267 1761512 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646083.737207 1761519 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646083.764569 1761523 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646083.792068 1761512 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646083.831634 1761519 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646083.868405 1761523 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646083.905721 1761512 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646083.925744 1761519 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646083.936236 1761523 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646083.947180 1761512 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646083.973210 1761519 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646083.981243 1761523 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646083.989416 1761512 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646084.052286 1761519 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646084.110664 1761523 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646084.169001 1761512 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646084.226038 1761519 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646084.273544 1761523 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646084.708110 1761519 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646084.714360 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646084.720827 1761519 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646084.727262 1761513 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646084.733674 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646084.739965 1761519 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646084.745363 1761513 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646084.751703 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646084.757047 1761519 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646084.764070 1761513 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646084.769512 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646084.776383 1761519 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646084.784669 1761513 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646084.791483 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646084.799691 1761519 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646084.809156 1761513 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646084.817302 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646084.826587 1761519 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646084.837100 1761513 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646084.846275 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646084.855607 1761519 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646085.003932 1761513 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646085.012947 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646085.164071 1761513 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0800"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:14:45.937733: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2024-10-23 01:14:45.937825: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "W0000 00:00:1729646085.949269 1761507 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646085.951636 1761507 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646085.954940 1761507 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646085.961846 1761507 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646085.974664 1761507 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646085.980919 1761507 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646085.992060 1761507 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.007167 1761507 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.011598 1761507 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.014964 1761507 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.038679 1761507 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.061488 1761507 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.064205 1761507 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.067335 1761507 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.074029 1761507 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.078123 1761507 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.084215 1761507 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.095096 1761507 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.110024 1761507 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.114407 1761507 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.117716 1761507 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.140868 1761507 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.174444 1761507 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.177863 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.181330 1761513 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.184866 1761507 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.187803 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.190771 1761513 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.193708 1761507 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.196318 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.198692 1761513 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.201193 1761507 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.204250 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.207417 1761513 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.210486 1761507 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.214393 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.218094 1761513 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.221859 1761507 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.225868 1761513 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.229883 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.233873 1761513 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.237752 1761507 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.241555 1761513 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.299420 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.303227 1761507 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0792"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1729646086.364597 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.810728 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.821436 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.829987 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.838515 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.847039 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.856271 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.865450 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.874917 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.884003 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.894899 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.903890 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.913979 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.924142 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.934511 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.944743 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.956296 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.966970 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.978660 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646086.989649 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.001300 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.012845 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.024215 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.035557 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.047219 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.063338 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.075022 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.090568 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.108346 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.123492 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.140690 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.155048 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.172272 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.186480 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.206689 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.221045 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.241281 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.255596 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.275828 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.290178 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.307079 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.321576 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.338509 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.354185 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.371267 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.387217 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.403577 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.426357 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.449149 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.471835 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.502051 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.531935 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.561735 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.622179 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.682368 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.742638 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.746719 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.750825 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.754924 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.761727 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.768551 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.775417 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.801636 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.827725 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.853872 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.891509 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.928994 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646087.967855 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.032352 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.092810 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.150184 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.159060 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.167852 1761511 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "2024-10-23 01:14:48.428844: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "W0000 00:00:1729646088.445250 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.450749 1761516 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.456459 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.465258 1761516 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.470289 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.475905 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.481601 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.487892 1761516 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.493547 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.498813 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.505088 1761516 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.511343 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.516911 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.523813 1761516 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.530379 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.536993 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.543537 1761516 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.550678 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.557841 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.564217 1761516 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.570984 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.580383 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.587866 1761516 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.595072 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.606104 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.612917 1761516 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.622402 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.631553 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.638747 1761516 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.649544 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.662123 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.671367 1761516 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.680327 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.689172 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.699649 1761516 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.712089 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.722549 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.731296 1761516 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.740207 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.749965 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.762383 1761516 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.772723 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.786247 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.794924 1761516 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.804843 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.823163 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.833575 1761516 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.846998 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.883693 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.893237 1761516 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.911515 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.914157 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.927556 1761516 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.963762 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.968034 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.986143 1761516 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646088.988796 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646089.004736 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646089.041132 1761516 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646089.045332 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646089.068259 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646089.070771 1761516 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646089.086738 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646089.121512 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646089.125754 1761516 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646089.148893 1761506 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646089.154392 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646089.170510 1761516 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646089.205452 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646089.231308 1761516 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729646089.237657 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 405ms/step - loss: 0.0785 - val_loss: 0.0561 - learning_rate: 0.0010\n",
      "Epoch 2/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1729646089.276508 1761508 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0527"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:14:50.247878: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0519 - val_loss: 0.0401 - learning_rate: 0.0010\n",
      "Epoch 3/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0368 - val_loss: 0.0309 - learning_rate: 0.0010\n",
      "Epoch 4/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:14:51.646905: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - loss: 0.0307 - val_loss: 0.0271 - learning_rate: 0.0010\n",
      "Epoch 5/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - loss: 0.0268 - val_loss: 0.0266 - learning_rate: 0.0010\n",
      "Epoch 6/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0255"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:14:54.889842: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0254 - val_loss: 0.0254 - learning_rate: 0.0010\n",
      "Epoch 7/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0248 - val_loss: 0.0266 - learning_rate: 0.0010\n",
      "Epoch 8/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0245 - val_loss: 0.0252 - learning_rate: 0.0010\n",
      "Epoch 9/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 105ms/step - loss: 0.0239 - val_loss: 0.0258 - learning_rate: 0.0010\n",
      "Epoch 10/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0234 - val_loss: 0.0247 - learning_rate: 0.0010\n",
      "Epoch 11/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0228 - val_loss: 0.0240 - learning_rate: 0.0010\n",
      "Epoch 12/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:15:01.025723: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - loss: 0.0225 - val_loss: 0.0260 - learning_rate: 0.0010\n",
      "Epoch 13/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - loss: 0.0220 - val_loss: 0.0254 - learning_rate: 0.0010\n",
      "Epoch 14/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0218 - val_loss: 0.0254 - learning_rate: 0.0010\n",
      "Epoch 15/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0213 - val_loss: 0.0252 - learning_rate: 0.0010\n",
      "Epoch 16/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0207 - val_loss: 0.0250 - learning_rate: 0.0010\n",
      "Epoch 17/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0205 - val_loss: 0.0250 - learning_rate: 0.0010\n",
      "Epoch 18/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 0.0198 - val_loss: 0.0242 - learning_rate: 0.0010\n",
      "Epoch 19/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0191 - val_loss: 0.0235 - learning_rate: 0.0010\n",
      "Epoch 20/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0190 - val_loss: 0.0237 - learning_rate: 0.0010\n",
      "Epoch 21/2000\n",
      "\u001b[1m 1/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 254ms/step - loss: 0.0184"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:15:11.585636: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0187 - val_loss: 0.0229 - learning_rate: 0.0010\n",
      "Epoch 22/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0179"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:15:13.536520: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0179 - val_loss: 0.0225 - learning_rate: 0.0010\n",
      "Epoch 23/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - loss: 0.0174 - val_loss: 0.0235 - learning_rate: 0.0010\n",
      "Epoch 24/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0170 - val_loss: 0.0233 - learning_rate: 0.0010\n",
      "Epoch 25/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0168 - val_loss: 0.0228 - learning_rate: 0.0010\n",
      "Epoch 26/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0163 - val_loss: 0.0227 - learning_rate: 0.0010\n",
      "Epoch 27/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - loss: 0.0161 - val_loss: 0.0224 - learning_rate: 0.0010\n",
      "Epoch 28/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0156 - val_loss: 0.0232 - learning_rate: 0.0010\n",
      "Epoch 29/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0151 - val_loss: 0.0231 - learning_rate: 0.0010\n",
      "Epoch 30/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0147 - val_loss: 0.0220 - learning_rate: 0.0010\n",
      "Epoch 31/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0142 - val_loss: 0.0238 - learning_rate: 0.0010\n",
      "Epoch 32/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0136 - val_loss: 0.0223 - learning_rate: 0.0010\n",
      "Epoch 33/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0133 - val_loss: 0.0236 - learning_rate: 0.0010\n",
      "Epoch 34/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0133 - val_loss: 0.0226 - learning_rate: 0.0010\n",
      "Epoch 35/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0126 - val_loss: 0.0231 - learning_rate: 0.0010\n",
      "Epoch 36/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0122 - val_loss: 0.0235 - learning_rate: 0.0010\n",
      "Epoch 37/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0116 - val_loss: 0.0232 - learning_rate: 0.0010\n",
      "Epoch 38/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - loss: 0.0113 - val_loss: 0.0227 - learning_rate: 0.0010\n",
      "Epoch 39/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - loss: 0.0110 - val_loss: 0.0234 - learning_rate: 0.0010\n",
      "Epoch 40/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0107\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0107 - val_loss: 0.0224 - learning_rate: 0.0010\n",
      "Epoch 41/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0098 - val_loss: 0.0224 - learning_rate: 9.0000e-04\n",
      "Epoch 42/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0095 - val_loss: 0.0215 - learning_rate: 9.0000e-04\n",
      "Epoch 43/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0089 - val_loss: 0.0222 - learning_rate: 9.0000e-04\n",
      "Epoch 44/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:15:38.456725: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0086 - val_loss: 0.0226 - learning_rate: 9.0000e-04\n",
      "Epoch 45/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0083 - val_loss: 0.0226 - learning_rate: 9.0000e-04\n",
      "Epoch 46/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0078 - val_loss: 0.0220 - learning_rate: 9.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:15:41.874381: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0073 - val_loss: 0.0234 - learning_rate: 9.0000e-04\n",
      "Epoch 48/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0072 - val_loss: 0.0234 - learning_rate: 9.0000e-04\n",
      "Epoch 49/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0070 - val_loss: 0.0222 - learning_rate: 9.0000e-04\n",
      "Epoch 50/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0065 - val_loss: 0.0226 - learning_rate: 9.0000e-04\n",
      "Epoch 51/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0061 - val_loss: 0.0242 - learning_rate: 9.0000e-04\n",
      "Epoch 52/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0061\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 0.0008100000384729356.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0063 - val_loss: 0.0228 - learning_rate: 9.0000e-04\n",
      "Epoch 53/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0058 - val_loss: 0.0229 - learning_rate: 8.1000e-04\n",
      "Epoch 54/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 0.0052 - val_loss: 0.0217 - learning_rate: 8.1000e-04\n",
      "Epoch 55/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0049 - val_loss: 0.0235 - learning_rate: 8.1000e-04\n",
      "Epoch 56/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0046 - val_loss: 0.0232 - learning_rate: 8.1000e-04\n",
      "Epoch 57/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0044 - val_loss: 0.0213 - learning_rate: 8.1000e-04\n",
      "Epoch 58/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0042 - val_loss: 0.0225 - learning_rate: 8.1000e-04\n",
      "Epoch 59/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0040 - val_loss: 0.0233 - learning_rate: 8.1000e-04\n",
      "Epoch 60/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0038 - val_loss: 0.0227 - learning_rate: 8.1000e-04\n",
      "Epoch 61/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 0.0037 - val_loss: 0.0230 - learning_rate: 8.1000e-04\n",
      "Epoch 62/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0034 - val_loss: 0.0233 - learning_rate: 8.1000e-04\n",
      "Epoch 63/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0035 - val_loss: 0.0242 - learning_rate: 8.1000e-04\n",
      "Epoch 64/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0035 - val_loss: 0.0219 - learning_rate: 8.1000e-04\n",
      "Epoch 65/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0030 - val_loss: 0.0237 - learning_rate: 8.1000e-04\n",
      "Epoch 66/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0031 - val_loss: 0.0220 - learning_rate: 8.1000e-04\n",
      "Epoch 67/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0027\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 0.0007290000503417104.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 0.0027 - val_loss: 0.0221 - learning_rate: 8.1000e-04\n",
      "Epoch 68/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 0.0027 - val_loss: 0.0224 - learning_rate: 7.2900e-04\n",
      "Epoch 69/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0024 - val_loss: 0.0226 - learning_rate: 7.2900e-04\n",
      "Epoch 70/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0024 - val_loss: 0.0225 - learning_rate: 7.2900e-04\n",
      "Epoch 71/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0022 - val_loss: 0.0226 - learning_rate: 7.2900e-04\n",
      "Epoch 72/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0021"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:16:12.517950: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0021 - val_loss: 0.0232 - learning_rate: 7.2900e-04\n",
      "Epoch 73/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0021 - val_loss: 0.0232 - learning_rate: 7.2900e-04\n",
      "Epoch 74/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 0.0020 - val_loss: 0.0223 - learning_rate: 7.2900e-04\n",
      "Epoch 75/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0019 - val_loss: 0.0223 - learning_rate: 7.2900e-04\n",
      "Epoch 76/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0019 - val_loss: 0.0220 - learning_rate: 7.2900e-04\n",
      "Epoch 77/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0020\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 0.0006561000715009868.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0020 - val_loss: 0.0235 - learning_rate: 7.2900e-04\n",
      "Epoch 78/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0020 - val_loss: 0.0230 - learning_rate: 6.5610e-04\n",
      "Epoch 79/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0019 - val_loss: 0.0230 - learning_rate: 6.5610e-04\n",
      "Epoch 80/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0016 - val_loss: 0.0232 - learning_rate: 6.5610e-04\n",
      "Epoch 81/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0015 - val_loss: 0.0233 - learning_rate: 6.5610e-04\n",
      "Epoch 82/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 0.0015 - val_loss: 0.0233 - learning_rate: 6.5610e-04\n",
      "Epoch 83/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0014 - val_loss: 0.0227 - learning_rate: 6.5610e-04\n",
      "Epoch 84/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - loss: 0.0013 - val_loss: 0.0226 - learning_rate: 6.5610e-04\n",
      "Epoch 85/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0013 - val_loss: 0.0223 - learning_rate: 6.5610e-04\n",
      "Epoch 86/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0013 - val_loss: 0.0240 - learning_rate: 6.5610e-04\n",
      "Epoch 87/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:16:29.383881: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0013\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 0.0005904900433961303.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0013 - val_loss: 0.0234 - learning_rate: 6.5610e-04\n",
      "Epoch 88/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0012 - val_loss: 0.0243 - learning_rate: 5.9049e-04\n",
      "Epoch 89/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0012 - val_loss: 0.0225 - learning_rate: 5.9049e-04\n",
      "Epoch 90/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0011 - val_loss: 0.0224 - learning_rate: 5.9049e-04\n",
      "Epoch 91/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0011 - val_loss: 0.0227 - learning_rate: 5.9049e-04\n",
      "Epoch 92/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0010 - val_loss: 0.0223 - learning_rate: 5.9049e-04\n",
      "Epoch 93/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 9.8149e-04 - val_loss: 0.0233 - learning_rate: 5.9049e-04\n",
      "Epoch 94/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 9.5563e-04 - val_loss: 0.0237 - learning_rate: 5.9049e-04\n",
      "Epoch 95/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 9.8042e-04 - val_loss: 0.0227 - learning_rate: 5.9049e-04\n",
      "Epoch 96/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.0138e-04 - val_loss: 0.0237 - learning_rate: 5.9049e-04\n",
      "Epoch 97/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 8.8752e-04\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 0.0005314410547725857.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 9.0057e-04 - val_loss: 0.0251 - learning_rate: 5.9049e-04\n",
      "Epoch 98/2000\n",
      "\u001b[1m 1/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 258ms/step - loss: 8.4209e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:16:42.530617: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 8.5140e-04 - val_loss: 0.0239 - learning_rate: 5.3144e-04\n",
      "Epoch 99/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 8.3171e-04 - val_loss: 0.0234 - learning_rate: 5.3144e-04\n",
      "Epoch 100/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.2182e-04 - val_loss: 0.0231 - learning_rate: 5.3144e-04\n",
      "Epoch 101/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.8620e-04 - val_loss: 0.0234 - learning_rate: 5.3144e-04\n",
      "Epoch 102/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.5807e-04 - val_loss: 0.0234 - learning_rate: 5.3144e-04\n",
      "Epoch 103/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.5672e-04 - val_loss: 0.0239 - learning_rate: 5.3144e-04\n",
      "Epoch 104/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.3579e-04 - val_loss: 0.0234 - learning_rate: 5.3144e-04\n",
      "Epoch 105/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 7.2087e-04 - val_loss: 0.0239 - learning_rate: 5.3144e-04\n",
      "Epoch 106/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.4781e-04 - val_loss: 0.0241 - learning_rate: 5.3144e-04\n",
      "Epoch 107/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 7.0567e-04\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 0.00047829695977270604.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.2184e-04 - val_loss: 0.0231 - learning_rate: 5.3144e-04\n",
      "Epoch 108/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 6.7898e-04 - val_loss: 0.0236 - learning_rate: 4.7830e-04\n",
      "Epoch 109/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.3670e-04 - val_loss: 0.0243 - learning_rate: 4.7830e-04\n",
      "Epoch 110/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 6.1632e-04 - val_loss: 0.0238 - learning_rate: 4.7830e-04\n",
      "Epoch 111/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.2394e-04 - val_loss: 0.0230 - learning_rate: 4.7830e-04\n",
      "Epoch 112/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.7709e-04 - val_loss: 0.0234 - learning_rate: 4.7830e-04\n",
      "Epoch 113/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 5.6891e-04 - val_loss: 0.0244 - learning_rate: 4.7830e-04\n",
      "Epoch 114/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.6964e-04 - val_loss: 0.0236 - learning_rate: 4.7830e-04\n",
      "Epoch 115/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.6294e-04 - val_loss: 0.0237 - learning_rate: 4.7830e-04\n",
      "Epoch 116/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.3669e-04 - val_loss: 0.0233 - learning_rate: 4.7830e-04\n",
      "Epoch 117/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 5.1810e-04\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 0.0004304672533180565.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.2666e-04 - val_loss: 0.0226 - learning_rate: 4.7830e-04\n",
      "Epoch 118/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.3572e-04 - val_loss: 0.0242 - learning_rate: 4.3047e-04\n",
      "Epoch 119/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.9996e-04 - val_loss: 0.0243 - learning_rate: 4.3047e-04\n",
      "Epoch 120/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.9204e-04 - val_loss: 0.0232 - learning_rate: 4.3047e-04\n",
      "Epoch 121/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 4.9627e-04 - val_loss: 0.0246 - learning_rate: 4.3047e-04\n",
      "Epoch 122/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 4.7804e-04 - val_loss: 0.0246 - learning_rate: 4.3047e-04\n",
      "Epoch 123/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 4.7274e-04 - val_loss: 0.0237 - learning_rate: 4.3047e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:17:12.997945: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.5165e-04 - val_loss: 0.0244 - learning_rate: 4.3047e-04\n",
      "Epoch 125/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.5086e-04 - val_loss: 0.0239 - learning_rate: 4.3047e-04\n",
      "Epoch 126/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5509e-04 - val_loss: 0.0251 - learning_rate: 4.3047e-04\n",
      "Epoch 127/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 4.2462e-04\n",
      "Epoch 127: ReduceLROnPlateau reducing learning rate to 0.00038742052274756136.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.3129e-04 - val_loss: 0.0243 - learning_rate: 4.3047e-04\n",
      "Epoch 128/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.2444e-04 - val_loss: 0.0236 - learning_rate: 3.8742e-04\n",
      "Epoch 129/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.1984e-04 - val_loss: 0.0243 - learning_rate: 3.8742e-04\n",
      "Epoch 130/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.1731e-04 - val_loss: 0.0245 - learning_rate: 3.8742e-04\n",
      "Epoch 131/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.0582e-04 - val_loss: 0.0244 - learning_rate: 3.8742e-04\n",
      "Epoch 132/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.1228e-04 - val_loss: 0.0232 - learning_rate: 3.8742e-04\n",
      "Epoch 133/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.9938e-04 - val_loss: 0.0243 - learning_rate: 3.8742e-04\n",
      "Epoch 134/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.9950e-04 - val_loss: 0.0239 - learning_rate: 3.8742e-04\n",
      "Epoch 135/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.8751e-04 - val_loss: 0.0238 - learning_rate: 3.8742e-04\n",
      "Epoch 136/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 3.7341e-04 - val_loss: 0.0237 - learning_rate: 3.8742e-04\n",
      "Epoch 137/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 3.6351e-04\n",
      "Epoch 137: ReduceLROnPlateau reducing learning rate to 0.0003486784757114947.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.6912e-04 - val_loss: 0.0245 - learning_rate: 3.8742e-04\n",
      "Epoch 138/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.4997e-04 - val_loss: 0.0238 - learning_rate: 3.4868e-04\n",
      "Epoch 139/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.4707e-04 - val_loss: 0.0243 - learning_rate: 3.4868e-04\n",
      "Epoch 140/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.3967e-04 - val_loss: 0.0248 - learning_rate: 3.4868e-04\n",
      "Epoch 141/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.3656e-04 - val_loss: 0.0240 - learning_rate: 3.4868e-04\n",
      "Epoch 142/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.3284e-04 - val_loss: 0.0256 - learning_rate: 3.4868e-04\n",
      "Epoch 143/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2743e-04 - val_loss: 0.0243 - learning_rate: 3.4868e-04\n",
      "Epoch 144/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.2225e-04 - val_loss: 0.0251 - learning_rate: 3.4868e-04\n",
      "Epoch 145/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 3.2302e-04 - val_loss: 0.0240 - learning_rate: 3.4868e-04\n",
      "Epoch 146/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.1564e-04 - val_loss: 0.0243 - learning_rate: 3.4868e-04\n",
      "Epoch 147/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 3.0416e-04\n",
      "Epoch 147: ReduceLROnPlateau reducing learning rate to 0.00031381062290165574.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.0904e-04 - val_loss: 0.0232 - learning_rate: 3.4868e-04\n",
      "Epoch 148/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9806e-04 - val_loss: 0.0247 - learning_rate: 3.1381e-04\n",
      "Epoch 149/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 2.9561e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:17:43.649741: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.0012e-04 - val_loss: 0.0249 - learning_rate: 3.1381e-04\n",
      "Epoch 150/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9917e-04 - val_loss: 0.0240 - learning_rate: 3.1381e-04\n",
      "Epoch 151/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 2.9247e-04 - val_loss: 0.0238 - learning_rate: 3.1381e-04\n",
      "Epoch 152/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.8988e-04 - val_loss: 0.0241 - learning_rate: 3.1381e-04\n",
      "Epoch 153/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.8745e-04 - val_loss: 0.0238 - learning_rate: 3.1381e-04\n",
      "Epoch 154/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.7944e-04 - val_loss: 0.0234 - learning_rate: 3.1381e-04\n",
      "Epoch 155/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.7911e-04 - val_loss: 0.0241 - learning_rate: 3.1381e-04\n",
      "Epoch 156/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.7384e-04 - val_loss: 0.0240 - learning_rate: 3.1381e-04\n",
      "Epoch 157/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 2.7668e-04\n",
      "Epoch 157: ReduceLROnPlateau reducing learning rate to 0.0002824295632308349.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.8059e-04 - val_loss: 0.0243 - learning_rate: 3.1381e-04\n",
      "Epoch 158/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6826e-04 - val_loss: 0.0239 - learning_rate: 2.8243e-04\n",
      "Epoch 159/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6387e-04 - val_loss: 0.0250 - learning_rate: 2.8243e-04\n",
      "Epoch 160/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.5666e-04 - val_loss: 0.0240 - learning_rate: 2.8243e-04\n",
      "Epoch 161/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.5579e-04 - val_loss: 0.0234 - learning_rate: 2.8243e-04\n",
      "Epoch 162/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5417e-04 - val_loss: 0.0240 - learning_rate: 2.8243e-04\n",
      "Epoch 163/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 2.5115e-04 - val_loss: 0.0248 - learning_rate: 2.8243e-04\n",
      "Epoch 164/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4379e-04 - val_loss: 0.0238 - learning_rate: 2.8243e-04\n",
      "Epoch 165/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.4474e-04 - val_loss: 0.0242 - learning_rate: 2.8243e-04\n",
      "Epoch 166/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.4925e-04 - val_loss: 0.0244 - learning_rate: 2.8243e-04\n",
      "Epoch 167/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 2.3860e-04\n",
      "Epoch 167: ReduceLROnPlateau reducing learning rate to 0.00025418660952709616.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4307e-04 - val_loss: 0.0251 - learning_rate: 2.8243e-04\n",
      "Epoch 168/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3568e-04 - val_loss: 0.0246 - learning_rate: 2.5419e-04\n",
      "Epoch 169/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.3931e-04 - val_loss: 0.0235 - learning_rate: 2.5419e-04\n",
      "Epoch 170/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3940e-04 - val_loss: 0.0239 - learning_rate: 2.5419e-04\n",
      "Epoch 171/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3387e-04 - val_loss: 0.0246 - learning_rate: 2.5419e-04\n",
      "Epoch 172/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 2.2473e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:18:11.041439: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.2730e-04 - val_loss: 0.0243 - learning_rate: 2.5419e-04\n",
      "Epoch 173/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.2081e-04 - val_loss: 0.0246 - learning_rate: 2.5419e-04\n",
      "Epoch 174/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.2149e-04 - val_loss: 0.0242 - learning_rate: 2.5419e-04\n",
      "Epoch 175/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:18:13.793775: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 2.1809e-04 - val_loss: 0.0242 - learning_rate: 2.5419e-04\n",
      "Epoch 176/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.2006e-04 - val_loss: 0.0243 - learning_rate: 2.5419e-04\n",
      "Epoch 177/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 2.0911e-04\n",
      "Epoch 177: ReduceLROnPlateau reducing learning rate to 0.00022876793809700757.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.1245e-04 - val_loss: 0.0251 - learning_rate: 2.5419e-04\n",
      "Epoch 178/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.1761e-04 - val_loss: 0.0238 - learning_rate: 2.2877e-04\n",
      "Epoch 179/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.1121e-04 - val_loss: 0.0226 - learning_rate: 2.2877e-04\n",
      "Epoch 180/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.0891e-04 - val_loss: 0.0255 - learning_rate: 2.2877e-04\n",
      "Epoch 181/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.1074e-04 - val_loss: 0.0248 - learning_rate: 2.2877e-04\n",
      "Epoch 182/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.0707e-04 - val_loss: 0.0248 - learning_rate: 2.2877e-04\n",
      "Epoch 183/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.0247e-04 - val_loss: 0.0253 - learning_rate: 2.2877e-04\n",
      "Epoch 184/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.0347e-04 - val_loss: 0.0240 - learning_rate: 2.2877e-04\n",
      "Epoch 185/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.0715e-04 - val_loss: 0.0239 - learning_rate: 2.2877e-04\n",
      "Epoch 186/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 1.9497e-04 - val_loss: 0.0239 - learning_rate: 2.2877e-04\n",
      "Epoch 187/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 1.9452e-04\n",
      "Epoch 187: ReduceLROnPlateau reducing learning rate to 0.00020589114428730683.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.9841e-04 - val_loss: 0.0239 - learning_rate: 2.2877e-04\n",
      "Epoch 188/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.9357e-04 - val_loss: 0.0241 - learning_rate: 2.0589e-04\n",
      "Epoch 189/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.9139e-04 - val_loss: 0.0236 - learning_rate: 2.0589e-04\n",
      "Epoch 190/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.9172e-04 - val_loss: 0.0237 - learning_rate: 2.0589e-04\n",
      "Epoch 191/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.8877e-04 - val_loss: 0.0225 - learning_rate: 2.0589e-04\n",
      "Epoch 192/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.8671e-04 - val_loss: 0.0239 - learning_rate: 2.0589e-04\n",
      "Epoch 193/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.8549e-04 - val_loss: 0.0258 - learning_rate: 2.0589e-04\n",
      "Epoch 194/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 1.7960e-04 - val_loss: 0.0247 - learning_rate: 2.0589e-04\n",
      "Epoch 195/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 1.8119e-04 - val_loss: 0.0244 - learning_rate: 2.0589e-04\n",
      "Epoch 196/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 1.7851e-04 - val_loss: 0.0247 - learning_rate: 2.0589e-04\n",
      "Epoch 197/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 1.7585e-04\n",
      "Epoch 197: ReduceLROnPlateau reducing learning rate to 0.00018530203378759326.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.7880e-04 - val_loss: 0.0247 - learning_rate: 2.0589e-04\n",
      "Epoch 198/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.8391e-04 - val_loss: 0.0247 - learning_rate: 1.8530e-04\n",
      "Epoch 199/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.7570e-04 - val_loss: 0.0235 - learning_rate: 1.8530e-04\n",
      "Epoch 200/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 1.7958e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:18:44.584042: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 1.8156e-04 - val_loss: 0.0259 - learning_rate: 1.8530e-04\n",
      "Epoch 201/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.7208e-04 - val_loss: 0.0245 - learning_rate: 1.8530e-04\n",
      "Epoch 202/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.7395e-04 - val_loss: 0.0243 - learning_rate: 1.8530e-04\n",
      "Epoch 203/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.7849e-04 - val_loss: 0.0243 - learning_rate: 1.8530e-04\n",
      "Epoch 204/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.7288e-04 - val_loss: 0.0244 - learning_rate: 1.8530e-04\n",
      "Epoch 205/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.6877e-04 - val_loss: 0.0252 - learning_rate: 1.8530e-04\n",
      "Epoch 206/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.6955e-04 - val_loss: 0.0245 - learning_rate: 1.8530e-04\n",
      "Epoch 207/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 1.6818e-04\n",
      "Epoch 207: ReduceLROnPlateau reducing learning rate to 0.00016677183302817866.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.7011e-04 - val_loss: 0.0249 - learning_rate: 1.8530e-04\n",
      "Epoch 208/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.6823e-04 - val_loss: 0.0242 - learning_rate: 1.6677e-04\n",
      "Epoch 209/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 1.6157e-04 - val_loss: 0.0229 - learning_rate: 1.6677e-04\n",
      "Epoch 210/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 1.6003e-04 - val_loss: 0.0253 - learning_rate: 1.6677e-04\n",
      "Epoch 211/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.6186e-04 - val_loss: 0.0257 - learning_rate: 1.6677e-04\n",
      "Epoch 212/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.6259e-04 - val_loss: 0.0254 - learning_rate: 1.6677e-04\n",
      "Epoch 213/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.6380e-04 - val_loss: 0.0243 - learning_rate: 1.6677e-04\n",
      "Epoch 214/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.5546e-04 - val_loss: 0.0248 - learning_rate: 1.6677e-04\n",
      "Epoch 215/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.6027e-04 - val_loss: 0.0251 - learning_rate: 1.6677e-04\n",
      "Epoch 216/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.6020e-04 - val_loss: 0.0238 - learning_rate: 1.6677e-04\n",
      "Epoch 217/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 1.5230e-04\n",
      "Epoch 217: ReduceLROnPlateau reducing learning rate to 0.00015009464841568844.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.5450e-04 - val_loss: 0.0235 - learning_rate: 1.6677e-04\n",
      "Epoch 218/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 1.5311e-04 - val_loss: 0.0243 - learning_rate: 1.5009e-04\n",
      "Epoch 219/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.5354e-04 - val_loss: 0.0244 - learning_rate: 1.5009e-04\n",
      "Epoch 220/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.5262e-04 - val_loss: 0.0243 - learning_rate: 1.5009e-04\n",
      "Epoch 221/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 1.5301e-04 - val_loss: 0.0246 - learning_rate: 1.5009e-04\n",
      "Epoch 222/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.5147e-04 - val_loss: 0.0247 - learning_rate: 1.5009e-04\n",
      "Epoch 223/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.4958e-04 - val_loss: 0.0256 - learning_rate: 1.5009e-04\n",
      "Epoch 224/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.4804e-04 - val_loss: 0.0254 - learning_rate: 1.5009e-04\n",
      "Epoch 225/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.5003e-04 - val_loss: 0.0238 - learning_rate: 1.5009e-04\n",
      "Epoch 226/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 1.4292e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:19:15.351775: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.4583e-04 - val_loss: 0.0248 - learning_rate: 1.5009e-04\n",
      "Epoch 227/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 1.4683e-04\n",
      "Epoch 227: ReduceLROnPlateau reducing learning rate to 0.0001350851875031367.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.4885e-04 - val_loss: 0.0250 - learning_rate: 1.5009e-04\n",
      "Epoch 228/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.4355e-04 - val_loss: 0.0250 - learning_rate: 1.3509e-04\n",
      "Epoch 229/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.4267e-04 - val_loss: 0.0261 - learning_rate: 1.3509e-04\n",
      "Epoch 230/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.4601e-04 - val_loss: 0.0237 - learning_rate: 1.3509e-04\n",
      "Epoch 231/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.4459e-04 - val_loss: 0.0255 - learning_rate: 1.3509e-04\n",
      "Epoch 232/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 1.4416e-04 - val_loss: 0.0253 - learning_rate: 1.3509e-04\n",
      "Epoch 233/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.4175e-04 - val_loss: 0.0246 - learning_rate: 1.3509e-04\n",
      "Epoch 234/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.4089e-04 - val_loss: 0.0246 - learning_rate: 1.3509e-04\n",
      "Epoch 235/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.4039e-04 - val_loss: 0.0258 - learning_rate: 1.3509e-04\n",
      "Epoch 236/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.3777e-04 - val_loss: 0.0250 - learning_rate: 1.3509e-04\n",
      "Epoch 237/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 1.3361e-04\n",
      "Epoch 237: ReduceLROnPlateau reducing learning rate to 0.00012157666351413355.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.3550e-04 - val_loss: 0.0253 - learning_rate: 1.3509e-04\n",
      "Epoch 238/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.3284e-04 - val_loss: 0.0255 - learning_rate: 1.2158e-04\n",
      "Epoch 239/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.3477e-04 - val_loss: 0.0248 - learning_rate: 1.2158e-04\n",
      "Epoch 240/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 1.3288e-04 - val_loss: 0.0251 - learning_rate: 1.2158e-04\n",
      "Epoch 241/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 1.3499e-04 - val_loss: 0.0256 - learning_rate: 1.2158e-04\n",
      "Epoch 242/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.3590e-04 - val_loss: 0.0244 - learning_rate: 1.2158e-04\n",
      "Epoch 243/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.3014e-04 - val_loss: 0.0249 - learning_rate: 1.2158e-04\n",
      "Epoch 244/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.3098e-04 - val_loss: 0.0253 - learning_rate: 1.2158e-04\n",
      "Epoch 245/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.3268e-04 - val_loss: 0.0256 - learning_rate: 1.2158e-04\n",
      "Epoch 246/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 1.2947e-04 - val_loss: 0.0256 - learning_rate: 1.2158e-04\n",
      "Epoch 247/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 1.3063e-04\n",
      "Epoch 247: ReduceLROnPlateau reducing learning rate to 0.00010941899454337544.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 1.3154e-04 - val_loss: 0.0257 - learning_rate: 1.2158e-04\n",
      "Epoch 248/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.3029e-04 - val_loss: 0.0260 - learning_rate: 1.0942e-04\n",
      "Epoch 249/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.2795e-04 - val_loss: 0.0250 - learning_rate: 1.0942e-04\n",
      "Epoch 250/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.2917e-04 - val_loss: 0.0244 - learning_rate: 1.0942e-04\n",
      "Epoch 251/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.2934e-04 - val_loss: 0.0258 - learning_rate: 1.0942e-04\n",
      "Epoch 252/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:19:45.366823: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.2572e-04 - val_loss: 0.0253 - learning_rate: 1.0942e-04\n",
      "Epoch 253/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.2490e-04 - val_loss: 0.0250 - learning_rate: 1.0942e-04\n",
      "Epoch 254/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.2561e-04 - val_loss: 0.0254 - learning_rate: 1.0942e-04\n",
      "Epoch 255/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.2500e-04 - val_loss: 0.0240 - learning_rate: 1.0942e-04\n",
      "Epoch 256/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.2576e-04 - val_loss: 0.0240 - learning_rate: 1.0942e-04\n",
      "Epoch 257/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 1.2368e-04\n",
      "Epoch 257: ReduceLROnPlateau reducing learning rate to 9.847709443420172e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.2574e-04 - val_loss: 0.0257 - learning_rate: 1.0942e-04\n",
      "Epoch 258/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.2335e-04 - val_loss: 0.0257 - learning_rate: 9.8477e-05\n",
      "Epoch 259/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 1.2430e-04 - val_loss: 0.0260 - learning_rate: 9.8477e-05\n",
      "Epoch 260/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.2368e-04 - val_loss: 0.0241 - learning_rate: 9.8477e-05\n",
      "Epoch 261/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.2292e-04 - val_loss: 0.0247 - learning_rate: 9.8477e-05\n",
      "Epoch 262/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 1.2438e-04 - val_loss: 0.0238 - learning_rate: 9.8477e-05\n",
      "Epoch 263/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.2101e-04 - val_loss: 0.0255 - learning_rate: 9.8477e-05\n",
      "Epoch 264/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.2303e-04 - val_loss: 0.0265 - learning_rate: 9.8477e-05\n",
      "Epoch 265/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.2287e-04 - val_loss: 0.0244 - learning_rate: 9.8477e-05\n",
      "Epoch 266/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.2065e-04 - val_loss: 0.0246 - learning_rate: 9.8477e-05\n",
      "Epoch 267/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 1.1667e-04\n",
      "Epoch 267: ReduceLROnPlateau reducing learning rate to 8.862938630045391e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.1859e-04 - val_loss: 0.0256 - learning_rate: 9.8477e-05\n",
      "Epoch 268/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.1952e-04 - val_loss: 0.0246 - learning_rate: 8.8629e-05\n",
      "Epoch 269/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 1.1688e-04 - val_loss: 0.0244 - learning_rate: 8.8629e-05\n",
      "Epoch 270/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.1771e-04 - val_loss: 0.0253 - learning_rate: 8.8629e-05\n",
      "Epoch 271/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.1926e-04 - val_loss: 0.0243 - learning_rate: 8.8629e-05\n",
      "Epoch 272/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.1775e-04 - val_loss: 0.0233 - learning_rate: 8.8629e-05\n",
      "Epoch 273/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 114ms/step - loss: 1.1810e-04 - val_loss: 0.0244 - learning_rate: 8.8629e-05\n",
      "Epoch 274/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.1348e-04 - val_loss: 0.0247 - learning_rate: 8.8629e-05\n",
      "Epoch 275/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.1875e-04 - val_loss: 0.0261 - learning_rate: 8.8629e-05\n",
      "Epoch 276/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.1292e-04 - val_loss: 0.0243 - learning_rate: 8.8629e-05\n",
      "Epoch 277/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:20:15.449114: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 1.1269e-04\n",
      "Epoch 277: ReduceLROnPlateau reducing learning rate to 7.976644701557234e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.1404e-04 - val_loss: 0.0244 - learning_rate: 8.8629e-05\n",
      "Epoch 278/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.1650e-04 - val_loss: 0.0240 - learning_rate: 7.9766e-05\n",
      "Epoch 279/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.1359e-04 - val_loss: 0.0245 - learning_rate: 7.9766e-05\n",
      "Epoch 280/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.1197e-04 - val_loss: 0.0253 - learning_rate: 7.9766e-05\n",
      "Epoch 281/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.1241e-04 - val_loss: 0.0253 - learning_rate: 7.9766e-05\n",
      "Epoch 282/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.1179e-04 - val_loss: 0.0248 - learning_rate: 7.9766e-05\n",
      "Epoch 283/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.1361e-04 - val_loss: 0.0249 - learning_rate: 7.9766e-05\n",
      "Epoch 284/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 1.1182e-04 - val_loss: 0.0245 - learning_rate: 7.9766e-05\n",
      "Epoch 285/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.0981e-04 - val_loss: 0.0253 - learning_rate: 7.9766e-05\n",
      "Epoch 286/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.1056e-04 - val_loss: 0.0245 - learning_rate: 7.9766e-05\n",
      "Epoch 287/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 1.0892e-04\n",
      "Epoch 287: ReduceLROnPlateau reducing learning rate to 7.178980231401511e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.1050e-04 - val_loss: 0.0255 - learning_rate: 7.9766e-05\n",
      "Epoch 288/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.0992e-04 - val_loss: 0.0241 - learning_rate: 7.1790e-05\n",
      "Epoch 289/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.0923e-04 - val_loss: 0.0247 - learning_rate: 7.1790e-05\n",
      "Epoch 290/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.0582e-04 - val_loss: 0.0239 - learning_rate: 7.1790e-05\n",
      "Epoch 291/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.0826e-04 - val_loss: 0.0250 - learning_rate: 7.1790e-05\n",
      "Epoch 292/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.0675e-04 - val_loss: 0.0243 - learning_rate: 7.1790e-05\n",
      "Epoch 293/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.0740e-04 - val_loss: 0.0242 - learning_rate: 7.1790e-05\n",
      "Epoch 294/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.0735e-04 - val_loss: 0.0260 - learning_rate: 7.1790e-05\n",
      "Epoch 295/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.0594e-04 - val_loss: 0.0241 - learning_rate: 7.1790e-05\n",
      "Epoch 296/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 1.0499e-04 - val_loss: 0.0258 - learning_rate: 7.1790e-05\n",
      "Epoch 297/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 1.0234e-04\n",
      "Epoch 297: ReduceLROnPlateau reducing learning rate to 6.461082011810504e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.0388e-04 - val_loss: 0.0253 - learning_rate: 7.1790e-05\n",
      "Epoch 298/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.0751e-04 - val_loss: 0.0246 - learning_rate: 6.4611e-05\n",
      "Epoch 299/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.0525e-04 - val_loss: 0.0238 - learning_rate: 6.4611e-05\n",
      "Epoch 300/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.0969e-04 - val_loss: 0.0247 - learning_rate: 6.4611e-05\n",
      "Epoch 301/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.0558e-04 - val_loss: 0.0238 - learning_rate: 6.4611e-05\n",
      "Epoch 302/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.0544e-04 - val_loss: 0.0253 - learning_rate: 6.4611e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:20:45.942477: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 303/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.0338e-04 - val_loss: 0.0245 - learning_rate: 6.4611e-05\n",
      "Epoch 304/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.0341e-04 - val_loss: 0.0263 - learning_rate: 6.4611e-05\n",
      "Epoch 305/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.0301e-04 - val_loss: 0.0252 - learning_rate: 6.4611e-05\n",
      "Epoch 306/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.0465e-04 - val_loss: 0.0255 - learning_rate: 6.4611e-05\n",
      "Epoch 307/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 1.0343e-04\n",
      "Epoch 307: ReduceLROnPlateau reducing learning rate to 5.8149741380475466e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.0507e-04 - val_loss: 0.0243 - learning_rate: 6.4611e-05\n",
      "Epoch 308/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.0453e-04 - val_loss: 0.0249 - learning_rate: 5.8150e-05\n",
      "Epoch 309/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.0260e-04 - val_loss: 0.0266 - learning_rate: 5.8150e-05\n",
      "Epoch 310/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 1.0201e-04 - val_loss: 0.0250 - learning_rate: 5.8150e-05\n",
      "Epoch 311/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step - loss: 9.9255e-05 - val_loss: 0.0245 - learning_rate: 5.8150e-05\n",
      "Epoch 312/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.0348e-04 - val_loss: 0.0246 - learning_rate: 5.8150e-05\n",
      "Epoch 313/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 1.0021e-04 - val_loss: 0.0246 - learning_rate: 5.8150e-05\n",
      "Epoch 314/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 9.8557e-05 - val_loss: 0.0248 - learning_rate: 5.8150e-05\n",
      "Epoch 315/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.0325e-04 - val_loss: 0.0244 - learning_rate: 5.8150e-05\n",
      "Epoch 316/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.0036e-04 - val_loss: 0.0246 - learning_rate: 5.8150e-05\n",
      "Epoch 317/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 9.7497e-05\n",
      "Epoch 317: ReduceLROnPlateau reducing learning rate to 5.233476658759173e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.9207e-05 - val_loss: 0.0242 - learning_rate: 5.8150e-05\n",
      "Epoch 318/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 9.9359e-05 - val_loss: 0.0249 - learning_rate: 5.2335e-05\n",
      "Epoch 319/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.0061e-04 - val_loss: 0.0244 - learning_rate: 5.2335e-05\n",
      "Epoch 320/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 1.0050e-04 - val_loss: 0.0253 - learning_rate: 5.2335e-05\n",
      "Epoch 321/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.6819e-05 - val_loss: 0.0244 - learning_rate: 5.2335e-05\n",
      "Epoch 322/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 9.8391e-05 - val_loss: 0.0247 - learning_rate: 5.2335e-05\n",
      "Epoch 323/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.7905e-05 - val_loss: 0.0244 - learning_rate: 5.2335e-05\n",
      "Epoch 324/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 9.9994e-05 - val_loss: 0.0239 - learning_rate: 5.2335e-05\n",
      "Epoch 325/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 9.5292e-05 - val_loss: 0.0260 - learning_rate: 5.2335e-05\n",
      "Epoch 326/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 9.9469e-05 - val_loss: 0.0257 - learning_rate: 5.2335e-05\n",
      "Epoch 327/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 9.6607e-05\n",
      "Epoch 327: ReduceLROnPlateau reducing learning rate to 4.7101289601414466e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.8209e-05 - val_loss: 0.0241 - learning_rate: 5.2335e-05\n",
      "Epoch 328/2000\n",
      "\u001b[1m 1/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 269ms/step - loss: 9.8211e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:21:16.103874: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 9.6367e-05 - val_loss: 0.0247 - learning_rate: 4.7101e-05\n",
      "Epoch 329/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 9.7267e-05 - val_loss: 0.0253 - learning_rate: 4.7101e-05\n",
      "Epoch 330/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 9.7317e-05 - val_loss: 0.0257 - learning_rate: 4.7101e-05\n",
      "Epoch 331/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.6171e-05 - val_loss: 0.0250 - learning_rate: 4.7101e-05\n",
      "Epoch 332/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.7031e-05 - val_loss: 0.0249 - learning_rate: 4.7101e-05\n",
      "Epoch 333/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.3356e-05 - val_loss: 0.0248 - learning_rate: 4.7101e-05\n",
      "Epoch 334/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.6832e-05 - val_loss: 0.0254 - learning_rate: 4.7101e-05\n",
      "Epoch 335/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.2881e-05 - val_loss: 0.0258 - learning_rate: 4.7101e-05\n",
      "Epoch 336/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.6881e-05 - val_loss: 0.0249 - learning_rate: 4.7101e-05\n",
      "Epoch 337/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 1.0003e-04\n",
      "Epoch 337: ReduceLROnPlateau reducing learning rate to 4.239116096869111e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 1.0015e-04 - val_loss: 0.0249 - learning_rate: 4.7101e-05\n",
      "Epoch 338/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 9.3917e-05 - val_loss: 0.0248 - learning_rate: 4.2391e-05\n",
      "Epoch 339/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 9.4877e-05 - val_loss: 0.0238 - learning_rate: 4.2391e-05\n",
      "Epoch 340/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 9.3698e-05 - val_loss: 0.0257 - learning_rate: 4.2391e-05\n",
      "Epoch 341/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.2683e-05 - val_loss: 0.0253 - learning_rate: 4.2391e-05\n",
      "Epoch 342/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step - loss: 9.4663e-05 - val_loss: 0.0253 - learning_rate: 4.2391e-05\n",
      "Epoch 343/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.2603e-05 - val_loss: 0.0263 - learning_rate: 4.2391e-05\n",
      "Epoch 344/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:21:35.232402: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 9.7365e-05 - val_loss: 0.0248 - learning_rate: 4.2391e-05\n",
      "Epoch 345/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 9.2339e-05 - val_loss: 0.0252 - learning_rate: 4.2391e-05\n",
      "Epoch 346/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.2265e-05 - val_loss: 0.0249 - learning_rate: 4.2391e-05\n",
      "Epoch 347/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 9.0658e-05\n",
      "Epoch 347: ReduceLROnPlateau reducing learning rate to 3.815204618149437e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.2738e-05 - val_loss: 0.0245 - learning_rate: 4.2391e-05\n",
      "Epoch 348/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.2351e-05 - val_loss: 0.0250 - learning_rate: 3.8152e-05\n",
      "Epoch 349/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 9.2121e-05 - val_loss: 0.0256 - learning_rate: 3.8152e-05\n",
      "Epoch 350/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 9.2840e-05 - val_loss: 0.0254 - learning_rate: 3.8152e-05\n",
      "Epoch 351/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 9.1296e-05 - val_loss: 0.0242 - learning_rate: 3.8152e-05\n",
      "Epoch 352/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.1852e-05 - val_loss: 0.0249 - learning_rate: 3.8152e-05\n",
      "Epoch 353/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 8.9917e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:21:46.951498: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.0845e-05 - val_loss: 0.0253 - learning_rate: 3.8152e-05\n",
      "Epoch 354/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.4807e-05 - val_loss: 0.0243 - learning_rate: 3.8152e-05\n",
      "Epoch 355/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 8.7594e-05 - val_loss: 0.0252 - learning_rate: 3.8152e-05\n",
      "Epoch 356/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 9.1454e-05 - val_loss: 0.0250 - learning_rate: 3.8152e-05\n",
      "Epoch 357/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 8.8630e-05\n",
      "Epoch 357: ReduceLROnPlateau reducing learning rate to 3.4336842873017304e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 9.0167e-05 - val_loss: 0.0247 - learning_rate: 3.8152e-05\n",
      "Epoch 358/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 9.2535e-05 - val_loss: 0.0257 - learning_rate: 3.4337e-05\n",
      "Epoch 359/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.2992e-05 - val_loss: 0.0255 - learning_rate: 3.4337e-05\n",
      "Epoch 360/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 9.0597e-05 - val_loss: 0.0246 - learning_rate: 3.4337e-05\n",
      "Epoch 361/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 9.1173e-05 - val_loss: 0.0247 - learning_rate: 3.4337e-05\n",
      "Epoch 362/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 8.9796e-05 - val_loss: 0.0246 - learning_rate: 3.4337e-05\n",
      "Epoch 363/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 8.7129e-05 - val_loss: 0.0252 - learning_rate: 3.4337e-05\n",
      "Epoch 364/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 8.9602e-05 - val_loss: 0.0249 - learning_rate: 3.4337e-05\n",
      "Epoch 365/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 8.9346e-05 - val_loss: 0.0250 - learning_rate: 3.4337e-05\n",
      "Epoch 366/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.8737e-05 - val_loss: 0.0243 - learning_rate: 3.4337e-05\n",
      "Epoch 367/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 9.1453e-05\n",
      "Epoch 367: ReduceLROnPlateau reducing learning rate to 3.0903160222806036e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 9.2182e-05 - val_loss: 0.0237 - learning_rate: 3.4337e-05\n",
      "Epoch 368/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 9.0249e-05 - val_loss: 0.0247 - learning_rate: 3.0903e-05\n",
      "Epoch 369/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.9090e-05 - val_loss: 0.0244 - learning_rate: 3.0903e-05\n",
      "Epoch 370/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 9.0107e-05 - val_loss: 0.0246 - learning_rate: 3.0903e-05\n",
      "Epoch 371/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.8993e-05 - val_loss: 0.0253 - learning_rate: 3.0903e-05\n",
      "Epoch 372/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step - loss: 8.8030e-05 - val_loss: 0.0244 - learning_rate: 3.0903e-05\n",
      "Epoch 373/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 8.7254e-05 - val_loss: 0.0253 - learning_rate: 3.0903e-05\n",
      "Epoch 374/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 8.7733e-05 - val_loss: 0.0250 - learning_rate: 3.0903e-05\n",
      "Epoch 375/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.7043e-05 - val_loss: 0.0249 - learning_rate: 3.0903e-05\n",
      "Epoch 376/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.6572e-05 - val_loss: 0.0245 - learning_rate: 3.0903e-05\n",
      "Epoch 377/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 8.5124e-05\n",
      "Epoch 377: ReduceLROnPlateau reducing learning rate to 2.7812844200525434e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.6401e-05 - val_loss: 0.0251 - learning_rate: 3.0903e-05\n",
      "Epoch 378/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.7647e-05 - val_loss: 0.0243 - learning_rate: 2.7813e-05\n",
      "Epoch 379/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:22:17.040929: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.7505e-05 - val_loss: 0.0236 - learning_rate: 2.7813e-05\n",
      "Epoch 380/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.8302e-05 - val_loss: 0.0256 - learning_rate: 2.7813e-05\n",
      "Epoch 381/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 8.6146e-05 - val_loss: 0.0242 - learning_rate: 2.7813e-05\n",
      "Epoch 382/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 8.7925e-05 - val_loss: 0.0250 - learning_rate: 2.7813e-05\n",
      "Epoch 383/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 8.6996e-05 - val_loss: 0.0250 - learning_rate: 2.7813e-05\n",
      "Epoch 384/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.5550e-05 - val_loss: 0.0248 - learning_rate: 2.7813e-05\n",
      "Epoch 385/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 8.8338e-05 - val_loss: 0.0247 - learning_rate: 2.7813e-05\n",
      "Epoch 386/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 8.6255e-05 - val_loss: 0.0257 - learning_rate: 2.7813e-05\n",
      "Epoch 387/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 8.6821e-05\n",
      "Epoch 387: ReduceLROnPlateau reducing learning rate to 2.5031560107890984e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.7241e-05 - val_loss: 0.0253 - learning_rate: 2.7813e-05\n",
      "Epoch 388/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 8.2967e-05 - val_loss: 0.0252 - learning_rate: 2.5032e-05\n",
      "Epoch 389/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 8.6803e-05 - val_loss: 0.0252 - learning_rate: 2.5032e-05\n",
      "Epoch 390/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 8.7640e-05 - val_loss: 0.0254 - learning_rate: 2.5032e-05\n",
      "Epoch 391/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 8.8391e-05 - val_loss: 0.0253 - learning_rate: 2.5032e-05\n",
      "Epoch 392/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.4396e-05 - val_loss: 0.0253 - learning_rate: 2.5032e-05\n",
      "Epoch 393/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 8.4910e-05 - val_loss: 0.0255 - learning_rate: 2.5032e-05\n",
      "Epoch 394/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 8.4251e-05 - val_loss: 0.0242 - learning_rate: 2.5032e-05\n",
      "Epoch 395/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.6279e-05 - val_loss: 0.0247 - learning_rate: 2.5032e-05\n",
      "Epoch 396/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.6971e-05 - val_loss: 0.0251 - learning_rate: 2.5032e-05\n",
      "Epoch 397/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 7.9262e-05\n",
      "Epoch 397: ReduceLROnPlateau reducing learning rate to 2.2528404588229024e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 8.0955e-05 - val_loss: 0.0249 - learning_rate: 2.5032e-05\n",
      "Epoch 398/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - loss: 8.3880e-05 - val_loss: 0.0245 - learning_rate: 2.2528e-05\n",
      "Epoch 399/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.5903e-05 - val_loss: 0.0245 - learning_rate: 2.2528e-05\n",
      "Epoch 400/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.1960e-05 - val_loss: 0.0253 - learning_rate: 2.2528e-05\n",
      "Epoch 401/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 8.2284e-05 - val_loss: 0.0252 - learning_rate: 2.2528e-05\n",
      "Epoch 402/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 8.3658e-05 - val_loss: 0.0247 - learning_rate: 2.2528e-05\n",
      "Epoch 403/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.0880e-05 - val_loss: 0.0250 - learning_rate: 2.2528e-05\n",
      "Epoch 404/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 8.5236e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:22:47.859479: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 8.6149e-05 - val_loss: 0.0250 - learning_rate: 2.2528e-05\n",
      "Epoch 405/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 8.3035e-05 - val_loss: 0.0255 - learning_rate: 2.2528e-05\n",
      "Epoch 406/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 8.2828e-05 - val_loss: 0.0252 - learning_rate: 2.2528e-05\n",
      "Epoch 407/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 8.3497e-05\n",
      "Epoch 407: ReduceLROnPlateau reducing learning rate to 2.0275563474569936e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.4339e-05 - val_loss: 0.0257 - learning_rate: 2.2528e-05\n",
      "Epoch 408/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.3403e-05 - val_loss: 0.0258 - learning_rate: 2.0276e-05\n",
      "Epoch 409/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 8.3775e-05 - val_loss: 0.0247 - learning_rate: 2.0276e-05\n",
      "Epoch 410/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.3766e-05 - val_loss: 0.0267 - learning_rate: 2.0276e-05\n",
      "Epoch 411/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 8.1559e-05 - val_loss: 0.0253 - learning_rate: 2.0276e-05\n",
      "Epoch 412/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 8.2642e-05 - val_loss: 0.0254 - learning_rate: 2.0276e-05\n",
      "Epoch 413/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.3616e-05 - val_loss: 0.0257 - learning_rate: 2.0276e-05\n",
      "Epoch 414/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.1612e-05 - val_loss: 0.0241 - learning_rate: 2.0276e-05\n",
      "Epoch 415/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 8.1238e-05 - val_loss: 0.0259 - learning_rate: 2.0276e-05\n",
      "Epoch 416/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 8.2224e-05 - val_loss: 0.0245 - learning_rate: 2.0276e-05\n",
      "Epoch 417/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 8.0241e-05\n",
      "Epoch 417: ReduceLROnPlateau reducing learning rate to 1.8248007290821987e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.1329e-05 - val_loss: 0.0257 - learning_rate: 2.0276e-05\n",
      "Epoch 418/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 8.0278e-05 - val_loss: 0.0248 - learning_rate: 1.8248e-05\n",
      "Epoch 419/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 8.1978e-05 - val_loss: 0.0252 - learning_rate: 1.8248e-05\n",
      "Epoch 420/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 8.1411e-05 - val_loss: 0.0256 - learning_rate: 1.8248e-05\n",
      "Epoch 421/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.3000e-05 - val_loss: 0.0237 - learning_rate: 1.8248e-05\n",
      "Epoch 422/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 8.2675e-05 - val_loss: 0.0257 - learning_rate: 1.8248e-05\n",
      "Epoch 423/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 8.3476e-05 - val_loss: 0.0245 - learning_rate: 1.8248e-05\n",
      "Epoch 424/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - loss: 8.0756e-05 - val_loss: 0.0254 - learning_rate: 1.8248e-05\n",
      "Epoch 425/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.1809e-05 - val_loss: 0.0250 - learning_rate: 1.8248e-05\n",
      "Epoch 426/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.1843e-05 - val_loss: 0.0248 - learning_rate: 1.8248e-05\n",
      "Epoch 427/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 8.0423e-05\n",
      "Epoch 427: ReduceLROnPlateau reducing learning rate to 1.6423206398030745e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 8.1445e-05 - val_loss: 0.0253 - learning_rate: 1.8248e-05\n",
      "Epoch 428/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 8.0828e-05 - val_loss: 0.0252 - learning_rate: 1.6423e-05\n",
      "Epoch 429/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 8.3103e-05 - val_loss: 0.0260 - learning_rate: 1.6423e-05\n",
      "Epoch 430/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:23:17.874307: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 8.0188e-05 - val_loss: 0.0250 - learning_rate: 1.6423e-05\n",
      "Epoch 431/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.2621e-05 - val_loss: 0.0248 - learning_rate: 1.6423e-05\n",
      "Epoch 432/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.9974e-05 - val_loss: 0.0259 - learning_rate: 1.6423e-05\n",
      "Epoch 433/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 8.0891e-05 - val_loss: 0.0245 - learning_rate: 1.6423e-05\n",
      "Epoch 434/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.9502e-05 - val_loss: 0.0237 - learning_rate: 1.6423e-05\n",
      "Epoch 435/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 7.8369e-05 - val_loss: 0.0246 - learning_rate: 1.6423e-05\n",
      "Epoch 436/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.9187e-05 - val_loss: 0.0249 - learning_rate: 1.6423e-05\n",
      "Epoch 437/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 7.9041e-05\n",
      "Epoch 437: ReduceLROnPlateau reducing learning rate to 1.4780885430809576e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 8.0346e-05 - val_loss: 0.0244 - learning_rate: 1.6423e-05\n",
      "Epoch 438/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.7607e-05 - val_loss: 0.0249 - learning_rate: 1.4781e-05\n",
      "Epoch 439/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 7.7929e-05 - val_loss: 0.0243 - learning_rate: 1.4781e-05\n",
      "Epoch 440/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 8.1008e-05 - val_loss: 0.0244 - learning_rate: 1.4781e-05\n",
      "Epoch 441/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 8.0304e-05 - val_loss: 0.0250 - learning_rate: 1.4781e-05\n",
      "Epoch 442/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.9188e-05 - val_loss: 0.0240 - learning_rate: 1.4781e-05\n",
      "Epoch 443/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.8235e-05 - val_loss: 0.0248 - learning_rate: 1.4781e-05\n",
      "Epoch 444/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.9150e-05 - val_loss: 0.0238 - learning_rate: 1.4781e-05\n",
      "Epoch 445/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.6361e-05 - val_loss: 0.0248 - learning_rate: 1.4781e-05\n",
      "Epoch 446/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.0791e-05 - val_loss: 0.0256 - learning_rate: 1.4781e-05\n",
      "Epoch 447/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 7.8226e-05\n",
      "Epoch 447: ReduceLROnPlateau reducing learning rate to 1.3302796560310526e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.8897e-05 - val_loss: 0.0241 - learning_rate: 1.4781e-05\n",
      "Epoch 448/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.9589e-05 - val_loss: 0.0252 - learning_rate: 1.3303e-05\n",
      "Epoch 449/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.8308e-05 - val_loss: 0.0249 - learning_rate: 1.3303e-05\n",
      "Epoch 450/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.7976e-05 - val_loss: 0.0255 - learning_rate: 1.3303e-05\n",
      "Epoch 451/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.9863e-05 - val_loss: 0.0256 - learning_rate: 1.3303e-05\n",
      "Epoch 452/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.6481e-05 - val_loss: 0.0262 - learning_rate: 1.3303e-05\n",
      "Epoch 453/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.8795e-05 - val_loss: 0.0259 - learning_rate: 1.3303e-05\n",
      "Epoch 454/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.8446e-05 - val_loss: 0.0256 - learning_rate: 1.3303e-05\n",
      "Epoch 455/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:23:48.011951: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 7.8781e-05 - val_loss: 0.0252 - learning_rate: 1.3303e-05\n",
      "Epoch 456/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.1334e-05 - val_loss: 0.0258 - learning_rate: 1.3303e-05\n",
      "Epoch 457/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 8.0167e-05\n",
      "Epoch 457: ReduceLROnPlateau reducing learning rate to 1.1972517313552089e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.0767e-05 - val_loss: 0.0253 - learning_rate: 1.3303e-05\n",
      "Epoch 458/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.9267e-05 - val_loss: 0.0247 - learning_rate: 1.1973e-05\n",
      "Epoch 459/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.9665e-05 - val_loss: 0.0257 - learning_rate: 1.1973e-05\n",
      "Epoch 460/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 8.0184e-05 - val_loss: 0.0254 - learning_rate: 1.1973e-05\n",
      "Epoch 461/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.9509e-05 - val_loss: 0.0253 - learning_rate: 1.1973e-05\n",
      "Epoch 462/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.9271e-05 - val_loss: 0.0254 - learning_rate: 1.1973e-05\n",
      "Epoch 463/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.6685e-05 - val_loss: 0.0254 - learning_rate: 1.1973e-05\n",
      "Epoch 464/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.7769e-05 - val_loss: 0.0253 - learning_rate: 1.1973e-05\n",
      "Epoch 465/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 8.1385e-05 - val_loss: 0.0253 - learning_rate: 1.1973e-05\n",
      "Epoch 466/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.6031e-05 - val_loss: 0.0250 - learning_rate: 1.1973e-05\n",
      "Epoch 467/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 7.6026e-05\n",
      "Epoch 467: ReduceLROnPlateau reducing learning rate to 1.077526558219688e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.7308e-05 - val_loss: 0.0251 - learning_rate: 1.1973e-05\n",
      "Epoch 468/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 7.8029e-05 - val_loss: 0.0254 - learning_rate: 1.0775e-05\n",
      "Epoch 469/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 8.0705e-05 - val_loss: 0.0245 - learning_rate: 1.0775e-05\n",
      "Epoch 470/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 7.7464e-05 - val_loss: 0.0247 - learning_rate: 1.0775e-05\n",
      "Epoch 471/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.6275e-05 - val_loss: 0.0260 - learning_rate: 1.0775e-05\n",
      "Epoch 472/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.8392e-05 - val_loss: 0.0249 - learning_rate: 1.0775e-05\n",
      "Epoch 473/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.9138e-05 - val_loss: 0.0248 - learning_rate: 1.0775e-05\n",
      "Epoch 474/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.9185e-05 - val_loss: 0.0263 - learning_rate: 1.0775e-05\n",
      "Epoch 475/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.6863e-05 - val_loss: 0.0261 - learning_rate: 1.0775e-05\n",
      "Epoch 476/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.6907e-05 - val_loss: 0.0245 - learning_rate: 1.0775e-05\n",
      "Epoch 477/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 7.7402e-05\n",
      "Epoch 477: ReduceLROnPlateau reducing learning rate to 9.697739187686238e-06.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 7.8046e-05 - val_loss: 0.0248 - learning_rate: 1.0775e-05\n",
      "Epoch 478/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.5649e-05 - val_loss: 0.0246 - learning_rate: 9.6977e-06\n",
      "Epoch 479/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.5695e-05 - val_loss: 0.0247 - learning_rate: 9.6977e-06\n",
      "Epoch 480/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 7.2640e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:24:18.659307: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.3819e-05 - val_loss: 0.0255 - learning_rate: 9.6977e-06\n",
      "Epoch 481/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.8484e-05 - val_loss: 0.0243 - learning_rate: 9.6977e-06\n",
      "Epoch 482/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.5961e-05 - val_loss: 0.0260 - learning_rate: 9.6977e-06\n",
      "Epoch 483/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.6487e-05 - val_loss: 0.0253 - learning_rate: 9.6977e-06\n",
      "Epoch 484/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.7355e-05 - val_loss: 0.0265 - learning_rate: 9.6977e-06\n",
      "Epoch 485/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.8135e-05 - val_loss: 0.0261 - learning_rate: 9.6977e-06\n",
      "Epoch 486/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.6289e-05 - val_loss: 0.0245 - learning_rate: 9.6977e-06\n",
      "Epoch 487/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 7.6076e-05\n",
      "Epoch 487: ReduceLROnPlateau reducing learning rate to 8.727965268917615e-06.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.7145e-05 - val_loss: 0.0252 - learning_rate: 9.6977e-06\n",
      "Epoch 488/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.8463e-05 - val_loss: 0.0251 - learning_rate: 8.7280e-06\n",
      "Epoch 489/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.6136e-05 - val_loss: 0.0259 - learning_rate: 8.7280e-06\n",
      "Epoch 490/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.5187e-05 - val_loss: 0.0247 - learning_rate: 8.7280e-06\n",
      "Epoch 491/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.6873e-05 - val_loss: 0.0254 - learning_rate: 8.7280e-06\n",
      "Epoch 492/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.6183e-05 - val_loss: 0.0261 - learning_rate: 8.7280e-06\n",
      "Epoch 493/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.5894e-05 - val_loss: 0.0252 - learning_rate: 8.7280e-06\n",
      "Epoch 494/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.5911e-05 - val_loss: 0.0251 - learning_rate: 8.7280e-06\n",
      "Epoch 495/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.8231e-05 - val_loss: 0.0260 - learning_rate: 8.7280e-06\n",
      "Epoch 496/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.4166e-05 - val_loss: 0.0243 - learning_rate: 8.7280e-06\n",
      "Epoch 497/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 7.4788e-05\n",
      "Epoch 497: ReduceLROnPlateau reducing learning rate to 7.855168496462283e-06.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.6257e-05 - val_loss: 0.0246 - learning_rate: 8.7280e-06\n",
      "Epoch 498/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.6840e-05 - val_loss: 0.0244 - learning_rate: 7.8552e-06\n",
      "Epoch 499/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.9478e-05 - val_loss: 0.0248 - learning_rate: 7.8552e-06\n",
      "Epoch 500/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.7289e-05 - val_loss: 0.0249 - learning_rate: 7.8552e-06\n",
      "Epoch 501/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.7167e-05 - val_loss: 0.0258 - learning_rate: 7.8552e-06\n",
      "Epoch 502/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.6006e-05 - val_loss: 0.0252 - learning_rate: 7.8552e-06\n",
      "Epoch 503/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.4096e-05 - val_loss: 0.0251 - learning_rate: 7.8552e-06\n",
      "Epoch 504/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.8070e-05 - val_loss: 0.0253 - learning_rate: 7.8552e-06\n",
      "Epoch 505/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.6781e-05 - val_loss: 0.0248 - learning_rate: 7.8552e-06\n",
      "Epoch 506/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:24:48.666867: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.5302e-05 - val_loss: 0.0253 - learning_rate: 7.8552e-06\n",
      "Epoch 507/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 7.3082e-05\n",
      "Epoch 507: ReduceLROnPlateau reducing learning rate to 7.069651564961533e-06.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 7.4208e-05 - val_loss: 0.0259 - learning_rate: 7.8552e-06\n",
      "Epoch 508/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.3488e-05 - val_loss: 0.0246 - learning_rate: 7.0697e-06\n",
      "Epoch 509/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.8300e-05 - val_loss: 0.0244 - learning_rate: 7.0697e-06\n",
      "Epoch 510/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.4295e-05 - val_loss: 0.0260 - learning_rate: 7.0697e-06\n",
      "Epoch 511/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.5380e-05 - val_loss: 0.0251 - learning_rate: 7.0697e-06\n",
      "Epoch 512/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.8963e-05 - val_loss: 0.0260 - learning_rate: 7.0697e-06\n",
      "Epoch 513/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.5374e-05 - val_loss: 0.0257 - learning_rate: 7.0697e-06\n",
      "Epoch 514/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 7.5665e-05 - val_loss: 0.0247 - learning_rate: 7.0697e-06\n",
      "Epoch 515/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.3646e-05 - val_loss: 0.0253 - learning_rate: 7.0697e-06\n",
      "Epoch 516/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.4364e-05 - val_loss: 0.0268 - learning_rate: 7.0697e-06\n",
      "Epoch 517/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 7.4263e-05\n",
      "Epoch 517: ReduceLROnPlateau reducing learning rate to 7e-06.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.5178e-05 - val_loss: 0.0251 - learning_rate: 7.0697e-06\n",
      "Epoch 518/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.6526e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 519/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.5355e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 520/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.5238e-05 - val_loss: 0.0242 - learning_rate: 7.0000e-06\n",
      "Epoch 521/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.5500e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 522/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.6381e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 523/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 7.3678e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 524/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.5092e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 525/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.3192e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 526/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.3717e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 527/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.5926e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 528/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.2464e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 529/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.4322e-05 - val_loss: 0.0242 - learning_rate: 7.0000e-06\n",
      "Epoch 530/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.4284e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 531/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:25:18.810361: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.5003e-05 - val_loss: 0.0244 - learning_rate: 7.0000e-06\n",
      "Epoch 532/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.2972e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 533/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.3648e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 534/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 7.4064e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 535/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 7.3429e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 536/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.6210e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 537/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.3099e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 538/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.6391e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 539/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.1788e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 540/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.4891e-05 - val_loss: 0.0242 - learning_rate: 7.0000e-06\n",
      "Epoch 541/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.3526e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 542/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.5298e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 543/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.3679e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 544/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.3010e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 545/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.4674e-05 - val_loss: 0.0244 - learning_rate: 7.0000e-06\n",
      "Epoch 546/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.5736e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 547/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.2920e-05 - val_loss: 0.0243 - learning_rate: 7.0000e-06\n",
      "Epoch 548/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.2683e-05 - val_loss: 0.0270 - learning_rate: 7.0000e-06\n",
      "Epoch 549/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.4219e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 550/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 7.5396e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 551/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.5411e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 552/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.3284e-05 - val_loss: 0.0245 - learning_rate: 7.0000e-06\n",
      "Epoch 553/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.1562e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 554/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 7.4479e-05 - val_loss: 0.0242 - learning_rate: 7.0000e-06\n",
      "Epoch 555/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.0739e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 556/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 7.2428e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:25:49.235397: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 7.3243e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 557/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.1640e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 558/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.2285e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 559/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.2788e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 560/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.1409e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 561/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.1119e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 562/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.3199e-05 - val_loss: 0.0242 - learning_rate: 7.0000e-06\n",
      "Epoch 563/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.2459e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 564/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.5915e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 565/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 7.2705e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 566/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.0866e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 567/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.1019e-05 - val_loss: 0.0240 - learning_rate: 7.0000e-06\n",
      "Epoch 568/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.4485e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 569/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.1926e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 570/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.3540e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 571/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.3846e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 572/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 7.3666e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 573/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.2516e-05 - val_loss: 0.0245 - learning_rate: 7.0000e-06\n",
      "Epoch 574/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.0375e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 575/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.0729e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 576/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.1174e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 577/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.0849e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 578/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.9557e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 579/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.2515e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 580/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.1902e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 581/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.2653e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 582/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:26:19.239861: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.0117e-05 - val_loss: 0.0245 - learning_rate: 7.0000e-06\n",
      "Epoch 583/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.1485e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 584/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.3070e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 585/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.2065e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 586/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.1663e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 587/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.9333e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 588/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.0011e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 589/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.2715e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 590/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.9928e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 591/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.1163e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 592/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.0260e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 593/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.1152e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 594/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.0996e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 595/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.1929e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 596/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.1157e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 597/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.9867e-05 - val_loss: 0.0245 - learning_rate: 7.0000e-06\n",
      "Epoch 598/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.1916e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 599/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.2494e-05 - val_loss: 0.0243 - learning_rate: 7.0000e-06\n",
      "Epoch 600/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.3032e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 601/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 7.1021e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 602/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.1287e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 603/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.1010e-05 - val_loss: 0.0244 - learning_rate: 7.0000e-06\n",
      "Epoch 604/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.9495e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 605/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.9056e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 606/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.0758e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 607/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 6.8027e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:26:50.025833: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.9155e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 608/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.0285e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 609/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.7796e-05 - val_loss: 0.0245 - learning_rate: 7.0000e-06\n",
      "Epoch 610/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.9180e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 611/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.0028e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 612/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 6.9627e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 613/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.9158e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 614/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 7.0037e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 615/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.0044e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 616/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.9315e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 617/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.9868e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 618/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.9869e-05 - val_loss: 0.0245 - learning_rate: 7.0000e-06\n",
      "Epoch 619/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.9292e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 620/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 7.0168e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 621/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.8935e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 622/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.9626e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 623/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.8353e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 624/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 6.9302e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 625/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.0487e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 626/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.1225e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 627/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 6.8310e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 628/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 7.1108e-05 - val_loss: 0.0236 - learning_rate: 7.0000e-06\n",
      "Epoch 629/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 6.8605e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 630/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.0000e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 631/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 7.0492e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 632/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.6071e-05 - val_loss: 0.0243 - learning_rate: 7.0000e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:27:20.218677: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 633/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.8648e-05 - val_loss: 0.0241 - learning_rate: 7.0000e-06\n",
      "Epoch 634/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.9426e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 635/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.6748e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 636/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.7729e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 637/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 6.7939e-05 - val_loss: 0.0244 - learning_rate: 7.0000e-06\n",
      "Epoch 638/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.7829e-05 - val_loss: 0.0266 - learning_rate: 7.0000e-06\n",
      "Epoch 639/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.8708e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 640/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.6928e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 641/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.9723e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 642/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.9853e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 643/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 7.1525e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 644/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.5731e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 645/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.8796e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 646/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.8683e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 647/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.8146e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 648/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.6599e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 649/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 7.0222e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 650/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 6.9036e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 651/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.8944e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 652/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.7955e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 653/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.7646e-05 - val_loss: 0.0245 - learning_rate: 7.0000e-06\n",
      "Epoch 654/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.8081e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 655/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.6621e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 656/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.7846e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 657/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 6.7849e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 658/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:27:50.389140: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.5615e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 659/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.6847e-05 - val_loss: 0.0245 - learning_rate: 7.0000e-06\n",
      "Epoch 660/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 6.6853e-05 - val_loss: 0.0242 - learning_rate: 7.0000e-06\n",
      "Epoch 661/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.7075e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 662/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 6.5440e-05 - val_loss: 0.0266 - learning_rate: 7.0000e-06\n",
      "Epoch 663/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.6795e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 664/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.8430e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 665/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.6703e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 666/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.8796e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 667/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.7975e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 668/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.7664e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 669/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 6.6958e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 670/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.6712e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 671/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 6.6954e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 672/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.8234e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 673/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.8314e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 674/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.5160e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 675/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 6.5416e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 676/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.5937e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 677/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.8288e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 678/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.4902e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 679/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 6.6770e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 680/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.7667e-05 - val_loss: 0.0244 - learning_rate: 7.0000e-06\n",
      "Epoch 681/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.7869e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 682/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 6.7144e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 683/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 6.3682e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:28:21.043426: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.4915e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 684/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.7175e-05 - val_loss: 0.0244 - learning_rate: 7.0000e-06\n",
      "Epoch 685/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.5464e-05 - val_loss: 0.0240 - learning_rate: 7.0000e-06\n",
      "Epoch 686/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:28:23.540838: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.5956e-05 - val_loss: 0.0245 - learning_rate: 7.0000e-06\n",
      "Epoch 687/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 6.6066e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 688/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 114ms/step - loss: 6.6722e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 689/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.6578e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 690/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.6376e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 691/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.7717e-05 - val_loss: 0.0242 - learning_rate: 7.0000e-06\n",
      "Epoch 692/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.5845e-05 - val_loss: 0.0241 - learning_rate: 7.0000e-06\n",
      "Epoch 693/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 6.4874e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 694/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.7491e-05 - val_loss: 0.0243 - learning_rate: 7.0000e-06\n",
      "Epoch 695/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.7736e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 696/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.3757e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 697/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.3392e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 698/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 6.6396e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 699/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.7238e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 700/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.6536e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 701/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.6521e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 702/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.5714e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 703/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.6651e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 704/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.6598e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 705/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.4403e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 706/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.4595e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 707/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.4181e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 708/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.4897e-05 - val_loss: 0.0243 - learning_rate: 7.0000e-06\n",
      "Epoch 709/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:28:51.136549: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.5519e-05 - val_loss: 0.0243 - learning_rate: 7.0000e-06\n",
      "Epoch 710/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.5289e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 711/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 6.4564e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 712/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.5753e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 713/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.4757e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 714/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.4452e-05 - val_loss: 0.0245 - learning_rate: 7.0000e-06\n",
      "Epoch 715/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.4495e-05 - val_loss: 0.0267 - learning_rate: 7.0000e-06\n",
      "Epoch 716/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.3235e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 717/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.3444e-05 - val_loss: 0.0244 - learning_rate: 7.0000e-06\n",
      "Epoch 718/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.3217e-05 - val_loss: 0.0241 - learning_rate: 7.0000e-06\n",
      "Epoch 719/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.5121e-05 - val_loss: 0.0245 - learning_rate: 7.0000e-06\n",
      "Epoch 720/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.5659e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 721/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.5304e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 722/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.4538e-05 - val_loss: 0.0240 - learning_rate: 7.0000e-06\n",
      "Epoch 723/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.4429e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 724/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.4799e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 725/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.3727e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 726/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step - loss: 6.4576e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 727/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.5790e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 728/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.3269e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 729/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.4744e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 730/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.3127e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 731/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 6.2198e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 732/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 6.5405e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 733/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.5701e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 734/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 6.0471e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:29:21.936658: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.1730e-05 - val_loss: 0.0268 - learning_rate: 7.0000e-06\n",
      "Epoch 735/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.1526e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 736/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.3226e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 737/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.2740e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 738/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 6.2578e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 739/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.3904e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 740/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.3418e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 741/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.3007e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 742/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.3263e-05 - val_loss: 0.0244 - learning_rate: 7.0000e-06\n",
      "Epoch 743/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.2436e-05 - val_loss: 0.0269 - learning_rate: 7.0000e-06\n",
      "Epoch 744/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.4118e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 745/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.3377e-05 - val_loss: 0.0245 - learning_rate: 7.0000e-06\n",
      "Epoch 746/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.3370e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 747/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.1560e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 748/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.3851e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 749/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.2103e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 750/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.2018e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 751/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.3292e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 752/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.3019e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 753/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 6.1893e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 754/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.2149e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 755/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.3045e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 756/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.0825e-05 - val_loss: 0.0245 - learning_rate: 7.0000e-06\n",
      "Epoch 757/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - loss: 6.1964e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 758/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.1447e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 759/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.2019e-05 - val_loss: 0.0244 - learning_rate: 7.0000e-06\n",
      "Epoch 760/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:29:52.109143: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.2056e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 761/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.0227e-05 - val_loss: 0.0241 - learning_rate: 7.0000e-06\n",
      "Epoch 762/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.0143e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 763/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.1140e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 764/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 6.2547e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 765/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.2143e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 766/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.1288e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 767/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.9297e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 768/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.3035e-05 - val_loss: 0.0245 - learning_rate: 7.0000e-06\n",
      "Epoch 769/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 6.2441e-05 - val_loss: 0.0270 - learning_rate: 7.0000e-06\n",
      "Epoch 770/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.0616e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 771/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.1989e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 772/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.3888e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 773/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.3096e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 774/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.9632e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 775/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 6.1345e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 776/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.9290e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 777/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.0442e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 778/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.8516e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 779/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.0348e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 780/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.9041e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 781/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.0586e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 782/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.1021e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 783/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 6.1051e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 784/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.9475e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 785/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.1909e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:30:22.600942: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 786/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.1350e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 787/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - loss: 5.9134e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 788/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.8592e-05 - val_loss: 0.0269 - learning_rate: 7.0000e-06\n",
      "Epoch 789/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 6.2170e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 790/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.9862e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 791/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.9402e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 792/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.9171e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 793/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.0150e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 794/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.7054e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 795/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.9934e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 796/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.9619e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 797/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 6.0362e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 798/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.0913e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 799/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.9870e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 800/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.1322e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 801/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.8972e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 802/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.8828e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 803/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.8789e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 804/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.0641e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 805/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.9559e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 806/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.9099e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 807/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.9396e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 808/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.9678e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 809/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.9133e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 810/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.8310e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 811/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:30:52.621634: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.8735e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 812/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.8281e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 813/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 115ms/step - loss: 5.8124e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 814/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 5.9045e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 815/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.9011e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 816/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.9389e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 817/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.0335e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 818/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.7195e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 819/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.8880e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 820/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.8684e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 821/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.9942e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 822/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 6.1446e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 823/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.8103e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 824/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.7476e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 825/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.7473e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 826/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.6331e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 827/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.7493e-05 - val_loss: 0.0243 - learning_rate: 7.0000e-06\n",
      "Epoch 828/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.8798e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 829/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.9802e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 830/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.7580e-05 - val_loss: 0.0240 - learning_rate: 7.0000e-06\n",
      "Epoch 831/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.6778e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 832/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.7580e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 833/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.8503e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 834/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.9834e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 835/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.8777e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 836/2000\n",
      "\u001b[1m 1/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 252ms/step - loss: 5.0490e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:31:22.691651: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.7709e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 837/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.7082e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 838/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.7779e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 839/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - loss: 5.9179e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 840/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.6313e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 841/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.7850e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 842/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.7391e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 843/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.7738e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 844/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.7612e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 845/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.7513e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 846/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.6292e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 847/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.5150e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 848/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.4282e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 849/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.6219e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 850/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.6665e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 851/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.7610e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 852/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.7308e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 853/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.8363e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 854/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.6081e-05 - val_loss: 0.0240 - learning_rate: 7.0000e-06\n",
      "Epoch 855/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.6285e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 856/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.8033e-05 - val_loss: 0.0267 - learning_rate: 7.0000e-06\n",
      "Epoch 857/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.6143e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 858/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.6438e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 859/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.6344e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 860/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 5.6874e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 861/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 5.5106e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 862/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:31:53.431862: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.7729e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 863/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.5899e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 864/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.6525e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 865/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.5053e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 866/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.5314e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 867/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.3909e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 868/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.7134e-05 - val_loss: 0.0242 - learning_rate: 7.0000e-06\n",
      "Epoch 869/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.6721e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 870/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.4693e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 871/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.5022e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 872/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.5913e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 873/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.5169e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 874/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.6052e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 875/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.6077e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 876/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.3982e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 877/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.4101e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 878/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.5848e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 879/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.6006e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 880/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.4640e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 881/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.6270e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 882/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 5.5917e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 883/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.7050e-05 - val_loss: 0.0236 - learning_rate: 7.0000e-06\n",
      "Epoch 884/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.5486e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 885/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.5454e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 886/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.5149e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 887/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:32:23.640971: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.4666e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 888/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.3914e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 889/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.5693e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 890/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.5116e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 891/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.5459e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 892/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 5.5306e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 893/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.5705e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 894/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.4190e-05 - val_loss: 0.0245 - learning_rate: 7.0000e-06\n",
      "Epoch 895/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.4119e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 896/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.4008e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 897/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.3946e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 898/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.4690e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 899/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.5406e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 900/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.3375e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 901/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.2549e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 902/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.4213e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 903/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.5631e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 904/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.5311e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 905/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.3307e-05 - val_loss: 0.0239 - learning_rate: 7.0000e-06\n",
      "Epoch 906/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.3870e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 907/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.5485e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 908/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.5038e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 909/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.4050e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 910/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.5057e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 911/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.5153e-05 - val_loss: 0.0267 - learning_rate: 7.0000e-06\n",
      "Epoch 912/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 5.1893e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:32:54.420170: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.2892e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 913/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.4459e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 914/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 5.3877e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 915/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.3923e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 916/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.3856e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 917/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.3608e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 918/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.1952e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 919/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.3694e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 920/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.2530e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 921/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.2836e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 922/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.3948e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 923/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.3513e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 924/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.3399e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 925/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.3205e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 926/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.2193e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 927/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.3866e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 928/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.1381e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 929/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.2857e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 930/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.2310e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 931/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.2490e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 932/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.1462e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 933/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.1937e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 934/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.2995e-05 - val_loss: 0.0241 - learning_rate: 7.0000e-06\n",
      "Epoch 935/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.1727e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 936/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 5.2932e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 937/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 5.4324e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 938/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 5.1710e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:33:25.215477: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.2326e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 939/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.2862e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 940/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.2723e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 941/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.3269e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 942/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.1898e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 943/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.1746e-05 - val_loss: 0.0241 - learning_rate: 7.0000e-06\n",
      "Epoch 944/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.2451e-05 - val_loss: 0.0244 - learning_rate: 7.0000e-06\n",
      "Epoch 945/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.3808e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 946/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.2359e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 947/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.1835e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 948/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.1713e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 949/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.3100e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 950/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.0512e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 951/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.0932e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 952/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.3602e-05 - val_loss: 0.0244 - learning_rate: 7.0000e-06\n",
      "Epoch 953/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 5.1595e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 954/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.1685e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 955/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.2305e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 956/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.1595e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 957/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.1983e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 958/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.2562e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 959/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 5.1087e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 960/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.2382e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 961/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.0587e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 962/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.2510e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 963/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 5.0785e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:33:55.293253: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.1498e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 964/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.0692e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 965/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.2253e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 966/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.1568e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 967/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.1117e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 968/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.9407e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 969/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.2324e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 970/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.0198e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 971/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.8546e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 972/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.0183e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 973/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 5.1280e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 974/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.9316e-05 - val_loss: 0.0266 - learning_rate: 7.0000e-06\n",
      "Epoch 975/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.9780e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 976/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.2229e-05 - val_loss: 0.0243 - learning_rate: 7.0000e-06\n",
      "Epoch 977/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.0842e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 978/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.0489e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 979/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.0438e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 980/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.9981e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 981/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.9823e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 982/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.1723e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 983/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.1001e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 984/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.0389e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 985/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.0036e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 986/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.9868e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 987/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.0206e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 988/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.0986e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 989/2000\n",
      "\u001b[1m 1/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 272ms/step - loss: 5.3210e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:34:25.459630: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.1598e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 990/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.0907e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 991/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.9853e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 992/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.9593e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 993/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.9870e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 994/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 5.1376e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 995/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.9776e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 996/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.9230e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 997/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.9620e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 998/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.8860e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 999/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 5.1283e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1000/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.9946e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 1001/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.9044e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1002/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.8965e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 1003/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.0474e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1004/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.8894e-05 - val_loss: 0.0242 - learning_rate: 7.0000e-06\n",
      "Epoch 1005/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.8323e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1006/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 4.9884e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1007/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.0990e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 1008/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.0582e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1009/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.9589e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1010/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.8461e-05 - val_loss: 0.0267 - learning_rate: 7.0000e-06\n",
      "Epoch 1011/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.9116e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1012/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 5.0108e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1013/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 4.8632e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1014/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 4.8989e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:34:56.009895: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.9746e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1015/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.6718e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 1016/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.8183e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1017/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.9596e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1018/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.9068e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 1019/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.9467e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1020/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.9517e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1021/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.8621e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1022/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.7277e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 1023/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.0034e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1024/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.9780e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1025/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 5.0579e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1026/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.7461e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1027/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.7984e-05 - val_loss: 0.0244 - learning_rate: 7.0000e-06\n",
      "Epoch 1028/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.9479e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1029/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.6886e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1030/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.7051e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1031/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.8862e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1032/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.9537e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1033/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.8482e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1034/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.6784e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1035/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.7971e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 1036/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.8380e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1037/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.8316e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1038/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.7740e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1039/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.6586e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1040/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:35:26.025951: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.5742e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1041/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.8668e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1042/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.7599e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 1043/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.8001e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1044/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.8414e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1045/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.7637e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1046/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.6898e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1047/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.7526e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1048/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.8073e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1049/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.7343e-05 - val_loss: 0.0272 - learning_rate: 7.0000e-06\n",
      "Epoch 1050/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.8136e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1051/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5995e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1052/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.7381e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1053/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.6329e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1054/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.6642e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1055/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.9160e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1056/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5801e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1057/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.6586e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1058/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.7372e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1059/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.6802e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1060/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5974e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1061/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.6160e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1062/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.8839e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1063/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 4.8266e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1064/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5612e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1065/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:35:56.206747: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.6975e-05 - val_loss: 0.0245 - learning_rate: 7.0000e-06\n",
      "Epoch 1066/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.6291e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1067/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.6309e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1068/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.6455e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1069/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.6640e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1070/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.6891e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1071/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.6129e-05 - val_loss: 0.0244 - learning_rate: 7.0000e-06\n",
      "Epoch 1072/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5058e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 1073/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.5991e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1074/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.6060e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1075/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.6321e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 1076/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.6356e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 1077/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.5609e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1078/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.4591e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1079/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 4.5139e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1080/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5654e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1081/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.5885e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 1082/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.5307e-05 - val_loss: 0.0244 - learning_rate: 7.0000e-06\n",
      "Epoch 1083/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.5006e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1084/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.5577e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1085/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.5538e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1086/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.5108e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1087/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.6923e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1088/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5786e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1089/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.6347e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1090/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 4.4914e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:36:26.730555: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5971e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1091/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.6089e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1092/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.6640e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1093/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.6935e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1094/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.7364e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1095/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.5183e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1096/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.4934e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 1097/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.6527e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1098/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.4712e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1099/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.4068e-05 - val_loss: 0.0271 - learning_rate: 7.0000e-06\n",
      "Epoch 1100/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.6501e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1101/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.5427e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1102/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5816e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1103/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - loss: 4.5588e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1104/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.4440e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1105/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.4023e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1106/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.5631e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1107/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.6242e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1108/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.6181e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1109/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.4512e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1110/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5490e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1111/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.4632e-05 - val_loss: 0.0266 - learning_rate: 7.0000e-06\n",
      "Epoch 1112/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.4706e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1113/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.4651e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 1114/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.4138e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1115/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.4048e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1116/2000\n",
      "\u001b[1m 1/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 253ms/step - loss: 3.9783e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:36:56.877369: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.4180e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1117/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5866e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1118/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5040e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1119/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.5478e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1120/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.3715e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1121/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.3230e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 1122/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.4487e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 1123/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.7010e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1124/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.4713e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1125/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 4.4243e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1126/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.6878e-05 - val_loss: 0.0236 - learning_rate: 7.0000e-06\n",
      "Epoch 1127/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5245e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1128/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.3024e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1129/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.5294e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1130/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.4782e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1131/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.4009e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1132/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.3921e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1133/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.1938e-05 - val_loss: 0.0244 - learning_rate: 7.0000e-06\n",
      "Epoch 1134/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.3200e-05 - val_loss: 0.0243 - learning_rate: 7.0000e-06\n",
      "Epoch 1135/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.3559e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1136/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.4615e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1137/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.4385e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1138/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.4249e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1139/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.4433e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 1140/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.3526e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1141/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 4.5882e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:37:27.766334: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - loss: 4.6404e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 1142/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.2694e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1143/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.5367e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 1144/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5279e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1145/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.4566e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1146/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.3911e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1147/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.2809e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1148/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.3428e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1149/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.2389e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1150/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.3252e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 1151/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.3532e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1152/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.3384e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1153/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.4443e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1154/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.4405e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1155/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.2380e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1156/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 4.3290e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1157/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.4063e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1158/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.1400e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1159/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.4318e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1160/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 4.2815e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 1161/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.2343e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1162/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.2667e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1163/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.3772e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1164/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.2596e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1165/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.3525e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1166/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.4444e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1167/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 4.1712e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:37:58.574396: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.2409e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1168/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.2146e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1169/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 4.3300e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1170/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.3193e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1171/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.2943e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1172/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step - loss: 4.2040e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1173/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.2234e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1174/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.2733e-05 - val_loss: 0.0245 - learning_rate: 7.0000e-06\n",
      "Epoch 1175/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.2501e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1176/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.3826e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1177/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.4272e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1178/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.2829e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1179/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 4.2442e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1180/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.4404e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1181/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.1060e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 1182/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.1482e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1183/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.1360e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1184/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.2617e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1185/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.1581e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1186/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.3091e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1187/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.1702e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1188/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 4.1021e-05 - val_loss: 0.0244 - learning_rate: 7.0000e-06\n",
      "Epoch 1189/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.2228e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 1190/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.2841e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1191/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 4.1093e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1192/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 4.0401e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:38:28.720783: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.0992e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1193/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.1351e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1194/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.1438e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 1195/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.3000e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1196/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.2754e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1197/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.2338e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1198/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.1005e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1199/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.0357e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1200/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.2491e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1201/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.0587e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1202/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - loss: 4.0910e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1203/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.1464e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1204/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 4.0234e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1205/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.2187e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1206/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.1843e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1207/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.1532e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 1208/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.0907e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1209/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.1030e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1210/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.1868e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 1211/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.2206e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1212/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.0752e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1213/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.1635e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1214/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.1036e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1215/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.0017e-05 - val_loss: 0.0242 - learning_rate: 7.0000e-06\n",
      "Epoch 1216/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.1433e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1217/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 4.1172e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:38:58.766706: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.1812e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1218/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.2259e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1219/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.0252e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 1220/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.0728e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1221/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.0295e-05 - val_loss: 0.0245 - learning_rate: 7.0000e-06\n",
      "Epoch 1222/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.2012e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1223/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.1230e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1224/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.9671e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1225/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.1089e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1226/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.0413e-05 - val_loss: 0.0243 - learning_rate: 7.0000e-06\n",
      "Epoch 1227/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.9756e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1228/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step - loss: 4.0707e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1229/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.2182e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1230/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.1706e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1231/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.0101e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1232/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.9686e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1233/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.9835e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1234/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.0361e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1235/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.9859e-05 - val_loss: 0.0266 - learning_rate: 7.0000e-06\n",
      "Epoch 1236/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.1285e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1237/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.0874e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1238/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.9631e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1239/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.0504e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1240/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.9362e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1241/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.9529e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1242/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.2539e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1243/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:39:28.910391: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.1697e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1244/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.0022e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1245/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.9039e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1246/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.0313e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1247/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.0119e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1248/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.0495e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1249/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.2018e-05 - val_loss: 0.0245 - learning_rate: 7.0000e-06\n",
      "Epoch 1250/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 4.0059e-05 - val_loss: 0.0272 - learning_rate: 7.0000e-06\n",
      "Epoch 1251/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.9451e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1252/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.9516e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1253/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.8529e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1254/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - loss: 3.8980e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1255/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.0109e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1256/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.8383e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1257/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.9026e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1258/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.8522e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1259/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.8619e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1260/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.1244e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1261/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.8901e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1262/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.8968e-05 - val_loss: 0.0270 - learning_rate: 7.0000e-06\n",
      "Epoch 1263/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 4.1030e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1264/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.9667e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1265/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.8755e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1266/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.8573e-05 - val_loss: 0.0242 - learning_rate: 7.0000e-06\n",
      "Epoch 1267/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.9556e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1268/2000\n",
      "\u001b[1m 1/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 282ms/step - loss: 3.7391e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:39:58.911912: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.8880e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1269/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.9077e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1270/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.9525e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1271/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.0437e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1272/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.8536e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1273/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.7884e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1274/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.9217e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 1275/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.8726e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1276/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.7677e-05 - val_loss: 0.0245 - learning_rate: 7.0000e-06\n",
      "Epoch 1277/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.9805e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 1278/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 4.0288e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1279/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.8765e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1280/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.8605e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1281/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.9374e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1282/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.8832e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1283/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 3.9131e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1284/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.9000e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1285/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.9355e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 1286/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.7382e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1287/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.8938e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1288/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.6740e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1289/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.9558e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1290/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.8859e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1291/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.8172e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1292/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 3.8641e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 1293/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:40:29.055770: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.8498e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1294/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.6568e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1295/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.9453e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1296/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.7757e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1297/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.9543e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1298/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.7675e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1299/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.8052e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1300/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.9066e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1301/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.6439e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1302/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.8698e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1303/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.8080e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1304/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.9132e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1305/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 3.7468e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1306/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.9379e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1307/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.7318e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1308/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.7166e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1309/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.7644e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1310/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.8705e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1311/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.7719e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1312/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.7668e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1313/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.7976e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1314/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.8666e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1315/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.7959e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1316/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.7415e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1317/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.7559e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1318/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 3.7960e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:40:59.868735: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.8469e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1319/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.7928e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1320/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.7804e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1321/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.7648e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1322/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.7562e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 1323/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.8334e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1324/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.7714e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1325/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.8544e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1326/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.5926e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 1327/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.6399e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1328/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.6104e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1329/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.8020e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1330/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.7214e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1331/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.7330e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1332/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.8112e-05 - val_loss: 0.0266 - learning_rate: 7.0000e-06\n",
      "Epoch 1333/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.6116e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 1334/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.7598e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1335/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.7837e-05 - val_loss: 0.0245 - learning_rate: 7.0000e-06\n",
      "Epoch 1336/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.7069e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1337/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.7810e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1338/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.7841e-05 - val_loss: 0.0268 - learning_rate: 7.0000e-06\n",
      "Epoch 1339/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.6456e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1340/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 3.8489e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1341/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.6347e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1342/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.7489e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1343/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 3.6467e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:41:30.114735: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.6896e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1344/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.5340e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1345/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.6669e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1346/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.7080e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1347/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.5446e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1348/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.7340e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1349/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.5602e-05 - val_loss: 0.0241 - learning_rate: 7.0000e-06\n",
      "Epoch 1350/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.6263e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 1351/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.6202e-05 - val_loss: 0.0268 - learning_rate: 7.0000e-06\n",
      "Epoch 1352/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.7028e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1353/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.6795e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1354/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.6238e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1355/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.7272e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1356/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.6248e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1357/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.5193e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1358/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.6513e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1359/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.7993e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1360/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.6303e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1361/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.6058e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1362/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.6379e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1363/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 3.6966e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1364/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.5915e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1365/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.6348e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1366/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.6247e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 1367/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.5260e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1368/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.7713e-05 - val_loss: 0.0239 - learning_rate: 7.0000e-06\n",
      "Epoch 1369/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:42:00.225067: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.7647e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1370/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 3.7303e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:42:02.515522: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.7550e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1371/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.4948e-05 - val_loss: 0.0243 - learning_rate: 7.0000e-06\n",
      "Epoch 1372/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.5456e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1373/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 3.5953e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1374/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.6935e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1375/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.6226e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 1376/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 3.5838e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1377/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.4492e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1378/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.7158e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1379/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.5509e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1380/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.6865e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1381/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.5560e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1382/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.6122e-05 - val_loss: 0.0266 - learning_rate: 7.0000e-06\n",
      "Epoch 1383/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.7241e-05 - val_loss: 0.0269 - learning_rate: 7.0000e-06\n",
      "Epoch 1384/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.6878e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1385/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.6396e-05 - val_loss: 0.0243 - learning_rate: 7.0000e-06\n",
      "Epoch 1386/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 3.5867e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1387/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 3.6571e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1388/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.6709e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 1389/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.4555e-05 - val_loss: 0.0279 - learning_rate: 7.0000e-06\n",
      "Epoch 1390/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.5912e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1391/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.5165e-05 - val_loss: 0.0267 - learning_rate: 7.0000e-06\n",
      "Epoch 1392/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.4612e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1393/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.5920e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 1394/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:42:30.479336: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.6530e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1395/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.4271e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1396/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.4566e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1397/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.5657e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1398/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.6532e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1399/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.3911e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 1400/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.6093e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1401/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.5623e-05 - val_loss: 0.0242 - learning_rate: 7.0000e-06\n",
      "Epoch 1402/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.5529e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1403/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.4925e-05 - val_loss: 0.0282 - learning_rate: 7.0000e-06\n",
      "Epoch 1404/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.4041e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1405/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.5568e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1406/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.4247e-05 - val_loss: 0.0244 - learning_rate: 7.0000e-06\n",
      "Epoch 1407/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.5904e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1408/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.4257e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1409/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - loss: 3.5727e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1410/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.5411e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 1411/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.4027e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1412/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.3744e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1413/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.5717e-05 - val_loss: 0.0270 - learning_rate: 7.0000e-06\n",
      "Epoch 1414/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.5689e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1415/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.4432e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1416/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.3980e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 1417/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.5570e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1418/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.4852e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1419/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:43:00.687583: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.4470e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1420/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.5184e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1421/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.4065e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 1422/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.3702e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1423/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.3598e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1424/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.5139e-05 - val_loss: 0.0267 - learning_rate: 7.0000e-06\n",
      "Epoch 1425/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.4686e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1426/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.5283e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1427/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.4145e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1428/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.5672e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1429/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.5168e-05 - val_loss: 0.0269 - learning_rate: 7.0000e-06\n",
      "Epoch 1430/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.3733e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1431/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 3.4311e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1432/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.3595e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1433/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.6026e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1434/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.3575e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1435/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.3970e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1436/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.4889e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 1437/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.3522e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1438/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 3.4528e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1439/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.5565e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1440/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 3.5437e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 1441/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.4039e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1442/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.3357e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1443/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 3.4559e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:43:30.891918: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.4665e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 1444/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.5178e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1445/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.3837e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1446/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.3662e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1447/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.3212e-05 - val_loss: 0.0269 - learning_rate: 7.0000e-06\n",
      "Epoch 1448/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.4777e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1449/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 3.3322e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1450/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.3359e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1451/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.4506e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1452/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.3254e-05 - val_loss: 0.0266 - learning_rate: 7.0000e-06\n",
      "Epoch 1453/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.3320e-05 - val_loss: 0.0267 - learning_rate: 7.0000e-06\n",
      "Epoch 1454/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.2926e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1455/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.4383e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1456/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.4117e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1457/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.4891e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1458/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.4462e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1459/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.3426e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1460/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.3244e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1461/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.3364e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1462/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.3405e-05 - val_loss: 0.0244 - learning_rate: 7.0000e-06\n",
      "Epoch 1463/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.2552e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1464/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.4491e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1465/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.3556e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1466/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.3062e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1467/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.2926e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1468/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 3.2058e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:44:00.917588: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2770e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1469/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2300e-05 - val_loss: 0.0266 - learning_rate: 7.0000e-06\n",
      "Epoch 1470/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.2794e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1471/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2811e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1472/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2619e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1473/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.3400e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1474/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.2797e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1475/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.3029e-05 - val_loss: 0.0245 - learning_rate: 7.0000e-06\n",
      "Epoch 1476/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.3827e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1477/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.3609e-05 - val_loss: 0.0268 - learning_rate: 7.0000e-06\n",
      "Epoch 1478/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.4090e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1479/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.3254e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1480/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.3112e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1481/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2590e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1482/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.2829e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 1483/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2256e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1484/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.2913e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1485/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2442e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1486/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.2823e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1487/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.3374e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1488/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.3328e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1489/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.3356e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1490/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2491e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1491/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2781e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1492/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.3497e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1493/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 3.3382e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:44:31.321394: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.3658e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 1494/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.2145e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1495/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2352e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1496/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.2996e-05 - val_loss: 0.0268 - learning_rate: 7.0000e-06\n",
      "Epoch 1497/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.3181e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1498/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.1222e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1499/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.3434e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1500/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.3169e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1501/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2542e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1502/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.4988e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1503/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2496e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1504/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.2528e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 1505/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.3897e-05 - val_loss: 0.0268 - learning_rate: 7.0000e-06\n",
      "Epoch 1506/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 3.1210e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1507/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1860e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 1508/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.2181e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1509/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.1975e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1510/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2240e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1511/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.3806e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1512/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2757e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1513/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.0934e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1514/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2934e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1515/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.2138e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1516/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.3158e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1517/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1229e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1518/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 3.2051e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:45:01.575044: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - loss: 3.2610e-05 - val_loss: 0.0268 - learning_rate: 7.0000e-06\n",
      "Epoch 1519/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2370e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1520/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.2354e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1521/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2418e-05 - val_loss: 0.0266 - learning_rate: 7.0000e-06\n",
      "Epoch 1522/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 3.0122e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1523/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.0963e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1524/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2070e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 1525/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1754e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1526/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.1989e-05 - val_loss: 0.0266 - learning_rate: 7.0000e-06\n",
      "Epoch 1527/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.1686e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1528/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.0731e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1529/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.1031e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1530/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2147e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1531/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.0601e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1532/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1781e-05 - val_loss: 0.0271 - learning_rate: 7.0000e-06\n",
      "Epoch 1533/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2107e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1534/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.3294e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1535/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 3.0853e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1536/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1350e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1537/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.0768e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1538/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.1934e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1539/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.2006e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1540/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.2839e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1541/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.1011e-05 - val_loss: 0.0266 - learning_rate: 7.0000e-06\n",
      "Epoch 1542/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1090e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1543/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.2299e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1544/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:45:31.631252: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.1751e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1545/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.1092e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1546/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.1662e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1547/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.0669e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1548/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 3.1537e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1549/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.2428e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1550/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1275e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1551/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1159e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1552/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1874e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1553/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.0770e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1554/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1810e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1555/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.2415e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1556/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 122ms/step - loss: 3.2084e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1557/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.0796e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1558/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.0360e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1559/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.0828e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1560/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1670e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1561/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.2115e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1562/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9734e-05 - val_loss: 0.0244 - learning_rate: 7.0000e-06\n",
      "Epoch 1563/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.0992e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1564/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 3.1880e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1565/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.0521e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1566/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.1172e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1567/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.0752e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1568/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1341e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1569/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:46:01.791715: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.0793e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1570/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9835e-05 - val_loss: 0.0267 - learning_rate: 7.0000e-06\n",
      "Epoch 1571/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.0570e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1572/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.1055e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1573/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.2159e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1574/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9877e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1575/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 3.1228e-05 - val_loss: 0.0271 - learning_rate: 7.0000e-06\n",
      "Epoch 1576/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.0095e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1577/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.1573e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1578/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1204e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1579/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9802e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1580/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.0784e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1581/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9213e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1582/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9846e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1583/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.0309e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1584/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1148e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1585/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.0632e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1586/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.1737e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1587/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - loss: 3.0797e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1588/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.0989e-05 - val_loss: 0.0266 - learning_rate: 7.0000e-06\n",
      "Epoch 1589/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.0679e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1590/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.0259e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1591/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.0801e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1592/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.1116e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1593/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.9723e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1594/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:46:31.837771: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.0471e-05 - val_loss: 0.0267 - learning_rate: 7.0000e-06\n",
      "Epoch 1595/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.1490e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1596/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.0172e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1597/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9966e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1598/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9768e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1599/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.0174e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1600/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9949e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1601/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1139e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 1602/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.0268e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1603/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.0719e-05 - val_loss: 0.0272 - learning_rate: 7.0000e-06\n",
      "Epoch 1604/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.0216e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1605/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.0455e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1606/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.0848e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1607/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.0501e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1608/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9848e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 1609/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.0444e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1610/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.1561e-05 - val_loss: 0.0268 - learning_rate: 7.0000e-06\n",
      "Epoch 1611/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 3.0178e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1612/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.9742e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1613/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.1067e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1614/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.0494e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1615/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.8971e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1616/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.9320e-05 - val_loss: 0.0241 - learning_rate: 7.0000e-06\n",
      "Epoch 1617/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - loss: 2.9420e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1618/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.9197e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1619/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:47:01.851596: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.1273e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1620/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.0060e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1621/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.0139e-05 - val_loss: 0.0244 - learning_rate: 7.0000e-06\n",
      "Epoch 1622/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.0855e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1623/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.9782e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1624/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.8613e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1625/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9855e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1626/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9561e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1627/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.1190e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1628/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.9639e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 1629/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.9516e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1630/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.8756e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 1631/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.8229e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1632/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9899e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1633/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.9117e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1634/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.0805e-05 - val_loss: 0.0275 - learning_rate: 7.0000e-06\n",
      "Epoch 1635/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9422e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1636/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 3.0104e-05 - val_loss: 0.0273 - learning_rate: 7.0000e-06\n",
      "Epoch 1637/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.9331e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1638/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.8833e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1639/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.0090e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1640/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.9722e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1641/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.9032e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1642/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.8120e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 1643/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 121ms/step - loss: 2.9825e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1644/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 2.9912e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:47:32.597348: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.0052e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1645/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.7332e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1646/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.8400e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1647/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.9631e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1648/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9052e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1649/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.7690e-05 - val_loss: 0.0245 - learning_rate: 7.0000e-06\n",
      "Epoch 1650/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.8983e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1651/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.0739e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1652/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.8831e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1653/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9455e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1654/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.8399e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1655/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 3.0138e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1656/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9522e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1657/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9748e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1658/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.8650e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1659/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.0187e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1660/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.8652e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1661/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.9952e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1662/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.8808e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1663/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.8811e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1664/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.8227e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1665/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.8542e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1666/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9787e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1667/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 3.0463e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1668/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.8500e-05 - val_loss: 0.0266 - learning_rate: 7.0000e-06\n",
      "Epoch 1669/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - loss: 2.8502e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1670/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:48:02.677522: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 3.0326e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1671/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.8912e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1672/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9591e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1673/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.9089e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1674/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.8041e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 1675/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.8317e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1676/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.8577e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1677/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.9889e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1678/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.8113e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1679/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.7734e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1680/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.8508e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 1681/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.8389e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1682/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.7690e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1683/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.8239e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1684/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.8864e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1685/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.7533e-05 - val_loss: 0.0267 - learning_rate: 7.0000e-06\n",
      "Epoch 1686/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.9041e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1687/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.7609e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1688/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.7816e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1689/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.7390e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1690/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6805e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1691/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.9754e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 1692/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.8637e-05 - val_loss: 0.0268 - learning_rate: 7.0000e-06\n",
      "Epoch 1693/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.7726e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1694/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.8329e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1695/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 2.6413e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:48:33.470969: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.7120e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1696/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.7341e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1697/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.8971e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1698/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.6988e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1699/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.7926e-05 - val_loss: 0.0282 - learning_rate: 7.0000e-06\n",
      "Epoch 1700/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.9512e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1701/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.7907e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 1702/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.7804e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1703/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.7896e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1704/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.7811e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1705/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.8101e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1706/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.7118e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1707/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.8508e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1708/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.6872e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1709/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.8069e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1710/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.7100e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 1711/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.7906e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1712/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.8076e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1713/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.8276e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1714/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.7211e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1715/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.8008e-05 - val_loss: 0.0269 - learning_rate: 7.0000e-06\n",
      "Epoch 1716/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.7429e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1717/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.7936e-05 - val_loss: 0.0266 - learning_rate: 7.0000e-06\n",
      "Epoch 1718/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6936e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1719/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6852e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1720/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.7853e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1721/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 2.6168e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:49:04.327375: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6791e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1722/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.7551e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1723/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.7736e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1724/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.7815e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1725/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6385e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1726/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.7580e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1727/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6874e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1728/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6637e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1729/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.7450e-05 - val_loss: 0.0271 - learning_rate: 7.0000e-06\n",
      "Epoch 1730/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.8624e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1731/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.8248e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 1732/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.6775e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1733/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.6584e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1734/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.7010e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1735/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.7264e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1736/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.7006e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1737/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6531e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1738/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.8372e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1739/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6951e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1740/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6606e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 1741/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 2.7538e-05 - val_loss: 0.0269 - learning_rate: 7.0000e-06\n",
      "Epoch 1742/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.8333e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1743/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6767e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1744/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6955e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 1745/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6558e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1746/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 2.7068e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:49:34.390699: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.7308e-05 - val_loss: 0.0243 - learning_rate: 7.0000e-06\n",
      "Epoch 1747/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6275e-05 - val_loss: 0.0266 - learning_rate: 7.0000e-06\n",
      "Epoch 1748/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.8219e-05 - val_loss: 0.0270 - learning_rate: 7.0000e-06\n",
      "Epoch 1749/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6046e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1750/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6282e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1751/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6457e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 1752/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6867e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 1753/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.6145e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1754/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 2.6704e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1755/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.6824e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1756/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.7240e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1757/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.8123e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1758/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.7068e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1759/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6504e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 1760/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.6718e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1761/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.6626e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1762/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.6949e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1763/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6806e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1764/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.7527e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1765/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6900e-05 - val_loss: 0.0271 - learning_rate: 7.0000e-06\n",
      "Epoch 1766/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6745e-05 - val_loss: 0.0269 - learning_rate: 7.0000e-06\n",
      "Epoch 1767/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6893e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1768/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5812e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1769/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6291e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1770/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.6764e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1771/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6905e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1772/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:50:04.586436: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6411e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1773/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.7680e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1774/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6360e-05 - val_loss: 0.0272 - learning_rate: 7.0000e-06\n",
      "Epoch 1775/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5755e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1776/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.7510e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1777/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.5311e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1778/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.6499e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1779/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6144e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 1780/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6821e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1781/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6102e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1782/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6160e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1783/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6540e-05 - val_loss: 0.0246 - learning_rate: 7.0000e-06\n",
      "Epoch 1784/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6608e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1785/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5220e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1786/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5338e-05 - val_loss: 0.0269 - learning_rate: 7.0000e-06\n",
      "Epoch 1787/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.7573e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1788/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6351e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1789/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6036e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1790/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.6721e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1791/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6377e-05 - val_loss: 0.0266 - learning_rate: 7.0000e-06\n",
      "Epoch 1792/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5684e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1793/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.7617e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1794/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.6521e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1795/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5991e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1796/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4939e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1797/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:50:34.590685: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6156e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1798/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6722e-05 - val_loss: 0.0266 - learning_rate: 7.0000e-06\n",
      "Epoch 1799/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - loss: 2.6306e-05 - val_loss: 0.0269 - learning_rate: 7.0000e-06\n",
      "Epoch 1800/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6009e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1801/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.7455e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1802/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5625e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1803/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.5745e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1804/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5895e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1805/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.5496e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 1806/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5907e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1807/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5742e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1808/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5621e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1809/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5686e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1810/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.5272e-05 - val_loss: 0.0266 - learning_rate: 7.0000e-06\n",
      "Epoch 1811/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6231e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1812/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.5926e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1813/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.5599e-05 - val_loss: 0.0268 - learning_rate: 7.0000e-06\n",
      "Epoch 1814/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.6203e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1815/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.6520e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1816/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.7487e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1817/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5945e-05 - val_loss: 0.0266 - learning_rate: 7.0000e-06\n",
      "Epoch 1818/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5183e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1819/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.5893e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1820/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4913e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1821/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 2.5372e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:51:04.595037: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5677e-05 - val_loss: 0.0268 - learning_rate: 7.0000e-06\n",
      "Epoch 1822/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.5415e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1823/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.5781e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1824/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5657e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1825/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5631e-05 - val_loss: 0.0266 - learning_rate: 7.0000e-06\n",
      "Epoch 1826/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.6619e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1827/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.5468e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1828/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.6180e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1829/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5669e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1830/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5751e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1831/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5474e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 1832/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 2.5772e-05 - val_loss: 0.0273 - learning_rate: 7.0000e-06\n",
      "Epoch 1833/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5988e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1834/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4910e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1835/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4812e-05 - val_loss: 0.0266 - learning_rate: 7.0000e-06\n",
      "Epoch 1836/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5026e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1837/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4406e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1838/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4948e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1839/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4533e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1840/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5878e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1841/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4885e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1842/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.5077e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1843/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.4132e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1844/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4905e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1845/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 2.5425e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1846/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 2.4263e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:51:34.614216: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4648e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1847/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5472e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1848/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5616e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1849/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.5482e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1850/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4293e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1851/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.5446e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1852/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5558e-05 - val_loss: 0.0271 - learning_rate: 7.0000e-06\n",
      "Epoch 1853/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5697e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 1854/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4545e-05 - val_loss: 0.0266 - learning_rate: 7.0000e-06\n",
      "Epoch 1855/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4910e-05 - val_loss: 0.0270 - learning_rate: 7.0000e-06\n",
      "Epoch 1856/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5029e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1857/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4375e-05 - val_loss: 0.0266 - learning_rate: 7.0000e-06\n",
      "Epoch 1858/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5746e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1859/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4992e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1860/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.4342e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1861/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.4800e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1862/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.4735e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1863/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.4606e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1864/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5333e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1865/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.5809e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1866/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5773e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1867/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4444e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1868/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.6304e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 1869/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4382e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1870/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4496e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1871/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 2.4025e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:52:04.734443: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 2.4445e-05 - val_loss: 0.0270 - learning_rate: 7.0000e-06\n",
      "Epoch 1872/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4331e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1873/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4233e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1874/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.4273e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1875/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.6012e-05 - val_loss: 0.0243 - learning_rate: 7.0000e-06\n",
      "Epoch 1876/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.4674e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1877/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4985e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1878/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.4363e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1879/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.5292e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1880/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.5244e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1881/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4189e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1882/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4235e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1883/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4865e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1884/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4459e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1885/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3965e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1886/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3912e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 1887/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.4639e-05 - val_loss: 0.0267 - learning_rate: 7.0000e-06\n",
      "Epoch 1888/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4450e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1889/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.3881e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1890/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3360e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1891/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4617e-05 - val_loss: 0.0248 - learning_rate: 7.0000e-06\n",
      "Epoch 1892/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.3870e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1893/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.3803e-05 - val_loss: 0.0267 - learning_rate: 7.0000e-06\n",
      "Epoch 1894/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4503e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1895/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.3998e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1896/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 2.6148e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:52:34.899550: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.6267e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1897/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.4485e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1898/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4314e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1899/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.3629e-05 - val_loss: 0.0274 - learning_rate: 7.0000e-06\n",
      "Epoch 1900/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.4506e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1901/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.3245e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 1902/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.5197e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1903/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3257e-05 - val_loss: 0.0268 - learning_rate: 7.0000e-06\n",
      "Epoch 1904/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3890e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1905/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5925e-05 - val_loss: 0.0267 - learning_rate: 7.0000e-06\n",
      "Epoch 1906/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.4305e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1907/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.5461e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1908/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4702e-05 - val_loss: 0.0266 - learning_rate: 7.0000e-06\n",
      "Epoch 1909/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4855e-05 - val_loss: 0.0242 - learning_rate: 7.0000e-06\n",
      "Epoch 1910/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.4164e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1911/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4621e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1912/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.3801e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1913/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3734e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1914/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5782e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 1915/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3842e-05 - val_loss: 0.0267 - learning_rate: 7.0000e-06\n",
      "Epoch 1916/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.5103e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1917/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.2807e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1918/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4017e-05 - val_loss: 0.0269 - learning_rate: 7.0000e-06\n",
      "Epoch 1919/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4009e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1920/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.4381e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1921/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 2.3450e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:53:04.945159: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3842e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1922/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3098e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1923/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4309e-05 - val_loss: 0.0270 - learning_rate: 7.0000e-06\n",
      "Epoch 1924/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4015e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1925/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.2936e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1926/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4961e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1927/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.3779e-05 - val_loss: 0.0266 - learning_rate: 7.0000e-06\n",
      "Epoch 1928/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3663e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1929/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.4139e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1930/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.2792e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1931/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.4162e-05 - val_loss: 0.0261 - learning_rate: 7.0000e-06\n",
      "Epoch 1932/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.3633e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1933/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - loss: 2.4162e-05 - val_loss: 0.0244 - learning_rate: 7.0000e-06\n",
      "Epoch 1934/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4622e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1935/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3198e-05 - val_loss: 0.0266 - learning_rate: 7.0000e-06\n",
      "Epoch 1936/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.4645e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1937/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3609e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1938/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3633e-05 - val_loss: 0.0250 - learning_rate: 7.0000e-06\n",
      "Epoch 1939/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.5159e-05 - val_loss: 0.0270 - learning_rate: 7.0000e-06\n",
      "Epoch 1940/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.3694e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1941/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.3868e-05 - val_loss: 0.0264 - learning_rate: 7.0000e-06\n",
      "Epoch 1942/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.3967e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1943/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3098e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1944/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3763e-05 - val_loss: 0.0249 - learning_rate: 7.0000e-06\n",
      "Epoch 1945/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3226e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1946/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.3535e-05 - val_loss: 0.0254 - learning_rate: 7.0000e-06\n",
      "Epoch 1947/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:53:35.280749: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3280e-05 - val_loss: 0.0272 - learning_rate: 7.0000e-06\n",
      "Epoch 1948/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.3489e-05 - val_loss: 0.0267 - learning_rate: 7.0000e-06\n",
      "Epoch 1949/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.2923e-05 - val_loss: 0.0270 - learning_rate: 7.0000e-06\n",
      "Epoch 1950/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.3738e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1951/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.2823e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1952/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3692e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1953/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.4914e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1954/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.5366e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1955/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.2233e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1956/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.3868e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1957/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3271e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1958/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 2.3849e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1959/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3814e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1960/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 2.2460e-05 - val_loss: 0.0266 - learning_rate: 7.0000e-06\n",
      "Epoch 1961/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.2360e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1962/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.4135e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1963/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3131e-05 - val_loss: 0.0271 - learning_rate: 7.0000e-06\n",
      "Epoch 1964/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3274e-05 - val_loss: 0.0275 - learning_rate: 7.0000e-06\n",
      "Epoch 1965/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3784e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1966/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.2617e-05 - val_loss: 0.0257 - learning_rate: 7.0000e-06\n",
      "Epoch 1967/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3414e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1968/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.2812e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1969/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3089e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1970/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.4816e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1971/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 2.3841e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:54:05.488706: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 121ms/step - loss: 2.4041e-05 - val_loss: 0.0251 - learning_rate: 7.0000e-06\n",
      "Epoch 1972/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.2413e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1973/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.3273e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1974/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3071e-05 - val_loss: 0.0259 - learning_rate: 7.0000e-06\n",
      "Epoch 1975/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.2243e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1976/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3046e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 1977/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.2831e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1978/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 2.3336e-05 - val_loss: 0.0256 - learning_rate: 7.0000e-06\n",
      "Epoch 1979/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.2910e-05 - val_loss: 0.0255 - learning_rate: 7.0000e-06\n",
      "Epoch 1980/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 2.2425e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1981/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.3334e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n",
      "Epoch 1982/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.2840e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1983/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.3038e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1984/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.4775e-05 - val_loss: 0.0247 - learning_rate: 7.0000e-06\n",
      "Epoch 1985/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3650e-05 - val_loss: 0.0262 - learning_rate: 7.0000e-06\n",
      "Epoch 1986/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.2012e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1987/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 2.1990e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1988/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3534e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1989/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3222e-05 - val_loss: 0.0258 - learning_rate: 7.0000e-06\n",
      "Epoch 1990/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.2838e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1991/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.2430e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1992/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.2920e-05 - val_loss: 0.0263 - learning_rate: 7.0000e-06\n",
      "Epoch 1993/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.3701e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1994/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3253e-05 - val_loss: 0.0252 - learning_rate: 7.0000e-06\n",
      "Epoch 1995/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3687e-05 - val_loss: 0.0266 - learning_rate: 7.0000e-06\n",
      "Epoch 1996/2000\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 2.4101e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 01:54:35.597537: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.4085e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1997/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.2602e-05 - val_loss: 0.0260 - learning_rate: 7.0000e-06\n",
      "Epoch 1998/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 2.3431e-05 - val_loss: 0.0265 - learning_rate: 7.0000e-06\n",
      "Epoch 1999/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - loss: 2.2795e-05 - val_loss: 0.0266 - learning_rate: 7.0000e-06\n",
      "Epoch 2000/2000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 2.2561e-05 - val_loss: 0.0253 - learning_rate: 7.0000e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model with the custom callback\n",
    "history = model_builder.train_model(\n",
    "    train_dataset, \n",
    "    val_dataset, \n",
    "    epochs=2000,\n",
    "    callbacks_list=[lr_scheduler]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Extract the losses from the history object\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "\n",
    "train_loss_x_midpoints = history.history.get('x_midpoints_reshape_loss', train_loss)\n",
    "val_loss_x_midpoints = history.history.get('val_x_midpoints_reshape_loss', val_loss)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2,1)\n",
    "plt.plot(train_loss_x_midpoints, label='Train Loss x_midpoints_reshape')\n",
    "plt.plot(val_loss_x_midpoints, label='Validation Loss x_midpoints_reshape')\n",
    "plt.xlabel('Epochs')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss ')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_builder.model.save(\"/home/da886/Analysis/13KFixed_Mixed_5_32by32_95indexFor19kernel.keras\")\n",
    "# loaded_model = tf.keras.models.load_model(\n",
    "# \"/home/da886/Analysis/30KFixed_13_SparsespotsrandomSPOTS.keras\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1, batch shape: (800, 32, 32)\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1729634555.874463 2936637 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.875507 2936637 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.875875 2936637 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.876224 2936637 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.876534 2936637 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.876836 2936637 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.876921 2936631 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.877005 2936610 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.877808 2936637 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.877965 2936610 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.877989 2936631 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.878337 2936637 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.878643 2936610 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.878696 2936631 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.878926 2936637 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.879419 2936610 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.879467 2936631 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.879636 2936637 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.880134 2936610 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.880170 2936631 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.880359 2936637 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.880955 2936610 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.881032 2936631 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.881061 2936637 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.881765 2936610 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.881850 2936631 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.881890 2936637 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.882416 2936610 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.882487 2936631 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.882812 2936637 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.883101 2936610 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.883215 2936631 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.883396 2936637 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.883804 2936610 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.884062 2936631 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.884139 2936637 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.884460 2936610 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.884897 2936631 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.884966 2936637 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.885245 2936610 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.885642 2936631 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.885696 2936637 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.885857 2936610 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.886409 2936631 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.886469 2936637 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.886594 2936610 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.887253 2936631 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.887396 2936610 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.887496 2936637 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.887859 2936631 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.888070 2936610 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.888603 2936631 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.888725 2936610 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.889053 2936631 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.889275 2936610 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.889625 2936631 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.889805 2936610 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.890110 2936631 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.890518 2936610 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729634555.890771 2936631 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 2, batch shape: (800, 32, 32)\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Processing batch 3, batch shape: (800, 32, 32)\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Processing batch 4, batch shape: (800, 32, 32)\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Processing batch 5, batch shape: (800, 32, 32)\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Processing batch 6, batch shape: (800, 32, 32)\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Processing batch 7, batch shape: (800, 32, 32)\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Processing batch 8, batch shape: (800, 32, 32)\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Processing batch 9, batch shape: (800, 32, 32)\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step   \n",
      "Processing batch 10, batch shape: (800, 32, 32)\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Processing batch 11, batch shape: (800, 32, 32)\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1000us/step\n",
      "Processing batch 12, batch shape: (800, 32, 32)\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Processing batch 13, batch shape: (800, 32, 32)\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 996us/step\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # Define the function for visualizing midpoints\n",
    "# def visualize_midpoints(image, midpoints, title=\"Predicted Midpoint Visualization\"):\n",
    "#     \"\"\"\n",
    "#     Visualizes midpoints on an image without using a probability vector.\n",
    "\n",
    "#     Parameters:\n",
    "#     - image: A 3D tensor representing the image.\n",
    "#     - midpoints: A 2D tensor representing the midpoint coordinates (x, y).\n",
    "#     - title: The title of the plot.\n",
    "\n",
    "#     Returns:\n",
    "#     None (displays the image with midpoints).\n",
    "#     \"\"\"\n",
    "#     # Convert to NumPy arrays for easier handling\n",
    "#     image_np = image\n",
    "#     midpoints_np = midpoints\n",
    "\n",
    "#     # Denormalize image if necessary (adjust based on your normalization method)\n",
    "#     denormalized_image = image_np  # Modify if normalization was applied during training\n",
    "\n",
    "#     # Visualize the image\n",
    "#     plt.figure(figsize=(4, 4))\n",
    "#     plt.imshow(denormalized_image, cmap='gray')\n",
    "#     plt.title(title)\n",
    "\n",
    "#     # Plot midpoints directly, only if they are not (0, 0)\n",
    "#     for i, (x, y) in enumerate(midpoints_np):\n",
    "#         if x >= 0 and y >= 0:  # Only plot if the point is not (0, 0)\n",
    "#             plt.scatter(x, y, color='red', s=5)\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "# # Create the validation dataset\n",
    "# # val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_midpoints))\n",
    "# # val_dataset = val_dataset.batch(800)\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_midpoints))\n",
    "# train_dataset = train_dataset.batch(800)\n",
    "# # inputs,targets = next(iter(train_dataset))\n",
    "# # outputs = model_builder.model.predict(inputs)\n",
    "# # # Initialize lists to collect the data\n",
    "# all_images = []\n",
    "# all_true_midpoints = []\n",
    "# all_pred_midpoints = []\n",
    "\n",
    "# # # Loop through each batch in the validation dataset, predict, and collect results\n",
    "# # for i, (data_batch, midpoints_batch) in enumerate(val_dataset):\n",
    "\n",
    "# for i, (data_batch, midpoints_batch) in enumerate(train_dataset):\n",
    "#     print(f\"Processing batch {i + 1}, batch shape: {data_batch.shape}\")\n",
    "    \n",
    "#     # Get the model predictions\n",
    "#     predictions =model_builder.model.predict(data_batch)\n",
    "\n",
    "#     # Extend the lists to store data from each batch\n",
    "#     all_images.extend(data_batch.numpy())  # Store all images\n",
    "#     all_true_midpoints.extend(midpoints_batch.numpy())  # Store all true midpoints\n",
    "#     all_pred_midpoints.extend(predictions)  # Store all predicted midpoints\n",
    "\n",
    "# # Convert lists to arrays for easier indexing\n",
    "# all_images = np.array(all_images)\n",
    "# all_true_midpoints = np.array(all_true_midpoints)\n",
    "# all_pred_midpoints = np.array(all_pred_midpoints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1, batch shape: (800, 32, 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 15:14:56.909282: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "W0000 00:00:1729696497.069734 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.070713 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.071345 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.071982 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.072732 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.073380 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.074106 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.074780 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.075812 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.076974 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.077911 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.078797 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.079662 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.080649 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.081729 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.082697 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.083666 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.084519 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.085533 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.086896 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.088101 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.090399 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.090939 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.091376 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.092209 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.094522 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.095854 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.101800 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.103664 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.109832 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.110376 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.110863 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.111383 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1729696497.111935 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.112497 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.113071 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.113650 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.114314 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.115083 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.115985 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.116678 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.117516 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.118305 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.119217 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.120143 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.120935 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.121744 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.122627 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.123795 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.124834 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.126883 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.127334 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.127699 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.128303 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.130313 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.131363 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.134769 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729696497.136149 1761502 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 2, batch shape: (800, 32, 32)\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Processing batch 3, batch shape: (800, 32, 32)\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Processing batch 4, batch shape: (800, 32, 32)\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Processing batch 5, batch shape: (800, 32, 32)\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Processing batch 6, batch shape: (800, 32, 32)\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Processing batch 7, batch shape: (800, 32, 32)\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Processing batch 8, batch shape: (800, 32, 32)\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Processing batch 9, batch shape: (800, 32, 32)\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Processing batch 10, batch shape: (800, 32, 32)\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Processing batch 11, batch shape: (800, 32, 32)\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Processing batch 12, batch shape: (800, 32, 32)\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Processing batch 13, batch shape: (800, 32, 32)\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the function for visualizing midpoints\n",
    "def visualize_midpoints_with_gt(image, true_midpoints, pred_midpoints, title=\"Predicted vs GT Midpoints\"):\n",
    "    \"\"\"\n",
    "    Visualizes ground truth and predicted midpoints on an image and draws lines to connect them.\n",
    "\n",
    "    Parameters:\n",
    "    - image: A 3D tensor representing the image.\n",
    "    - true_midpoints: A 2D tensor representing the ground truth midpoint coordinates (x, y).\n",
    "    - pred_midpoints: A 2D tensor representing the predicted midpoint coordinates (x, y).\n",
    "    - title: The title of the plot.\n",
    "\n",
    "    Returns:\n",
    "    None (displays the image with midpoints and lines).\n",
    "    \"\"\"\n",
    "    # Convert to NumPy arrays for easier handling\n",
    "    image_np = image\n",
    "\n",
    "    # Ensure that midpoints are in the shape (num_points, 2) for both true and predicted midpoints\n",
    "    true_midpoints_np = np.reshape(true_midpoints, (-1, 2))\n",
    "    pred_midpoints_np = np.reshape(pred_midpoints, (-1, 2))\n",
    "\n",
    "    # Visualize the image\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(image_np, cmap='gray')\n",
    "    plt.title(title)\n",
    "\n",
    "    # Plot both ground truth and predicted midpoints\n",
    "    for i, ((gt_x, gt_y), (pred_x, pred_y)) in enumerate(zip(true_midpoints_np, pred_midpoints_np)):\n",
    "        if gt_x >= 0 and gt_y >= 0:  # Only plot if the GT point is valid\n",
    "            plt.scatter(gt_x, gt_y, color='blue', label='Ground Truth' if i == 0 else \"\", s=30)\n",
    "            plt.scatter(pred_x, pred_y, color='red', label='Prediction' if i == 0 else \"\", s=30)\n",
    "\n",
    "            # Draw a line connecting the GT and predicted points\n",
    "            plt.plot([gt_x, pred_x], [gt_y, pred_y], color='green', linewidth=1)\n",
    "\n",
    "    # Add legend only once\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Create the training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_midpoints))\n",
    "train_dataset = train_dataset.batch(800)\n",
    "\n",
    "# Initialize lists to collect the data\n",
    "all_images = []\n",
    "all_true_midpoints = []\n",
    "all_pred_midpoints = []\n",
    "\n",
    "# Loop through each batch in the training dataset, predict, and collect results\n",
    "for i, (data_batch, midpoints_batch) in enumerate(train_dataset):\n",
    "    print(f\"Processing batch {i + 1}, batch shape: {data_batch.shape}\")\n",
    "    \n",
    "    # Get the model predictions\n",
    "    predictions = model_builder.model.predict(data_batch)\n",
    "\n",
    "    # Extend the lists to store data from each batch\n",
    "    all_images.extend(data_batch.numpy())  # Store all images\n",
    "    all_true_midpoints.extend(midpoints_batch.numpy())  # Store all true midpoints\n",
    "    all_pred_midpoints.extend(predictions)  # Store all predicted midpoints\n",
    "\n",
    "# Convert lists to arrays for easier indexing\n",
    "all_images = np.array(all_images)\n",
    "all_true_midpoints = np.array(all_true_midpoints)\n",
    "all_pred_midpoints = np.array(all_pred_midpoints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index_to_visualize = np.random.randint(0, len(all_images))\n",
    "# Visualize the results for the first sample (you can change the index to visualize others)\n",
    "visualize_midpoints_with_gt(all_images[index_to_visualize ], all_true_midpoints[index_to_visualize ]*np.max(centers), all_pred_midpoints[index_to_visualize ]*np.max(centers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_midpoints))\n",
    "# train_dataset = train_dataset.batch(800)\n",
    "# inputs,targets = next(iter(train_dataset))\n",
    "# outputs = model_builder.model.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9600, 32, 32), (9600, 1, 13, 2), (9600, 1, 13, 2))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images.shape,all_pred_midpoints.shape,all_true_midpoints.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Select an index to visualize from the entire dataset\n",
    "index_to_visualize = np.random.randint(0, len(all_images))\n",
    "\n",
    "# index_to_visualize = 11548\n",
    "# Visualize the selected image with predicted and true midpoints\n",
    "visualize_midpoints(all_images[index_to_visualize], all_pred_midpoints[index_to_visualize, 0, :, :] * np.max(centers), title=\"Predicted Midpoints\")\n",
    "visualize_midpoints(all_images[index_to_visualize], all_true_midpoints[index_to_visualize, 0, :, :] * np.max(centers), title=\"Ground Truth Midpoints\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.843155e-06, 30.997658)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(all_pred_midpoints)*np.max(centers),np.max(all_pred_midpoints)*np.max(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 31.0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(all_true_midpoints)*np.max(centers),np.max(all_true_midpoints)*np.max(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.0, 31.0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(train_images), np.max(train_images), np.min(train_midpoints)*np.max(centers), np.max(train_midpoints)*np.max(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 4.2590027,  3.908059 ],\n",
       "         [27.263153 ,  4.3831873],\n",
       "         [ 5.8743353,  6.6574388],\n",
       "         [14.020337 ,  8.760142 ],\n",
       "         [ 4.1754484, 10.7192545],\n",
       "         [ 4.123699 , 11.2907295],\n",
       "         [22.326601 , 11.401616 ],\n",
       "         [ 7.203351 , 12.493043 ],\n",
       "         [ 5.814977 , 12.613149 ],\n",
       "         [21.202332 , 14.556165 ],\n",
       "         [ 8.0557995, 17.444569 ],\n",
       "         [ 8.055367 , 18.544151 ],\n",
       "         [ 5.2744675, 26.596989 ]]], dtype=float32),\n",
       " array([[[ 4.,  3.],\n",
       "         [28.,  6.],\n",
       "         [ 6.,  7.],\n",
       "         [14.,  9.],\n",
       "         [ 4., 10.],\n",
       "         [ 4., 10.],\n",
       "         [22., 11.],\n",
       "         [ 7., 12.],\n",
       "         [ 6., 14.],\n",
       "         [21., 15.],\n",
       "         [ 8., 17.],\n",
       "         [ 8., 18.],\n",
       "         [ 5., 28.]]], dtype=float32))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pred_midpoints[2]*np.max(centers),all_true_midpoints[2]*np.max(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1., 35., 12.],\n",
       "        [ 1., 63., 52.],\n",
       "        [ 1., 11., 43.],\n",
       "        ...,\n",
       "        [ 1., 10., 33.],\n",
       "        [ 1., 11., 48.],\n",
       "        [ 1., 59., 49.]],\n",
       "\n",
       "       [[ 1.,  5., 24.],\n",
       "        [ 1., 34., 55.],\n",
       "        [ 1., 42., 29.],\n",
       "        ...,\n",
       "        [ 1.,  2., 55.],\n",
       "        [ 1., 13., 17.],\n",
       "        [ 1., 45.,  8.]],\n",
       "\n",
       "       [[ 1., 38., 36.],\n",
       "        [ 1., 11., 33.],\n",
       "        [ 1., 40., 40.],\n",
       "        ...,\n",
       "        [ 1., 45.,  5.],\n",
       "        [ 1.,  8.,  7.],\n",
       "        [ 1., 55., 37.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1., 50., 18.],\n",
       "        [ 1., 20., 19.],\n",
       "        [ 1., 63.,  8.],\n",
       "        ...,\n",
       "        [ 1., 32., 44.],\n",
       "        [ 1., 63., 24.],\n",
       "        [ 1., 51., 52.]],\n",
       "\n",
       "       [[ 1.,  6., 33.],\n",
       "        [ 1., 23., 59.],\n",
       "        [ 1., 13., 48.],\n",
       "        ...,\n",
       "        [ 1., 26., 62.],\n",
       "        [ 1., 20., 42.],\n",
       "        [ 1., 53.,  2.]],\n",
       "\n",
       "       [[ 1., 24., 40.],\n",
       "        [ 1., 30., 43.],\n",
       "        [ 1., 38., 55.],\n",
       "        ...,\n",
       "        [ 1., 31., 56.],\n",
       "        [ 1., 21., 50.],\n",
       "        [ 1., 15.,  6.]]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[2],)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbAAAAHDCAYAAABF+E9FAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAABKFElEQVR4nO3deXxM5/4H8M9km0SWiSRIIhERGju3EWmundhKlailqhJVa6i1RW8rQW9Tii4oraqgUS29qtzS2oIqQVDVllpiCUKFTEhkf35/uDk/I+tJ5mTmJJ/36zUv5jnPnPmec2bynXPOc75HI4QQICIiUhkLUwdARERUHkxgRESkSkxgRESkSkxgRESkSkxgRESkSkxgRESkSkxgRESkSkxgRESkSkxgRESkSkxgVCFRUVHQaDSy+t65c0fhqIioOmACK4OYmBhoNBocP37c1KGowrvvvovvvvvO6PMNDw+Hg4OD0edrDm7cuIGoqCicOnWqTP0LPpMajQY///xzoelCCHh7e0Oj0aBv374G0x48eIDIyEg0b94c9vb2cHV1RevWrTF58mTcuHFD6lfwg6O4R3JysuzlDA8Ph0ajgZOTEx4+fFho+vnz56X5L1q0yGDa5cuXMXLkSPj5+cHW1hbu7u7o2LEjIiMjDfp17ty52JgbN24sO+YCWVlZmDlzJjw9PWFnZ4egoCDs2rWrzK+/fv06Bg8eDGdnZzg5OeH555/HpUuXiuy7evVqNGnSBLa2tmjUqBGWLl1aqE9x28fW1tag37Vr1zB37ly0bdsWNWvWhJubGzp37ozdu3fLWwFmyMrUAZC6vfXWW5g1a5ZB27vvvosXXngB/fv3N01QKnTjxg3MnTsX9evXR+vWrcv8OltbW2zYsAHt27c3aN+/fz+SkpKg1WoN2nNyctCxY0ecPXsWYWFhmDRpEh48eIDff/8dGzZswIABA+Dp6WnwmhUrVhT5w8HZ2bnMcT7OysoKGRkZ2LZtGwYPHmwwLTY2Fra2tsjMzDRov3DhAgIDA2FnZ4dXXnkF9evXx82bN3HixAksWLAAc+fONejv5eWF6OjoQu+t0+nKFTPwKPlu3rwZU6ZMQaNGjRATE4Nnn30W+/btK7T+n/TgwQN06dIFer0eb775JqytrfHBBx+gU6dOOHXqFFxdXaW+n376KcaNG4eBAwdi2rRpOHjwIF577TVkZGRg5syZheb95PaxtLQ0mL5161YsWLAA/fv3R1hYGHJzc7Fu3Tp0794dX3zxBUaOHFnudWJygkq1Zs0aAUAcO3bM1KGogr29vQgLCyvUHhkZKQCIv//+u1zzDQsLE/b29hWMrngPHjxQbN6lOXbsmAAg1qxZU6b+BZ/J0NBQ4ebmJnJycgymjx49WgQEBAgfHx/Rp08fqf2bb74RAERsbGyheT58+FDo9XrpeUW3V1EKtmGPHj1E//79C01v1KiRGDhwoAAg3n//fal9woQJwsrKSly+fLnQa27dumXwvFOnTqJZs2ZGi1kIIeLj4wvF9PDhQ+Hn5yeCg4NLff2CBQsEAHH06FGp7c8//xSWlpZi9uzZUltGRoZwdXU12GZCCPHSSy8Je3t7cffuXamtrNvnzJkzhfpkZmaKxo0bCy8vr1JjN2c8hFhOBYezrl69ir59+8LBwQF169bF8uXLAQC//fYbunbtCnt7e/j4+GDDhg0Gr7979y5mzJiBFi1awMHBAU5OTujduzd+/fXXQu915coV9OvXD/b29qhduzamTp2KH3/8ERqNBnFxcQZ94+Pj0atXL+h0OtSoUQOdOnXCoUOHSlwWIQTc3Nwwbdo0qS0/Px/Ozs6wtLREamqq1L5gwQJYWVnhwYMHAAqfA9NoNEhPT8fatWulQxrh4eEG75eamorw8HA4OztDp9Nh5MiRyMjIKDHGsrpy5QomTJgAf39/2NnZwdXVFYMGDcLly5cN+hUcgtu/fz8mTJiA2rVrw8vLS5q+fPlyNGjQAHZ2dmjbti0OHjyIzp07o3PnzgbzycrKQmRkJBo2bAitVgtvb2+88cYbyMrKMui3a9cutG/fHs7OznBwcIC/vz/efPNNAEBcXBwCAwMBACNHjpTWW0xMTKnL++KLLyIlJcXgUFZ2djY2b96MYcOGFep/8eJFAEC7du0KTbO1tYWTk1Op72kMw4YNw44dOww+W8eOHcP58+eLjdvLyws+Pj6FptWuXbvccZw9exZXr14ttd/mzZthaWmJMWPGSG22trYYNWoUDh8+jGvXrpX6+sDAQGk7A0Djxo3RrVs3fPPNN1Lbvn37kJKSggkTJhi8PiIiAunp6fjvf/9baN5CCKSlpUEUc2ORZs2awc3NzaBNq9Xi2WefRVJSEu7fv19i7OaMCawC8vLy0Lt3b3h7e2PhwoWoX78+Jk6ciJiYGPTq1Qtt2rTBggUL4OjoiBEjRiAxMVF67aVLl/Ddd9+hb9++WLJkCV5//XX89ttv6NSpk8F5iPT0dHTt2hW7d+/Ga6+9hn/961/45ZdfijyUsHfvXnTs2BFpaWmIjIzEu+++i9TUVHTt2hVHjx4tdjk0Gg3atWuHAwcOSG2nT5+GXq8HAIMEePDgQfzjH/8o9lzU+vXrodVq0aFDB6xfvx7r16/H2LFjDfoMHjwY9+/fR3R0NAYPHoyYmJhCh4DK69ixY/jll18wdOhQfPzxxxg3bhz27NmDzp07F5kkJ0yYgD/++ANz5syRDoWuWLECEydOhJeXFxYuXIgOHTqgf//+SEpKMnhtfn4++vXrh0WLFuG5557D0qVL0b9/f3zwwQcYMmSI1O/3339H3759kZWVhXnz5mHx4sXo16+ftF6bNGmCefPmAQDGjBkjrbeOHTuWurz169dHcHAwvvrqK6ltx44d0Ov1GDp0aKH+BQlg3bp1xf7Be9Ldu3dx584dg8fjiac8QkNDodFo8J///Edq27BhAxo3boynn366yLivXbuGvXv3lmn+eXl5hWK+c+cO0tPTDfo1adIEI0aMKHV+J0+exFNPPVUowbdt2xYASjx3mZ+fj9OnT6NNmzaFprVt2xYXL16UksjJkycBoFDfgIAAWFhYSNMf16BBA+h0Ojg6OmL48OG4detWqcsDAMnJyahRowZq1KhRpv5mybQ7gOpQ1CHEsLAwAUC8++67Utu9e/eEnZ2d0Gg0YuPGjVL72bNnBQARGRkptWVmZoq8vDyD90lMTBRarVbMmzdPalu8eLEAIL777jup7eHDh6Jx48YCgNi3b58QQoj8/HzRqFEj0bNnT5Gfny/1zcjIEL6+vqJ79+4lLuP7778vLC0tRVpamhBCiI8//lj4+PiItm3bipkzZwohhMjLyxPOzs5i6tSp0usKDmM8rrRDiK+88opB+4ABA4Srq2uJ8QlRtkOIGRkZhdoOHz4sAIh169ZJbQXbtH379iI3N1dqz8rKEq6uriIwMNDgsFxMTIwAIDp16iS1rV+/XlhYWIiDBw8avN/KlSsFAHHo0CEhhBAffPBBqYd6ynsI8dixY2LZsmXC0dFRWvZBgwaJLl26CCFEoUOIGRkZwt/fXwAQPj4+Ijw8XKxevbrQYTgh/n97FfXw9/cvU5xPenwbvvDCC6Jbt25CiEefLXd3dzF37lyRmJhY6HDdmTNnhJ2dnQAgWrduLSZPniy+++47kZ6eXug9OnXqVGzcY8eONej75DYtTrNmzUTXrl0Ltf/+++8CgFi5cmWxr/37778FAIPvdYHly5cLAOLs2bNCCCEiIiKEpaVlkfOpVauWGDp0qPT8ww8/FBMnThSxsbFi8+bNYvLkycLKyko0atTI4FBwUc6fPy9sbW3Fyy+/XGI/c8c9sAp69dVXpf87OzvD398f9vb2Bien/f394ezsbDDiSKvVwsLi0erPy8tDSkqKdGjpxIkTUr+dO3eibt266Nevn9Rma2uL0aNHG8Rx6tQp6fBLSkqKwS/Obt264cCBA8jPzy92OTp06IC8vDz88ssvAB7taXXo0AEdOnTAwYMHAQBnzpxBamoqOnToUJ5VJRk3blyh905JSUFaWlqF5gsAdnZ20v9zcnKQkpKChg0bwtnZ2WC9Fhg9erTBSe/jx48jJSUFo0ePhpXV/49xeumll1CzZk2D127atAlNmjRB48aNDX7ld+3aFcCjw0HA/w922Lp1a4nboLwGDx6Mhw8fYvv27bh//z62b99e5GE44NH6iY+Px+uvvw7g0aHUUaNGwcPDA5MmTSp06BMAvv32W+zatcvgsWbNmgrHPWzYMMTFxSE5ORl79+5FcnJysXE3a9YMp06dwvDhw3H58mV89NFH6N+/P+rUqYNVq1YV6l+/fv1CMe/atQtTpkwx6CeEKHQYvigPHz4sNCAGgDTir6gRlY+/FkCZXv/w4UPY2NgUOR9bW1uD95k8eTKWLl2KYcOGYeDAgfjwww+xdu1anD9/Hp988kmx8WRkZGDQoEGws7PDe++9V2w/NeAoxAqwtbVFrVq1DNp0Oh28vLwKXRul0+lw79496Xl+fj4++ugjfPLJJ0hMTEReXp407fERSVeuXIGfn1+h+TVs2NDg+fnz5wEAYWFhxcar1+sL/REu8PTTT6NGjRo4ePAgevbsiYMHD2Lu3Llwd3fH0qVLkZmZKSWy0kZclaZevXoGzwtiunfvXoXPwTx8+BDR0dFYs2YNrl+/bnCYrOCQ6ON8fX0Nnl+5cgVA4fVrZWWF+vXrG7SdP38ef/75Z6HPQIHbt28DAIYMGYLPP/8cr776KmbNmoVu3bohNDQUL7zwgvQjpiJq1aqFkJAQbNiwARkZGcjLy8MLL7xQbH+dToeFCxdi4cKFuHLlCvbs2YNFixZh2bJl0Ol0eOeddwz6d+zYsdA5FGN49tln4ejoiK+//hqnTp1CYGAgGjZsWOh8ZYGnnnoK69evR15eHv744w9s374dCxcuxJgxY+Dr64uQkBCpr729vcHzirKzsysyuReMlnz8h1NRrwVQptfb2dkhOzu7yPlkZmaW+D7Aox8F06dPx+7duwuNDgYe/VgeOnQo/vjjD+zYsaPQiFO1YQKrgCeHq5bW/vgf03fffRdvv/02XnnlFcyfPx8uLi6wsLDAlClTyvUrveA177//frHDsEu6hsra2hpBQUE4cOAALly4gOTkZHTo0AF16tRBTk4O4uPjcfDgQTRu3LjYP9hlVZb1U16TJk3CmjVrMGXKFAQHB0On00Gj0WDo0KFFrtfS/iCUJD8/Hy1atMCSJUuKnO7t7S29x4EDB7Bv3z7897//xc6dO/H111+ja9eu+Omnn4pdH3IMGzYMo0ePRnJyMnr37l3mIe4+Pj545ZVXMGDAADRo0ACxsbGFEphStFotQkNDsXbtWly6dAlRUVFlep2lpSVatGiBFi1aIDg4GF26dEFsbKxRE9aTPDw8cP369ULtN2/eBIASE4GLiwu0Wq3Ut6TXe3h4IC8vD7dv3zYYnJKdnY2UlJQyJRxvb2/cvXu3yGmjR4/G9u3bERsbKx0pUDMmMBPZvHkzunTpgtWrVxu0p6amGvza9fHxwR9//AEhhMFe2IULFwxe5+fnBwBwcnIq9xe5Q4cOWLBgAXbv3g03Nzc0btwYGo0GzZo1w8GDB3Hw4MFCF8UWpayVOZSwefNmhIWFYfHixVJbZmZmmQcdFAxyuHDhArp06SK15+bm4vLly2jZsqXU5ufnh19//RXdunUrdZktLCzQrVs3dOvWDUuWLMG7776Lf/3rX9i3bx9CQkIqvM4GDBiAsWPH4siRI/j6669lv75mzZrw8/PDmTNnKhSHXMOGDcMXX3wBCwuLIgedlKZgsENRycGYWrdujX379iEtLc3gKEF8fLw0vTgWFhZo0aJFkYUQ4uPj0aBBAzg6OhrM5/jx43j22WelfsePH0d+fn6p1wgKIXD58mX84x//KDTt9ddfx5o1a/Dhhx/ixRdfLHE+asFzYCZiaWlZaI9j06ZNhX7l9ezZE9evX8f3338vtWVmZhY67h8QEAA/Pz8sWrRIGuL+uL///rvUmDp06ICsrCx8+OGHaN++vfRHtWBE4Y0bN8p0/sve3r7Co9TKq6j1unTpUoNDtCVp06YNXF1dsWrVKuTm5krtsbGxBoeAgUfnnq5fv17kOZiHDx9KI96K+jVc8Ieo4LCSvb09AJR7vTk4OGDFihWIiorCc889V2y/X3/9tchSXleuXMEff/wBf3//cr1/WYejP6lLly6YP38+li1bBnd392L7HTx4EDk5OYXaf/jhBwBQPO4XXngBeXl5+Oyzz6S2rKwsrFmzBkFBQdLeNgBcvXoVZ8+eLfT6Y8eOGSSxc+fOYe/evRg0aJDU1rVrV7i4uGDFihUGr1+xYgVq1KiBPn36SG1FfadXrFiBv//+G7169TJof//997Fo0SK8+eabmDx5cqnLqxbcAzORvn37Yt68eRg5ciT++c9/4rfffkNsbCwaNGhg0G/s2LFYtmwZXnzxRUyePBkeHh5StQLg//d2LCws8Pnnn6N3795o1qwZRo4cibp16+L69evYt28fnJycsG3bthJjCg4OhpWVFc6dO2dwvUvHjh2lL1RZElhAQAB2796NJUuWwNPTE76+vggKCpK1foqTk5NT5CEuFxcXTJgwAX379sX69euh0+nQtGlTHD58GLt37zY4r1gSGxsbREVFYdKkSejatSsGDx6My5cvIyYmptC5yJdffhnffPMNxo0bh3379qFdu3bIy8vD2bNn8c033+DHH39EmzZtMG/ePBw4cAB9+vSBj48Pbt++jU8++QReXl7S+UQ/Pz84Oztj5cqVcHR0hL29PYKCggqdoytJSec/C+zatQuRkZHo168fnnnmGTg4OODSpUv44osvkJWVVeRhvM2bNxd5+Ll79+6oU6cOgEfD0Tt16lSmARGPs7CwwFtvvVVqvwULFiAhIQGhoaHSXvCJEyewbt06uLi4FBqcodfr8eWXXxY5r+HDh0v/L2vcQUFBGDRoEGbPno3bt2+jYcOGWLt2LS5fvlzoKMqIESOwf/9+gx9SEyZMwKpVq9CnTx/MmDED1tbWWLJkCerUqYPp06dL/ezs7DB//nxERERg0KBB0vnoL7/8Ev/+97/h4uIi9fXx8cGQIUPQokUL2Nra4ueff8bGjRvRunVrg0tXtmzZgjfeeAONGjVCkyZNCq2Xx7ej6phs/KOKFDeMvqgh3cVVAXhyOHNmZqaYPn268PDwEHZ2dqJdu3bi8OHDolOnToWG9V66dEn06dNH2NnZiVq1aonp06eLb7/9VgAQR44cMeh78uRJERoaKlxdXYVWqxU+Pj5i8ODBYs+ePWVa1sDAQAFAxMfHS21JSUkCgPD29i7Uv6hh9GfPnhUdO3aUhj0XDKkvrnJAwfpNTEwsMbaCSxeKevj5+QkhHl3KMHLkSOHm5iYcHBxEz549xdmzZ4WPj4/B0P7SqqsUXEag1WpF27ZtxaFDh0RAQIDo1auXQb/s7GyxYMEC0axZM6HVakXNmjVFQECAmDt3rjSUec+ePeL5558Xnp6ewsbGRnh6eooXX3xR/PXXXwbz2rp1q2jatKmwsrIqdUh9WavDPPm5u3TpkpgzZ4545plnRO3atYWVlZWoVauW6NOnj9i7d6/Ba0saRo/HLuEQouzD0ctyKURRw+gPHTokIiIiRPPmzYVOpxPW1taiXr16Ijw8XFy8eNHg9SUNo3/ys1rWuIV4dPnKjBkzhLu7u9BqtSIwMFDs3LmzUL+C93/StWvXxAsvvCCcnJyEg4OD6Nu3rzh//nyR7/XZZ58Jf39/YWNjI/z8/MQHH3xgcHmMEEK8+uqromnTpsLR0VFYW1uLhg0bipkzZ0qXwhSQsx3VRiOEEc6cU6X78MMPMXXqVCQlJaFu3bqmDqfKy8/PR61atRAaGlrkIUMiqnw8B6YCT15jkpmZiU8//RSNGjVi8lJAZmZmofNo69atw927dwuVkiIi0+E5MBUIDQ1FvXr10Lp1a+nY/tmzZxEbG2vq0KqkI0eOYOrUqRg0aBBcXV1x4sQJrF69Gs2bNzc44U5EpsUEpgI9e/bE559/jtjYWOTl5aFp06bYuHGjQb09Mp769evD29sbH3/8Me7evQsXFxeMGDEC7733XrFVEoio8vEcGBERqRLPgRERkSoxgRERkSqZ3Tmw/Px83LhxA46OjiYtSURERJVPCIH79+/D09Oz1GLXZpfAbty4YVCWhYiIqp9r164Z3CW9KGaXwAqKWgJlLwort5r34zXuzIHc+Mta14/Mg9wjCRxXRWSYC4qj2Dmw5cuXo379+rC1tUVQUFCJt7R/XMGXXaPRKPZQmpKxm+PyUsnMcXuZW0xKx6P2+as9nvIoS1yKJLCvv/4a06ZNQ2RkJE6cOIFWrVqhZ8+e0g3+iIiIKkqRBLZkyRKMHj0aI0eORNOmTbFy5UrUqFEDX3zxhRJvR0RE1ZDRE1h2djYSEhIMbqpoYWGBkJAQHD58uFD/rKwspKWlGTyIiIhKY/RBHHfu3EFeXl6h+8vUqVOn0E3eACA6Ohpz586V/T41atSAm5sbNBqN7EEQRd0Yz5jkHlNWehBHVR8UIITAnTt3kJGRYepQiKgSmXwU4uzZszFt2jTpeVpaWonD6DUaDcLDw9GvXz9YW1ub7QlIqjxCCGRnZ+P777/HmjVrqnzCJqJHjJ7A3NzcYGlpiVu3bhm037p1q8hbhmu1Wmi12jLPPzw8HEOHDoWzs7PUJjeJmdsfOLXHby5efPFFAOC5ViITaQvgKQB/ASjbuPOKMfo5MBsbGwQEBGDPnj1SW35+Pvbs2YPg4OAKzdve3h79+vUzSF5EBZydndGvXz/UqFHD1KEQVTvRAOIBrP/fv9GV8J6KjEKcNm0aVq1ahbVr1+LPP//E+PHjkZ6ejpEjR1Zovq6urrC2tjZSlFQV2djYwM3NzdRhEFUrbQHMeqJt1v/alaTIObAhQ4bg77//xpw5c5CcnIzWrVtj586dhQZ2yGXOF92ReeBnhKjyPVVCu5KHEhUbxDFx4kRMnDix3K8XQhQ615Ofn19sXzVTe/xyKX3OT24SU3r9yx1lWtzn3JiUTvLmdl5X7jo1t/jNLZ4n/SWz3Vh4OxUq0WeffYZhw4ZV6nveuHEDbdq0wblz5yr1fYmofI4CeO+JtmgoP5DD5MPoq5M7d+4gJiYGhw4dwu3bt+Hg4AAvLy/07t0bffv2ha2tralDLFVUVBQePHiARYsWGW1+9+/fx+LFi40yPyIyjdkAtqByRyEygVWSpKQkvPrqq3B0dMSECRPQsGFDWFtb4+LFi9iyZQtq1aqFTp06FXpdbm4urKzUt5nUGjdVH6tPrMax68cQWDcQo54eZepwqoSjqJzEVYCHECvJggULYGlpiXXr1qF79+7w9fWFl5cXOnXqhA8//BAdO3YEAAQGBmLz5s2YNm0aOnToIF3TtHnzZvTv3x/BwcEYOHAgfvjhB2neN27cQGBgoMEht/v37yMwMBAJCQkAgISEBAQGBuLo0aMYMWIE2rdvj1deeQWXL182iDMmJgY9e/ZEp06dMH/+fGRlZUnTPvvsM/z3v//F/v37ERgYKM2/4P1/+uknjBkzBu3atcOOHTuKPPy4YcMGPPfccwCATz/9FNu3b8f+/fvRpk0btGnTBsePH5f6Xr9+HWPHjkX79u0xbNgwnD592ghbgggIWhWEV7e9ik9PfIpXt72KoFVBpg6JyqFaJ7Az987gh6QfcObeGUXfJzU1FfHx8Rg0aBDs7OyK7PP4SdpVq1ahc+fO+Oqrr9CvXz/s27cPixcvxksvvYSNGzciNDQU8+bNM/hjX1YrVqzA5MmTsW7dOlhZWWH+/PnStF27dmHVqlWYMGEC1q5dCzc3N3z77bfS9OHDhyMkJATBwcHYsWMHduzYgZYtW0rTly9fjqFDh+Kbb74p0zV/L7/8Mrp3745//vOf2LlzJ3bu3IlWrVpJ0z/55BO8/PLLiI2NRb169fDWW2+Z3b3cSH1Wn1iNozcM9xOO3jiK1SdWmygiKq9qe4xn6Z9Lse7SOun5iAYjMKnJJEXeKykpCUII+Pj4GLSHhIQgOzsbADBo0CBMmvTo/Xv27Il+/fpJ/f71r3+hb9++GDRoEADAx8cHZ86cwZdffok2bdrIimX8+PEICAgAAISFhWHKlCnIysqCVquVEubzzz8v9T169Ki0F1ajRg1otVrk5OQUea3V0KFD0bVr1zLHUjC/7OzsIuc3fPhwtG/fHkIIjBkzBkOGDEFSUhLq168va5mJHnfs+rFi23koUV2q5R7YmXtnDJIXAKy7tE7xPbEnxcTEIDY2Fg0aNJASGQA0adLEoN/ly5cN9kwAoGXLlkhMTJT9no0aNZL+X5A07t27J71P8+bNDfq3aNGizPNu2rSp7HhKUlSsd+/eNep7UPUTWDdQVjuZr2qZwK6mX5XVXlFeXl7QaDS4cuVKoXZvb+9CtSCLO8xYHAuLwpuxuENtRQ2sMNZ1R0+Ooizq2hU5lfQfj7VgXtXtmjkyvlFPj0JbT8MaEUF1g7j3pULVMoHVs68nq72inJ2dERQUhE2bNuHhw4eyX1+/fn38+uuvBm2nT59GgwYNpPkDj4bpF/jrL/mXENavXx9nzhjuhT753NrausxJqGbNmkhJSTFIOk9e22VtbV0pF+4SPS5+dDw+f+5zjH16LLbWfxNHtBFAfLypwyKZqmUCa16zOUY0GGHQFtYgDM1rNi/mFRU3c+ZM5ObmYsSIEfjpp5+QmJiIy5cv44cffsDly5eL3Isq8PLLL2P79u3YvHkzrl69itjYWOzbtw/Dhw8H8GjPp0WLFli7di0SExORkJCAFStWyI5x6NCh2LZtG77//ntcuXIFn376KS5dumTQx9PTExcuXMDly5eRmppa4qCKgIAA3Lt3D+vWrUNSUhK++eabQjc19fDwwPnz58s0PyJjGvX0KKz8WYd+4e8CI0YAzzwDzJxp6rBIhmo7iGNSk0no4t4FV9Ovop59PUWTF/DocGFsbCzWrFmD5cuX4/bt27CxsYGvry+GDx8uDdAoSufOnTF9+nR8+eWXWLx4MTw9PTFnzhxpMAYAvP3225g/fz5efvll+Pj44LXXXpNdyqtHjx64fv06li5diuzsbHTp0gUDBw40SDr9+/dHQkICwsLCkJGRgZUrV8LDw6PI+fn6+mLmzJlYs2YNVq9eja5du2L48OHYsmWL1GfAgAFISEjAiBEjpPl5enrKipuoXOLjgYULDdsWLgRCQ00TD8mmEWZ2UiEtLQ06nQ6WlpaFzqH4+Phg2bJlqFWrlkF7eWrlyWFu869u5KzPv//+G+PHjy90vtFY8wfknzMsae/aGPMvz3vI/cyp/TNa1F0sXsrPx5oiDoePtLTEWpnbQOn1o/RnqDI+o3Lp9Xo4OTmV2KdaHkIkIjJVAVoyHiYwIqqWjllY4P0n9r4XajQ4JnNvhEyn2p4DIyL6l5UVvsvPlwrQMnmpCxMYEVVrxywsUHRtDjJ3/LlBRESqxARGRESqxARGRESqxARGRESqxARGRESqxARWxURFRWHGjBnS87Fjx2Lx4sUVmqcx5kFEZGwcRl9JoqKisH37dgCPbhPi7u6OPn36YOTIkbC0tFTsfRcuXFjkLVSKkpCQgHHjxmHv3r1wdHQs1zyIiCqL2f5VKuqWHQWVyitad8wUtQ2FEAgODsacOXOQk5ODQ4cOYeHChbC0tMTIkSMN+ubk5BRZu608dDqd7NdoNBqDZS64XYuxKF2Hz8bGptA91kqSmZkpa/5yVUbdOLm17Kpb1f+cnBxF5y/3+yo3HqVrG6qV2SawqsjGxka6s/ALL7yAuLg4HDx4EFeuXMGDBw/QtGlTbNq0CTY2Nti6dSuSk5Px0Ucf4ciRI7CwsEDr1q0xffp0qVp7Xl4ePv74Y3z//fewtLREv379Cr3n2LFj8dRTT2H69OkAgOzsbHz66afYuXMn7t27hzp16iA8PByBgYEYN24cAKBLly4AgL59+yIqKgpjxoyBv7+/NI+0tDQsWrQIBw8eRHZ2NgICAjBjxgzUq/fofmrbtm3D4sWLER0djcWLF+PWrVto3bo1IiMjpeVPSEjAxx9/jEuXLsHKygoNGjTAO++8U2xleyKiJ1XrBGZ/5gy0V68iq149pDdX9nYqRdFqtdDr9QCAY8eOwd7eHsuWLQPw6Bfya6+9hhYtWmDVqlWwtLTE6tWr8dprr+Grr76CtbU1YmNjsX37drz99tvw9fVFbGws4uLi0KZNm2LfMzIyEr/99htmzJiBRo0a4caNG0hNTUWdOnWwYMECzJw5E99++y3s7e0L3WG5QFRUFK5du4YlS5bA3t4eS5cuxeTJk7Fp0ybpUGNmZibWr1+PefPmwcLCAm+//TY+/PBDvPPOO8jNzcWMGTPQv39//Pvf/0ZOTg5+//132Xu6RFS9VdsEVnfpUnisWyc9vzliBK5PmlQp7y2EwNGjR3HkyBEMHjwY9+7dg62tLd566y3pUMQPP/yA/Px8vPXWW9If9sjISHTp0gUJCQl45pln8NVXXyE8PBxdu3YFAMyaNavQDSMfd+XKFezevRvLli1DUFAQgEf3KStQcLjRxcXF4BzY465evYoDBw5g9erVaNWqFQBg/vz56NOnD+Li4hASEgLgUQJ+8803pfkPHjwYn3/+OQAgPT0dDx48QPv27aXpvr6+5ViTRFSdVcsEZn/mjEHyAgCPdeuQ2qWLontiP//8Mzp27Ijc3Fzk5+ejV69eGDNmDBYsWICGDRsaHEc/f/48kpKS0KlTJ4N5ZGdnIykpCQ8ePMCdO3fQrFkzaZqVlRWaNm1a7Dmjv/76C5aWlgY3wpQrMTERlpaWaP7YenJ2doaPjw8SExOlNltbW4Pk6Obmhrt37wJ4lCj79u2L1157DW3btkXbtm3RvXt36fAiEVFZVMsEpr16tdh2JRNYQEAAZs2aBWtra7i5uRmM7LOzszPo+/DhQzRu3Bjz588vNJ+aNWuW6/3lDGyoqCdHLWo0GoPEGhkZiaFDh+KXX37Brl27sHLlSixbtgwtWrSotBiJSN2qx1CVJ2T9b7BBWduNxc7ODt7e3nB3dy91WLq/vz+uXbuGmjVrwtvb2+Dh4OAABwcHuLm54ffff5dek5ubiz///LPYeTZs2BD5+flISEgocnpBTEWNAC3g6+uLvLw8nDlzRmpLTU3FlStXZB8G9Pf3x8iRI/HFF1/Az88PP/74o6zXE1H1Vi0TWHrz5rg5YoRB282wMJMM5ChO79694ezsjBkzZuDkyZO4fv06EhISsGjRIty6dQsAMHToUKxduxZxcXG4fPkyFixYgAcPHhQ7T09PT/Tp0wfz589HXFycNM9du3YBADw8PKDRaPDzzz/j3r17yMjIKDSPevXqoVOnTvj3v/+NU6dO4a+//sKcOXNQu3ZtdO7cuUzLdv36dSxbtgynT5/GzZs3ceTIEVy9ehX169eXvZ6IqPqqlocQAeD6pElI7dLFpKMQS2Jra4tPP/0Uy5YtwxtvvIGMjAzUqlULgYGBsLe3BwC89NJLuHPnDqKiomBhYYHnnnsOnTt3LjGJzZo1C5988gkWLFgAvV4Pd3d3hIeHAwBq166NMWPGYOnSpZg7dy769OmDqKioQvOIjIzEokWLMGXKFOTk5ODpp5/GRx99VOaLnW1tbXHlyhXMnDkTer0ebm5uGDRoEEJDQ2WvJyKqvjSiolcFG1laWlqxF9/6+Phg5cqVlX6y3xgXMquJ0sPZlVw/d+7cweTJk3G1mPOcRVH6QubKILdSSnW7kFlpSl/ILJfSFzJXxsX5er0eTk5OJfaplocQiYhI/ZjAiIhIlartOTA5zO2QoNKHNM1teeXKyspS9LCg0uu/PIdw5R4SrG6HxeUeYi1pJG5R5B4SlLv+5cav9CFKueQsr5zPGvfAiIhIlZjAiIhIlVSVwPLz81V/KIOUJYSQffiHiNRJVQns5s2buHPnTpUY9kzGl5mZiTt37iA5OdnUoRBRJVDVII7c3FxMnz4d48ePR5s2bWBlZcVbcBCEEMjNzcWxY8ewcuVKXuNEVE2o6kLmAhqNBjqdDk5OTtBoNLIv2quMUWFyyL0oUO5Fk3IPqVXGRYrGJIRAWloa9Hp9pRxiNsdRiKa4y7iaKD0KUen1Xx1HIZblQmZV7YEVEEIgNTUVqampAORfdc4EVjK1JTAiqp5UdQ6MiIioABMYERGpEhMYERGpEhMYERGpkioHcTzJxsZGVn+5w6zl9lf6VgZKjzCSO+JJ7qAPtQ8SkTsISO7noTLWj9KjCuUus9zPXHZ2tqz+5kbu+je3UYVyKfV54x4YERGpktETWFRUFDQajcGjcePGxn4bIiKq5hQ5hNisWTPs3r37/99E5uEBIiKi0iiSWaysrODu7q7ErImIiAAodA7s/Pnz8PT0RIMGDfDSSy/h6tWrxfbNyspCWlqawYOIiKg0Rk9gQUFBiImJwc6dO7FixQokJiaiQ4cOuH//fpH9o6OjodPppIe3t7exQyIioipI8WK+qamp8PHxwZIlSzBq1KhC07OyspCVlSU9T0tLk53EbG1tZfU3t2H05jasnMPojUvtn4fyMLdh9OZWzJdKZxbFfJ2dnfHUU0/hwoULRU7XarXQarVKh0FERFWM4teBPXjwABcvXoSHh4fSb0VERNWI0RPYjBkzsH//fly+fBm//PILBgwYAEtLS7z44ovGfisiIqrGjH4IMSkpCS+++CJSUlJQq1YttG/fHkeOHEGtWrWM/VZERFSNqfKOzFWN3BtUyq2Lpva77yp9Q9GqsLxqr8+o9u+Aua3PqqAsgzhYC5GIiFSJCYyIiFSJCYyIiFSJCYyIiFSJCYyIiFSJCYyIiFSJCYyIiFSJCYyIiFSJCYyIiFSJCYyIiFSJCYyIiFRJ8fuBVUdy66IpXXfN3Gr9KV33Tu7NC+XesFQupZe3PMytFp/cbaB0fUwyLjnbS87fK+6BERGRKjGBERGRKjGBERGRKjGBERGRKjGBERGRKjGBERGRKjGBERGRKjGBERGRKjGBERGRKjGBERGRKjGBERGRKpl1LcSy1s+SWxdN6TpwSs9fbm1DpWshyq3NqHStP6VrG8olNx656xNQ/jOndH1Jc/uMyo1H6fUvN35zq3Up5/MjhCjz54d7YEREpEpMYEREpEpMYEREpEpMYEREpEpMYEREpEpMYEREpEpMYEREpEpMYEREpEpMYEREpEpMYEREpEpMYEREpEpmWwtRo9GUucahudX9sra2ltVf6bpxcmtFmlutRbnrU26tRbnzz8vLk9VfLqXXJ2B+9UOVrrVobn8j5FJ7/ErVP+UeGBERqRITGBERqRITGBERqRITGBERqRITGBERqRITGBERqRITGBERqRITGBERqRITGBERqRITGBERqRITGBERqZLZ1kKsjHpwSjG32JWOR+najErVUaus+Std5w+Qv07l9rewUPa3rtL1Jc2N3M+EXHLXp7nVxiwr7oEREZEqyU5gBw4cwHPPPQdPT09oNBp89913BtOFEJgzZw48PDxgZ2eHkJAQnD9/3ljxEhERAShHAktPT0erVq2wfPnyIqcvXLgQH3/8MVauXIn4+HjY29ujZ8+eyMzMrHCwREREBTSiAidINBoNtmzZgv79+wN4tPfl6emJ6dOnY8aMGQAAvV6POnXqICYmBkOHDi11nmlpadDpdNL8y8LczjnJPb4t93i1uS2vXErfn8zcVMdzYHLPkZjbPeiUxnNgpdPr9XByciqxj1HPgSUmJiI5ORkhISFSm06nQ1BQEA4fPmzMtyIiomrOqD8DkpOTAQB16tQxaK9Tp4407UlZWVnIysqSnqelpRkzJCIiqqJMPgoxOjoaOp1Oenh7e5s6JCIiUgGjJjB3d3cAwK1btwzab926JU170uzZs6HX66XHtWvXjBkSERFVUUZNYL6+vnB3d8eePXuktrS0NMTHxyM4OLjI12i1Wjg5ORk8iIiISiP7HNiDBw9w4cIF6XliYiJOnToFFxcX1KtXD1OmTME777yDRo0awdfXF2+//TY8PT2lkYpERETGIDuBHT9+HF26dJGeT5s2DQAQFhaGmJgYvPHGG0hPT8eYMWOQmpqK9u3bY+fOnbC1tTVe1EREVO1V6DowJTx+HVhZKX1dkdrnL5fceOSSG7+1tbWs/nKvozLHa2DMjdxtIHcby12n5rYNzO07XBVU+nVgRERElYUJjIiIVIkJjIiIVIkJjIiIVIkJjIiIVIkJjIiIVIkJjIiIVIkJjIiIVIkJjIiIVIkJjIiIVIkJjIiIVMmod2Q2FaXriik9f0tLS0XnL7dOW05OjkKRPKJ0PBYW8n6Xya2rJ3f+clVGnT8rK3lffbnfgby8PEXnL5fc5ZX7GTW35a0utRm5B0ZERKrEBEZERKrEBEZERKrEBEZERKrEBEZERKrEBEZERKrEBEZERKrEBEZERKrEBEZERKrEBEZERKrEBEZERKpUJWohql1ubq6pQ6gQuXXXlFYZtQTlkFtnztraWvZ7yP0Mye0vt/6j3GW2s7OT1f/hw4ey+qv9Oya3lqO5La+c+IUQZa4tyT0wIiJSJSYwIiJSJSYwIiJSJSYwIiJSJSYwIiJSJSYwIiJSJSYwIiJSJSYwIiJSJSYwIiJSJSYwIiJSJSYwIiJSJbOthajRaMpcY0/p2ndya9PJrQ2YnZ0tq79cStdR02q1svpnZmbK6q80c6vlWNY6cI+TW3tQLqW/Y3JrG8oldxsrvT7lkvuZkPs3KycnR1Z/uZSqzcg9MCIiUiUmMCIiUiUmMCIiUiUmMCIiUiUmMCIiUiUmMCIiUiUmMCIiUiUmMCIiUiUmMCIiUiUmMCIiUiUmMCIiUiWzrYUohDCbemRy64TZ2NjI6q90nTal6pAVkFvbUG5txvLUBpTD0tJSVn+l16fceADAwkLeb1G5tQ2VroWo9HdA7bUQ5X5nlK5tKJec9S9n3XMPjIiIVEl2Ajtw4ACee+45eHp6QqPR4LvvvjOYHh4eLlWSL3j06tXLWPESEREBKEcCS09PR6tWrbB8+fJi+/Tq1Qs3b96UHl999VWFgiQiInqS7HNgvXv3Ru/evUvso9Vq4e7uXu6giIiISqPIObC4uDjUrl0b/v7+GD9+PFJSUpR4GyIiqsaMPgqxV69eCA0Nha+vLy5evIg333wTvXv3xuHDh4scXZWVlYWsrCzpeVpamrFDIiKiKsjoCWzo0KHS/1u0aIGWLVvCz88PcXFx6NatW6H+0dHRmDt3rrHDICKiKk7xYfQNGjSAm5sbLly4UOT02bNnQ6/XS49r164pHRIREVUBil/InJSUhJSUFHh4eBQ5XavVQqvVKh0GERFVMbIT2IMHDwz2phITE3Hq1Cm4uLjAxcUFc+fOxcCBA+Hu7o6LFy/ijTfeQMOGDdGzZ0+jBk5ERNWb7AR2/PhxdOnSRXo+bdo0AEBYWBhWrFiB06dPY+3atUhNTYWnpyd69OiB+fPncy+LiIiMSiPMrOhXWloadDqdou+h9rpocildJ09p5lYnTy61f37Kw9w+c3LjkbvNlN7GSn8HrK2tZfWvjFqLer0eTk5OJfZhLUQiIlIlJjAiIlIlJjAiIlIlJjAiIlIlJjAiIlIlJjAiIlIlJjAiIlIlJjAiIlIlJjAiIlIlJjAiIlIlJjAiIlIlJjAiIlIlxe8HZo7UXnjT3AqlKk3p4rxq/zyUh9Kfoer2mVOapaWlrP55eXmy+ldGcV4lcA+MiIhUiQmMiIhUiQmMiIhUiQmMiIhUiQmMiIhUiQmMiIhUiQmMiIhUiQmMiIhUiQmMiIhUiQmMiIhUiQmMiIhUqUrUQrSykrcYubm5is5fbh0yubXy1F5nTmnmVjvR3OrqAcp/hsytXqfceOR+57Ozs2X1l0vu+qku9T25B0ZERKrEBEZERKrEBEZERKrEBEZERKrEBEZERKrEBEZERKrEBEZERKrEBEZERKrEBEZERKrEBEZERKrEBEZERKpUJWohyq1tKLeOl9LzV7p2n1zmVrtPbh07c6tVaC514yrC3GobKv0dVpraPxPmEg/3wIiISJWYwIiISJWYwIiISJWYwIiISJWYwIiISJWYwIiISJWYwIiISJWYwIiISJWYwIiISJWYwIiISJWYwIiISJWqRC1EuZSu42VtbS2rf3Z2tqz+StdOVLru3dy5c2X1j4qKktVf7vpxcHCQ1V/u9pLb3xwpXdtQLnOpxVdA6dqGStcDVStZayU6OhqBgYFwdHRE7dq10b9/f5w7d86gT2ZmJiIiIuDq6goHBwcMHDgQt27dMmrQREREshLY/v37ERERgSNHjmDXrl3IyclBjx49kJ6eLvWZOnUqtm3bhk2bNmH//v24ceMGQkNDjR44ERFVb7IOIe7cudPgeUxMDGrXro2EhAR07NgRer0eq1evxoYNG9C1a1cAwJo1a9CkSRMcOXIEzzzzjPEipyrH+toJ2KdcR7prXeR4P23qcIjIzFXoHJherwcAuLi4AAASEhKQk5ODkJAQqU/jxo1Rr149HD58mAmMitXgp88w/ND1/z07ji/bHcelHmNMGhMRmbdyj0LMz8/HlClT0K5dOzRv3hwAkJycDBsbGzg7Oxv0rVOnDpKTk4ucT1ZWFtLS0gweVL1YXzvxWPJ6ZPih67C+dsJEERGRGpQ7gUVERODMmTPYuHFjhQKIjo6GTqeTHt7e3hWaH6mPfcp1We1EREA5E9jEiROxfft27Nu3D15eXlK7u7s7srOzkZqaatD/1q1bcHd3L3Jes2fPhl6vlx7Xrl0rT0ikYumudWW1ExEBMhOYEAITJ07Eli1bsHfvXvj6+hpMDwgIgLW1Nfbs2SO1nTt3DlevXkVwcHCR89RqtXBycjJ4UPWS4/00vmxnmKy+bMeBHERUMlmDOCIiIrBhwwZs3boVjo6O0nktnU4HOzs76HQ6jBo1CtOmTYOLiwucnJwwadIkBAcHcwAHlehSjzGIbsJRiERUdrIS2IoVKwAAnTt3Nmhfs2YNwsPDAQAffPABLCwsMHDgQGRlZaFnz5745JNPjBIsVW053k8jlYmLiMpIVgIrS3kSW1tbLF++HMuXLy93UERERKXRCDMrmpWWlgadTgeNRlPm+mLmVqdNbt0yudS+vErHb27xVAYrK3mXdObm5ioUSeWobttY6VqLSpMTf0Hser2+1DERrEZPRESqxARGRESqxARGRESqxARGRESqxARGRESqxARGRESqxARGRESqxARGRESqxARGRESqxARGRESqxARGRESqJK+AmplSug6c3DpkcuuuyZ2/udWBkzt/pbeX0sv79ttvy+o/f/58hSL5f2qvbSiXuX0HlK5VKLe/uf1NVKo2I/fAiIhIlZjAiIhIlarEIUQic2Bz/SQc7t7AAxdPZNf9h6nDIarymMCIjKDRntV4+Zcb/3t2Auv/eQLnu40yaUxEVR0PIRJVkM31k48lr0de/uUGbK6fNFFERNUDExhRBTncvSGrnYiMgwmMqIIeuHjKaici42ACI6qg7Lr/wPp/Giar9f/kQA4ipXEQB5ERnO82Cu805ihEosrEBEZkJNl1/4G7TFxElYaHEImISJU0QqkiVeWUlpYGnU6n6HuYSx2vAuZW100uc4tf7vaVy8y+MgDkbwO5/dVea1FubUC5n1GlPxNKz1/p9VOe77xer4eTk1OJfbgHRkREqsQERkREqsQERkREqsQERkREqsQERkREqsQERkREqsQERkREqsQERkREqsQERkREqsQERkREqsQERkREqmS21eg1Gk2Za9qZW90yueTGb261HOXGr3TtRKWX19zWP1A5tenkUHoby90GeXl5svqb298Iucyt1qWNjU2Z+wohkJOTU6a+3AMjIiJVYgIjIiJVYgIjIiJVYgIjIiJVYgIjIiJVYgIjIiJVYgIjIiJVYgIjIiJVYgIjIiJVYgIjIiJVYgIjIiJVMttaiEIIs6lHZm6178xlvZSX0nX4lGZutRYB84tJ6W1sbt+B8mwzOeQur9LrX05tQwDIzs5WJA7ugRERkSrJSmDR0dEIDAyEo6Mjateujf79++PcuXMGfTp37ixVki94jBs3zqhBExERyUpg+/fvR0REBI4cOYJdu3YhJycHPXr0QHp6ukG/0aNH4+bNm9Jj4cKFRg2aiIhI1jmwnTt3GjyPiYlB7dq1kZCQgI4dO0rtNWrUgLu7u3EiJCIiKkKFzoHp9XoAgIuLi0F7bGws3Nzc0Lx5c8yePRsZGRnFziMrKwtpaWkGDyIiotKUexRifn4+pkyZgnbt2qF58+ZS+7Bhw+Dj4wNPT0+cPn0aM2fOxLlz5/Cf//ynyPlER0dj7ty55Q2DiIiqKY0o53jU8ePHY8eOHfj555/h5eVVbL+9e/eiW7duuHDhAvz8/ApNz8rKQlZWlvQ8LS0N3t7e5QlJMeY2jJ6qtqowjL66fQfMbRi90ipjGL1er4eTk1OJfcq1BzZx4kRs374dBw4cKDF5AUBQUBAAFJvAtFottFptecIgIqJqTFYCE0Jg0qRJ2LJlC+Li4uDr61vqa06dOgUA8PDwKFeARERERZGVwCIiIrBhwwZs3boVjo6OSE5OBgDodDrY2dnh4sWL2LBhA5599lm4urri9OnTmDp1Kjp27IiWLVsqsgBERFQ9yToHVtxx3jVr1iA8PBzXrl3D8OHDcebMGaSnp8Pb2xsDBgzAW2+9VeqxzAJpaWnQ6XRlDalS8Pg/VSaeA1MfngMrmVLnwMo9iEMp5pjAyLgsLORdvSH3Iyq3v9x41F7LEZC/zHKpfR2Z22fU3MhN2HL6F9TBLUsCYy1EIiJSJSYwIiJSJSYwIiJSJSYwIiJSJSYwIiJSJSYwIiJSJSYwIiJSJSYwIiJSJSYwIiJSJSYwIiJSJSYwIiJSpXLfkdmcVMdadnLIrVtmaWkpq7/c9V+ewp5yWFtby+qfm5srq7851hG0spL3VZa7zErWvitPf7nrSOn5mxuliy+bS+1H7oEREZEqMYEREZEqMYEREZEqMYEREZEqMYEREZEqMYEREZEqMYEREZEqMYEREZEqMYEREZEqMYEREZEqMYEREZEqVYlaiErV2SovpeuQya3FJ3f+cuvkyaV07cqcnBxZ/eWSuz7l1iksz+c5Ly9PVn+lt4G51SeVu07lfoflUvpvltLzl1tvVKnvJPfAiIhIlZjAiIhIlZjAiIhIlZjAiIhIlZjAiIhIlZjAiIhIlZjAiIhIlZjAiIhIlZjAiIhIlZjAiIhIlZjAiIhIlapELURzo3QdMqXrxildx07p+OVSe90+c3wPpet1yqV0LUSlPxNy41G6Hqvc2ptK4R4YERGpEhMYERGpEhMYERGpEhMYERGpEhMYERGpEhMYERGpEhMYERGpEhMYERGpEhMYERGpEhMYERGpEhMYERGpklnXQixrPa/KqB1XnZhbXTe55H4elF7e3NxcRedfGeRuM7nLbGNjI6t/dna2rP5ymVu9TrmfaXOrLakU7oEREZEqyUpgK1asQMuWLeHk5AQnJycEBwdjx44d0vTMzExERETA1dUVDg4OGDhwIG7dumX0oImIiGQlMC8vL7z33ntISEjA8ePH0bVrVzz//PP4/fffAQBTp07Ftm3bsGnTJuzfvx83btxAaGioIoETEVE1JyqoZs2a4vPPPxepqanC2tpabNq0SZr2559/CgDi8OHDZZ6fXq8XAAQAodFoyvQo6M+HOh5l3a7lfZh6+ariQ+ltYGNjI+th6vXBh/IPvV5far4o9zmwvLw8bNy4Eenp6QgODkZCQgJycnIQEhIi9WncuDHq1auHw4cPFzufrKwspKWlGTyIiIhKIzuB/fbbb3BwcIBWq8W4ceOwZcsWNG3aFMnJybCxsYGzs7NB/zp16iA5ObnY+UVHR0On00kPb29v2QtBRETVj+wE5u/vj1OnTiE+Ph7jx49HWFgY/vjjj3IHMHv2bOj1eulx7dq1cs+LiIiqD9nXgdnY2KBhw4YAgICAABw7dgwfffQRhgwZguzsbKSmphrshd26dQvu7u7Fzk+r1UKr1cqPnIiIqrUKXweWn5+PrKwsBAQEwNraGnv27JGmnTt3DlevXkVwcHBF34aIiMiArD2w2bNno3fv3qhXrx7u37+PDRs2IC4uDj/++CN0Oh1GjRqFadOmwcXFBU5OTpg0aRKCg4PxzDPPKBU/ERFVU7IS2O3btzFixAjcvHkTOp0OLVu2xI8//oju3bsDAD744ANYWFhg4MCByMrKQs+ePfHJJ58oEjgREVVvGiHMpKjV/6SlpUGn0wGoPrUQ5daZU/vyqp3c7RUVFSWrf2RkpKz+pD5Kf+erwt8UvV4PJyenEvuwFiIREamSWVejJ6rK6iYlwSUlBXddXXHdy8vU4RCpDhMYkQl0++kntDt0SHp+qF077OnRw4QREakPDyESVbK6SUkGyQsA2h06hLpJSSaKiEidmMCIKpnmznlZ7URUNCYwokr2l6u8diIqGhMYUSW769UI77UzbItu96idiMqOgziIKpmXxgvbu7fDliaH8FTKoz0vG6/26K7hSEQiOZjAiEygh6YHkryaIsUrBa3gCi8mLyLZmMCITMRL4wUvMHERlZfZJbDHS5qYY3kTJVSX5awq5G6vzMxMhSIhtVL6O18V/qaUZRnMrhZiUlIS78pMRFTNXbt2DV6lVKgxuwSWn5+PGzduwNHR0aAgZVpaGry9vXHt2rVSCzxWFdVtmbm8VRuXt2oz1vIKIXD//n14enrCwqLkgfJmdwjRwsKixKzr5ORULT4Mj6tuy8zlrdq4vFWbMZa34I4kpeF1YEREpEpMYEREpEqqSWBarRaRkZHQarWmDqXSVLdl5vJWbVzeqs0Uy2t2gziIiIjKQjV7YERERI9jAiMiIlViAiMiIlViAiMiIlVSTQJbvnw56tevD1tbWwQFBeHo0aOmDkkRUVFR0Gg0Bo/GjRubOiyjOXDgAJ577jl4enpCo9Hgu+++M5guhMCcOXPg4eEBOzs7hISE4Px5dd+puLRlDg8PL7TNe/XqZZpgKyg6OhqBgYFwdHRE7dq10b9/f5w7d86gT2ZmJiIiIuDq6goHBwcMHDgQt27dMlHEFVOW5e3cuXOh7Ttu3DgTRVxxK1asQMuWLaULloODg7Fjxw5pemVuX1UksK+//hrTpk1DZGQkTpw4gVatWqFnz564ffu2qUNTRLNmzXDz5k3p8fPPP5s6JKNJT09Hq1atsHz58iKnL1y4EB9//DFWrlyJ+Ph42Nvbo2fPnqouiFvaMgNAr169DLb5V199VYkRGs/+/fsRERGBI0eOYNeuXcjJyUGPHj2Qnp4u9Zk6dSq2bduGTZs2Yf/+/bhx4wZCQ0NNGHX5lWV5AWD06NEG23fhwoUmirjivLy88N577yEhIQHHjx9H165d8fzzz+P3338HUMnbV6hA27ZtRUREhPQ8Ly9PeHp6iujoaBNGpYzIyEjRqlUrU4dRKQCILVu2SM/z8/OFu7u7eP/996W21NRUodVqxVdffWWCCI3vyWUWQoiwsDDx/PPPmyQepd2+fVsAEPv37xdCPNqe1tbWYtOmTVKfP//8UwAQhw8fNlWYRvPk8gohRKdOncTkyZNNF1QlqFmzpvj8888rffua/R5YdnY2EhISEBISIrVZWFggJCQEhw8fNmFkyjl//jw8PT3RoEEDvPTSS7h69aqpQ6oUiYmJSE5ONtjWOp0OQUFBVXZbF4iLi0Pt2rXh7++P8ePHIyUlxdQhGYVerwcAuLi4AAASEhKQk5NjsI0bN26MevXqVYlt/OTyFoiNjYWbmxuaN2+O2bNnIyMjwxThGV1eXh42btyI9PR0BAcHV/r2Nbtivk+6c+cO8vLyUKdOHYP2OnXq4OzZsyaKSjlBQUGIiYmBv78/bt68iblz56JDhw44c+YMHB0dTR2eopKTkwGgyG1dMK0q6tWrF0JDQ+Hr64uLFy/izTffRO/evXH48GFYWlqaOrxyy8/Px5QpU9CuXTs0b94cwKNtbGNjA2dnZ4O+VWEbF7W8ADBs2DD4+PjA09MTp0+fxsyZM3Hu3Dn85z//MWG0FfPbb78hODgYmZmZcHBwwJYtW9C0aVOcOnWqUrev2Sew6qZ3797S/1u2bImgoCD4+Pjgm2++wahRo0wYGSll6NCh0v9btGiBli1bws/PD3FxcejWrZsJI6uYiIgInDlzpkqdwy1Jccs7ZswY6f8tWrSAh4cHunXrhosXL8LPz6+ywzQKf39/nDp1Cnq9Hps3b0ZYWBj2799f6XGY/SFENzc3WFpaFhrFcuvWLbi7u5soqsrj7OyMp556ChcuXDB1KIor2J7VdVsXaNCgAdzc3FS9zSdOnIjt27dj3759BrdHcnd3R3Z2NlJTUw36q30bF7e8RQkKCgIAVW9fGxsbNGzYEAEBAYiOjkarVq3w0UcfVfr2NfsEZmNjg4CAAOzZs0dqy8/Px549exAcHGzCyCrHgwcPcPHiRXh4eJg6FMX5+vrC3d3dYFunpaUhPj6+WmzrAklJSUhJSVHlNhdCYOLEidiyZQv27t0LX19fg+kBAQGwtrY22Mbnzp3D1atXVbmNS1veopw6dQoAVLl9i5Ofn4+srKzK375GHxaigI0bNwqtVitiYmLEH3/8IcaMGSOcnZ1FcnKyqUMzuunTp4u4uDiRmJgoDh06JEJCQoSbm5u4ffu2qUMzivv374uTJ0+KkydPCgBiyZIl4uTJk+LKlStCCCHee+894ezsLLZu3SpOnz4tnn/+eeHr6ysePnxo4sjLr6Rlvn//vpgxY4Y4fPiwSExMFLt37xZPP/20aNSokcjMzDR16LKNHz9e6HQ6ERcXJ27evCk9MjIypD7jxo0T9erVE3v37hXHjx8XwcHBIjg42IRRl19py3vhwgUxb948cfz4cZGYmCi2bt0qGjRoIDp27GjiyMtv1qxZYv/+/SIxMVGcPn1azJo1S2g0GvHTTz8JISp3+6oigQkhxNKlS0W9evWEjY2NaNu2rThy5IipQ1LEkCFDhIeHh7CxsRF169YVQ4YMERcuXDB1WEazb98+AaDQIywsTAjxaCj922+/LerUqSO0Wq3o1q2bOHfunGmDrqCSljkjI0P06NFD1KpVS1hbWwsfHx8xevRo1f44K2o5AYg1a9ZIfR4+fCgmTJggatasKWrUqCEGDBggbt68abqgK6C05b169aro2LGjcHFxEVqtVjRs2FC8/vrrQq/XmzbwCnjllVeEj4+PsLGxEbVq1RLdunWTkpcQlbt9eTsVIiJSJbM/B0ZERFQUJjAiIlIlJjAiIlIlJjAiIlIlJjAiIlIlJjAiIlIlJjAiIlIlJjAiIlIlJjAiIlIlJjAiIlIlJjAiIlIlJjAiIlKl/wOv1AZiCZvl9gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize variables to track the min and max MSE\n",
    "min_mse = float('inf')\n",
    "max_mse = float('-inf')\n",
    "min_mse_index = -1\n",
    "max_mse_index = -1\n",
    "\n",
    "# Loop through each prediction to calculate the MSE\n",
    "for i in range(len(all_pred_midpoints)):\n",
    "    mse = np.mean((all_pred_midpoints[i] - all_true_midpoints[i]) **2)\n",
    "    \n",
    "    if mse < min_mse:\n",
    "        min_mse = mse\n",
    "        min_mse_index = i\n",
    "    \n",
    "    if mse > max_mse:\n",
    "        max_mse = mse\n",
    "        max_mse_index = i\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot an image with its centers\n",
    "def plot_image_with_centers(image, true_center, predicted_center, title):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(image.squeeze(), cmap='gray')  # Display the image\n",
    "\n",
    "    # Plot the actual center (Groundtruth)\n",
    "    plt.scatter(true_center[:, 0], true_center[:, 1], color='green', label='Groundtruth', s=10)\n",
    "\n",
    "    # Plot the predicted center\n",
    "    plt.scatter(predicted_center[:, 0], predicted_center[:, 1], color='red', label='Predictions', s=10)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plotting the image with the least MSE\n",
    "plot_image_with_centers(all_images[min_mse_index],\n",
    "                        all_true_midpoints[min_mse_index][0] * np.max(centers),  # Adjust for scaling if needed\n",
    "                        all_pred_midpoints[min_mse_index][0] * np.max(centers),  # Adjust for scaling if needed\n",
    "                        f'Image with Least MSE. MSE: {min_mse:.4f}')\n",
    "\n",
    "# Plotting the image with the largest MSE\n",
    "plot_image_with_centers(all_images[max_mse_index],\n",
    "                        all_true_midpoints[max_mse_index][0] * np.max(centers),  # Adjust for scaling if needed\n",
    "                        all_pred_midpoints[max_mse_index][0] * np.max(centers),  # Adjust for scaling if needed\n",
    "                        f'Image with Largest MSE. MSE: {max_mse:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5266"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_mse_index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
